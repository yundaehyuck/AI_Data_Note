{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment_MSNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBg1n1I2JZai"
      },
      "source": [
        "##reference : https://github.com/cogsys-tuebingen/mobilestereonet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te11eWZaJWs5"
      },
      "source": [
        "##load library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_QEvcsfJUX"
      },
      "source": [
        "from mobilestereonet import *\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable, Function\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "from skimage import io\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4uQ2ZSrgOWh"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3h-TsBMfJom"
      },
      "source": [
        "##loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bapmeMnmfH4H"
      },
      "source": [
        "def model_loss(disp_ests, disp_gt, mask):\n",
        "    weights = [0.5, 0.5, 0.7, 1.0]\n",
        "    all_losses = []\n",
        "    for disp_est, weight in zip(disp_ests, weights):\n",
        "        all_losses.append(weight * F.smooth_l1_loss(disp_est[mask], disp_gt[mask], size_average=True))\n",
        "    return sum(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hau_X8qwfZBQ"
      },
      "source": [
        "## sceneflow dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoK59BrfJo2w"
      },
      "source": [
        "#download sample dataset\n",
        "\n",
        "!wget https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlow/assets/Sampler.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn6XXpX-JqQD"
      },
      "source": [
        "!tar zxvf /content/Sampler.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJOiU6crfxJ6"
      },
      "source": [
        "def get_transform():\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUct2aRjfu2A"
      },
      "source": [
        "def read_all_lines(filename):\n",
        "    with open(filename) as f:\n",
        "        lines = [line.rstrip() for line in f.readlines()]\n",
        "    return lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RDzMByvfqhY"
      },
      "source": [
        "def pfm_imread(filename):\n",
        "    file = open(filename, 'rb')\n",
        "    color = None\n",
        "    width = None\n",
        "    height = None\n",
        "    scale = None\n",
        "    endian = None\n",
        "\n",
        "    header = file.readline().decode('utf-8').rstrip()\n",
        "    if header == 'PF':\n",
        "        color = True\n",
        "    elif header == 'Pf':\n",
        "        color = False\n",
        "    else:\n",
        "        raise Exception('Not a PFM file.')\n",
        "\n",
        "    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode('utf-8'))\n",
        "    if dim_match:\n",
        "        width, height = map(int, dim_match.groups())\n",
        "    else:\n",
        "        raise Exception('Malformed PFM header.')\n",
        "\n",
        "    scale = float(file.readline().rstrip())\n",
        "    if scale < 0:  \n",
        "        endian = '<'\n",
        "        scale = -scale\n",
        "    else:\n",
        "        endian = '>'  \n",
        "\n",
        "    data = np.fromfile(file, endian + 'f')\n",
        "    shape = (height, width, 3) if color else (height, width)\n",
        "\n",
        "    data = np.reshape(data, shape)\n",
        "    data = np.flipud(data)\n",
        "    return data, scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP7CC3X-fX06"
      },
      "source": [
        "class SceneFlowDataset(Dataset):\n",
        "    def __init__(self, datapath, list_filename, training):\n",
        "        self.datapath = datapath\n",
        "        self.left_filenames, self.right_filenames, self.disp_filenames = self.load_path(list_filename)\n",
        "        self.training = training\n",
        "\n",
        "    def load_path(self, list_filename):\n",
        "        lines = read_all_lines(list_filename)\n",
        "        splits = [line.split() for line in lines]\n",
        "        left_images = [x[0] for x in splits]\n",
        "        right_images = [x[1] for x in splits]\n",
        "        disp_images = [x[2] for x in splits]\n",
        "        return left_images, right_images, disp_images\n",
        "\n",
        "    def load_image(self, filename):\n",
        "        return Image.open(filename).convert('RGB')\n",
        "\n",
        "    def load_disp(self, filename):\n",
        "        data, scale = pfm_imread(filename)\n",
        "        data = np.ascontiguousarray(data, dtype=np.float32)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.left_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        left_img = self.load_image(os.path.join(self.datapath, self.left_filenames[index]))\n",
        "        right_img = self.load_image(os.path.join(self.datapath, self.right_filenames[index]))\n",
        "        disparity = self.load_disp(os.path.join(self.datapath, self.disp_filenames[index]))\n",
        "\n",
        "        if self.training:\n",
        "            w, h = left_img.size\n",
        "            crop_w, crop_h = 512, 256\n",
        "\n",
        "            x1 = random.randint(0, w - crop_w)\n",
        "            y1 = random.randint(0, h - crop_h)\n",
        "\n",
        "            # random crop\n",
        "            left_img = left_img.crop((x1, y1, x1 + crop_w, y1 + crop_h))\n",
        "            right_img = right_img.crop((x1, y1, x1 + crop_w, y1 + crop_h))\n",
        "            disparity = disparity[y1:y1 + crop_h, x1:x1 + crop_w]\n",
        "\n",
        "            # to tensor, normalize\n",
        "            processed = get_transform()\n",
        "            left_img = processed(left_img)\n",
        "            right_img = processed(right_img)\n",
        "\n",
        "            return {\"left\": left_img,\n",
        "                    \"right\": right_img,\n",
        "                    \"disparity\": disparity,\n",
        "                    }\n",
        "        else:\n",
        "            w, h = left_img.size\n",
        "            crop_w, crop_h = 960, 512\n",
        "\n",
        "            left_img = left_img.crop((w - crop_w, h - crop_h, w, h))\n",
        "            right_img = right_img.crop((w - crop_w, h - crop_h, w, h))\n",
        "            disparity = disparity[h - crop_h:h, w - crop_w: w]\n",
        "\n",
        "            processed = get_transform()\n",
        "            left_img = processed(left_img)\n",
        "            right_img = processed(right_img)\n",
        "\n",
        "            return {\"left\": left_img,\n",
        "                    \"right\": right_img,\n",
        "                    \"disparity\": disparity,\n",
        "                    \"top_pad\": 0,\n",
        "                    \"right_pad\": 0,\n",
        "                    \"left_filename\": self.left_filenames[index]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGWuBgNiJ4pK"
      },
      "source": [
        "##hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyIgGHj8iTeX"
      },
      "source": [
        "#hyperparameter\n",
        "\n",
        "seed = 1\n",
        "\n",
        "datapath = \"/content/\"\n",
        "\n",
        "trainlist = '/content/trainsamplelist.txt'\n",
        "\n",
        "testlist = '/content/testsamplelist.txt' \n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "test_batch_size = 1\n",
        "\n",
        "maxdisp = 192\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "lrepochs = \"1,2:2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqiBIOOEjnrU"
      },
      "source": [
        "#set seeds\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "#os.makedirs(args.logdir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PRH3RfEKXW_"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch, base_lr, lrepochs):\n",
        "    splits = lrepochs.split(':')\n",
        "    assert len(splits) == 2\n",
        "\n",
        "    # parse the epochs to downscale the learning rate (before :)\n",
        "    downscale_epochs = [int(eid_str) for eid_str in splits[0].split(',')]\n",
        "    # parse downscale rate (after :)\n",
        "    downscale_rate = float(splits[1])\n",
        "    print(\"downscale epochs: {}, downscale rate: {}\".format(downscale_epochs, downscale_rate))\n",
        "\n",
        "    lr = base_lr\n",
        "    for eid in downscale_epochs:\n",
        "        if epoch >= eid:\n",
        "            lr /= downscale_rate\n",
        "        else:\n",
        "            break\n",
        "    print(\"setting learning rate to {}\".format(lr))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu_eB2XaJ9IB"
      },
      "source": [
        "## create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyXRTpGOjmsl"
      },
      "source": [
        "# dataset, dataloader\n",
        "train_dataset = SceneFlowDataset(datapath, trainlist, True)\n",
        "test_dataset = SceneFlowDataset(datapath, testlist, False)\n",
        "TrainImgLoader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
        "TestImgLoader = DataLoader(test_dataset, test_batch_size, shuffle=False, num_workers=4, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjbKc5eo21-J",
        "outputId": "688b8eb0-53ed-4b93-f3f4-3a2e1df38558"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'disparity': array([[ 45.36875 ,  45.28208 ,  45.195408, ...,  16.93244 ,  16.952013,\n",
              "          16.997654],\n",
              "        [ 45.36269 ,  45.27602 ,  45.189346, ...,  16.911587,  16.944834,\n",
              "          16.991657],\n",
              "        [ 45.35663 ,  45.269962,  45.18329 , ...,  16.90439 ,  16.941797,\n",
              "          16.970854],\n",
              "        ...,\n",
              "        [214.51234 , 214.5125  , 214.51254 , ..., 214.51263 , 214.51254 ,\n",
              "         214.5125  ],\n",
              "        [215.51253 , 215.51266 , 215.51257 , ..., 215.51247 , 215.51253 ,\n",
              "         215.51253 ],\n",
              "        [216.51242 , 216.51237 , 216.51218 , ..., 216.51222 , 216.51218 ,\n",
              "         216.51228 ]], dtype=float32),\n",
              " 'left': tensor([[[-1.6384, -1.6555, -1.6727,  ...,  2.2318,  2.2318,  2.2318],\n",
              "          [-1.6727, -1.7069, -1.7069,  ...,  2.2318,  2.2318,  2.2318],\n",
              "          [-1.6384, -1.6727, -1.6555,  ...,  2.2318,  2.2318,  2.2318],\n",
              "          ...,\n",
              "          [ 0.4851,  0.5193,  0.5364,  ...,  0.2282,  0.2111,  0.2282],\n",
              "          [ 0.4679,  0.5364,  0.6049,  ...,  0.1597,  0.2453,  0.3138],\n",
              "          [ 0.3652,  0.4166,  0.3823,  ...,  0.1939,  0.2624,  0.2967]],\n",
              " \n",
              "         [[-1.5630, -1.5805, -1.5980,  ...,  2.4111,  2.4111,  2.4111],\n",
              "          [-1.5980, -1.6155, -1.6331,  ...,  2.4111,  2.4111,  2.4111],\n",
              "          [-1.5630, -1.5805, -1.5805,  ...,  2.4111,  2.4111,  2.4111],\n",
              "          ...,\n",
              "          [ 0.4153,  0.4503,  0.4503,  ...,  0.4153,  0.3978,  0.3978],\n",
              "          [ 0.4853,  0.5378,  0.5903,  ...,  0.3627,  0.4503,  0.4853],\n",
              "          [ 0.5203,  0.5378,  0.4853,  ...,  0.4503,  0.5028,  0.5028]],\n",
              " \n",
              "         [[-1.3513, -1.3687, -1.3861,  ...,  2.4657,  2.4831,  2.4831],\n",
              "          [-1.3861, -1.4036, -1.4036,  ...,  2.4483,  2.4831,  2.5006],\n",
              "          [-1.3687, -1.3861, -1.3861,  ...,  2.4134,  2.4831,  2.5006],\n",
              "          ...,\n",
              "          [ 0.2173,  0.2522,  0.2696,  ...,  0.3742,  0.3393,  0.3568],\n",
              "          [ 0.2871,  0.3568,  0.4265,  ...,  0.3045,  0.3916,  0.4265],\n",
              "          [ 0.4439,  0.4962,  0.4962,  ...,  0.3568,  0.3916,  0.4091]]]),\n",
              " 'right': tensor([[[-1.6384, -1.6042, -1.6213,  ..., -0.0287, -2.1179, -1.8097],\n",
              "          [-1.6727, -1.6384, -1.6042,  ..., -0.0458, -2.1179, -1.7754],\n",
              "          [-1.6555, -1.6555, -1.6042,  ..., -0.0116, -2.1179, -1.8097],\n",
              "          ...,\n",
              "          [ 0.5364,  0.5878,  0.6392,  ...,  0.2967,  0.2967,  0.3481],\n",
              "          [ 0.8789,  0.7591,  0.6563,  ...,  0.4337,  0.4679,  0.4508],\n",
              "          [ 0.8789,  0.7933,  0.6392,  ...,  0.4679,  0.5878,  0.5536]],\n",
              " \n",
              "         [[-1.5630, -1.5630, -1.5630,  ...,  0.0126, -2.0357, -1.7031],\n",
              "          [-1.5980, -1.5805, -1.5455,  ...,  0.0126, -2.0357, -1.6681],\n",
              "          [-1.5805, -1.5805, -1.5455,  ...,  0.0301, -2.0357, -1.7031],\n",
              "          ...,\n",
              "          [ 0.5903,  0.6604,  0.6954,  ...,  0.5903,  0.5903,  0.6429],\n",
              "          [ 0.9405,  0.8354,  0.7304,  ...,  0.7304,  0.7654,  0.7479],\n",
              "          [ 0.9580,  0.8704,  0.7129,  ...,  0.7304,  0.8704,  0.8354]],\n",
              " \n",
              "         [[-1.3687, -1.3513, -1.3513,  ..., -0.0267, -1.7522, -1.4384],\n",
              "          [-1.3687, -1.3687, -1.3339,  ..., -0.0267, -1.6824, -1.4036],\n",
              "          [-1.3513, -1.3687, -1.3339,  ...,  0.0082, -1.7173, -1.4210],\n",
              "          ...,\n",
              "          [ 0.6705,  0.7228,  0.7402,  ...,  0.6008,  0.6008,  0.6531],\n",
              "          [ 0.9494,  0.8274,  0.7054,  ...,  0.7228,  0.7576,  0.7402],\n",
              "          [ 0.8797,  0.7751,  0.6182,  ...,  0.7402,  0.8448,  0.8099]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Nn9WoOKAvZ"
      },
      "source": [
        "## create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYW-vD5t3ai7"
      },
      "source": [
        "# model, optimizer\n",
        "model = MSNet2D(maxdisp)\n",
        "model = nn.DataParallel(model)\n",
        "model.cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtRJInqSKSxz"
      },
      "source": [
        "## metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc1PnF9T6OEg"
      },
      "source": [
        "def make_nograd_func(func):\n",
        "    def wrapper(*f_args, **f_kwargs):\n",
        "        with torch.no_grad():\n",
        "            ret = func(*f_args, **f_kwargs)\n",
        "        return ret\n",
        "\n",
        "    return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLr5LpfC6GG8"
      },
      "source": [
        "def check_shape_for_metric_computation(*vars):\n",
        "    assert isinstance(vars, tuple)\n",
        "    for var in vars:\n",
        "        assert len(var.size()) == 3\n",
        "        assert var.size() == vars[0].size()\n",
        "\n",
        "\n",
        "# a wrapper to compute metrics for each image individually\n",
        "def compute_metric_for_each_image(metric_func):\n",
        "    def wrapper(D_ests, D_gts, masks, *nargs):\n",
        "        check_shape_for_metric_computation(D_ests, D_gts, masks)\n",
        "        bn = D_gts.shape[0]  # batch size\n",
        "        results = []  # a list to store results for each image\n",
        "        # compute result one by one\n",
        "        for idx in range(bn):\n",
        "            # if tensor, then pick idx, else pass the same value\n",
        "            cur_nargs = [x[idx] if isinstance(x, (Tensor, Variable)) else x for x in nargs]\n",
        "            if masks[idx].float().mean() / (D_gts[idx] > 0).float().mean() < 0.1:\n",
        "                print(\"masks[idx].float().mean() too small, skip\")\n",
        "            else:\n",
        "                ret = metric_func(D_ests[idx], D_gts[idx], masks[idx], *cur_nargs)\n",
        "                results.append(ret)\n",
        "        if len(results) == 0:\n",
        "            print(\"masks[idx].float().mean() too small for all images in this batch, return 0\")\n",
        "            return torch.tensor(0, dtype=torch.float32, device=D_gts.device)\n",
        "        else:\n",
        "            return torch.stack(results).mean()\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@make_nograd_func\n",
        "@compute_metric_for_each_image\n",
        "def D1_metric(D_est, D_gt, mask):\n",
        "    D_est, D_gt = D_est[mask], D_gt[mask]\n",
        "    E = torch.abs(D_gt - D_est)\n",
        "    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n",
        "    return torch.mean(err_mask.float())\n",
        "\n",
        "\n",
        "@make_nograd_func\n",
        "@compute_metric_for_each_image\n",
        "def Thres_metric(D_est, D_gt, mask, thres):\n",
        "    assert isinstance(thres, (int, float))\n",
        "    D_est, D_gt = D_est[mask], D_gt[mask]\n",
        "    E = torch.abs(D_gt - D_est)\n",
        "    err_mask = E > thres\n",
        "    return torch.mean(err_mask.float())\n",
        "\n",
        "\n",
        "@make_nograd_func\n",
        "@compute_metric_for_each_image\n",
        "def EPE_metric(D_est, D_gt, mask):\n",
        "    D_est, D_gt = D_est[mask], D_gt[mask]\n",
        "    return F.l1_loss(D_est, D_gt, size_average=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ZfBGSiKe4l"
      },
      "source": [
        "##training setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkn-w1co6clv"
      },
      "source": [
        "def make_iterative_func(func):\n",
        "    def wrapper(vars):\n",
        "        if isinstance(vars, list):\n",
        "            return [wrapper(x) for x in vars]\n",
        "        elif isinstance(vars, tuple):\n",
        "            return tuple([wrapper(x) for x in vars])\n",
        "        elif isinstance(vars, dict):\n",
        "            return {k: wrapper(v) for k, v in vars.items()}\n",
        "        else:\n",
        "            return func(vars)\n",
        "\n",
        "    return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGztfeIt6Va6"
      },
      "source": [
        "@make_iterative_func\n",
        "def tensor2float(vars):\n",
        "    if isinstance(vars, float):\n",
        "        return vars\n",
        "    elif isinstance(vars, torch.Tensor):\n",
        "        return vars.data.item()\n",
        "    else:\n",
        "        raise NotImplementedError(\"invalid input type for tensor2float\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgqLRsye-QPz"
      },
      "source": [
        "@make_iterative_func\n",
        "def check_allfloat(vars):\n",
        "    assert isinstance(vars, float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qh1nGl6oOl"
      },
      "source": [
        "class AverageMeterDict(object):\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, x):\n",
        "        check_allfloat(x)\n",
        "        self.count += 1\n",
        "        if self.data is None:\n",
        "            self.data = copy.deepcopy(x)\n",
        "        else:\n",
        "            for k1, v1 in x.items():\n",
        "                if isinstance(v1, float):\n",
        "                    self.data[k1] += v1\n",
        "                elif isinstance(v1, tuple) or isinstance(v1, list):\n",
        "                    for idx, v2 in enumerate(v1):\n",
        "                        self.data[k1][idx] += v2\n",
        "                else:\n",
        "                    assert NotImplementedError(\"error input type for update AvgMeterDict\")\n",
        "\n",
        "    def mean(self):\n",
        "        @make_iterative_func\n",
        "        def get_mean(v):\n",
        "            return v / float(self.count)\n",
        "\n",
        "        return get_mean(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRStrhzLKkH7"
      },
      "source": [
        "## train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbMzdlo2e5tV"
      },
      "source": [
        "def train():\n",
        "    best_checkpoint_loss = 100\n",
        "    for epoch_idx in range(start_epoch, epochs):\n",
        "        adjust_learning_rate(optimizer, epoch_idx, lr, lrepochs)\n",
        "\n",
        "        # training\n",
        "        for batch_idx, sample in enumerate(TrainImgLoader):\n",
        "            global_step = len(TrainImgLoader) * epoch_idx + batch_idx\n",
        "            #start_time = time.time()\n",
        "            #do_summary = global_step % args.summary_freq == 0\n",
        "            loss, scalar_outputs, image_outputs = train_sample(sample, compute_metrics=True)\n",
        "            #if do_summary:\n",
        "                #save_scalars(logger, 'train', scalar_outputs, global_step)\n",
        "                #save_images(logger, 'train', image_outputs, global_step)\n",
        "            del scalar_outputs, image_outputs\n",
        "            print('Epoch {}/{}, Iter {}/{}, train loss = {:.3f}'.format(epoch_idx, epochs,\n",
        "                                                                                       batch_idx,\n",
        "                                                                                       len(TrainImgLoader), loss))\n",
        "        # saving checkpoints\n",
        "        #if (epoch_idx + 1) % args.save_freq == 0:\n",
        "            #checkpoint_data = {'epoch': epoch_idx, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "            #torch.save(checkpoint_data, \"{}/checkpoint_{:0>6}.ckpt\".format(args.logdir, epoch_idx))\n",
        "        #gc.collect()\n",
        "\n",
        "        # testing\n",
        "        avg_test_scalars = AverageMeterDict()\n",
        "        for batch_idx, sample in enumerate(TestImgLoader):\n",
        "            global_step = len(TestImgLoader) * epoch_idx + batch_idx\n",
        "            #start_time = time.time()\n",
        "            #do_summary = global_step % args.summary_freq == 0\n",
        "            loss, scalar_outputs, image_outputs = test_sample(sample, compute_metrics=True)\n",
        "            #if do_summary:\n",
        "                #save_scalars(logger, 'test', scalar_outputs, global_step)\n",
        "                #save_images(logger, 'test', image_outputs, global_step)\n",
        "            avg_test_scalars.update(scalar_outputs)\n",
        "            del scalar_outputs, image_outputs\n",
        "            print('Epoch {}/{}, Iter {}/{}, test loss = {:.3f}'.format(epoch_idx,epochs,\n",
        "                                                                                     batch_idx,\n",
        "                                                                                     len(TestImgLoader), loss))\n",
        "        avg_test_scalars = avg_test_scalars.mean()\n",
        "\n",
        "        #save_scalars(logger, 'fulltest', avg_test_scalars, len(TrainImgLoader) * (epoch_idx + 1))\n",
        "        #print(\"avg_test_scalars\", avg_test_scalars)\n",
        "\n",
        "        # saving new best checkpoint\n",
        "        #if avg_test_scalars['loss'] < best_checkpoint_loss:\n",
        "            #best_checkpoint_loss = avg_test_scalars['loss']\n",
        "            #print(\"Overwriting best checkpoint\")\n",
        "            #checkpoint_data = {'epoch': epoch_idx, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "            #torch.save(checkpoint_data, \"{}/best.ckpt\".format(args.logdir))\n",
        "\n",
        "        #gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QvDwhbK48Lq"
      },
      "source": [
        "# train one sample\n",
        "def train_sample(sample, compute_metrics=False):\n",
        "    model.train()\n",
        "\n",
        "    imgL, imgR, disp_gt = sample['left'], sample['right'], sample['disparity']\n",
        "    imgL = imgL.cuda()\n",
        "    imgR = imgR.cuda()\n",
        "    disp_gt = disp_gt.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    disp_ests = model(imgL, imgR)\n",
        "    mask = (disp_gt < maxdisp) & (disp_gt > 0)\n",
        "    loss = model_loss(disp_ests, disp_gt, mask)\n",
        "\n",
        "    scalar_outputs = {\"loss\": loss}\n",
        "    image_outputs = {\"disp_est\": disp_ests, \"disp_gt\": disp_gt, \"imgL\": imgL, \"imgR\": imgR}\n",
        "    if compute_metrics:\n",
        "        with torch.no_grad():\n",
        "            #image_outputs[\"errormap\"] = [disp_error_image_func.apply(disp_est, disp_gt) for disp_est in disp_ests]\n",
        "            scalar_outputs[\"EPE\"] = [EPE_metric(disp_est, disp_gt, mask) for disp_est in disp_ests]\n",
        "            scalar_outputs[\"D1\"] = [D1_metric(disp_est, disp_gt, mask) for disp_est in disp_ests]\n",
        "            scalar_outputs[\"Thres1\"] = [Thres_metric(disp_est, disp_gt, mask, 1.0) for disp_est in disp_ests]\n",
        "            scalar_outputs[\"Thres2\"] = [Thres_metric(disp_est, disp_gt, mask, 2.0) for disp_est in disp_ests]\n",
        "            scalar_outputs[\"Thres3\"] = [Thres_metric(disp_est, disp_gt, mask, 3.0) for disp_est in disp_ests]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return tensor2float(loss), tensor2float(scalar_outputs), image_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIvM095_45wj"
      },
      "source": [
        "# test one sample\n",
        "@make_nograd_func\n",
        "def test_sample(sample, compute_metrics=True):\n",
        "    model.eval()\n",
        "\n",
        "    imgL, imgR, disp_gt = sample['left'], sample['right'], sample['disparity']\n",
        "    imgL = imgL.cuda()\n",
        "    imgR = imgR.cuda()\n",
        "    disp_gt = disp_gt.cuda()\n",
        "\n",
        "    disp_ests = model(imgL, imgR)\n",
        "    mask = (disp_gt < maxdisp) & (disp_gt > 0)\n",
        "    loss = model_loss(disp_ests, disp_gt, mask)\n",
        "\n",
        "    scalar_outputs = {\"loss\": loss}\n",
        "    image_outputs = {\"disp_est\": disp_ests, \"disp_gt\": disp_gt, \"imgL\": imgL, \"imgR\": imgR}\n",
        "\n",
        "    scalar_outputs[\"D1\"] = [D1_metric(disp_est, disp_gt, mask) for disp_est in disp_ests]\n",
        "    scalar_outputs[\"EPE\"] = [EPE_metric(disp_est, disp_gt, mask) for disp_est in disp_ests]\n",
        "    scalar_outputs[\"Thres1\"] = [Thres_metric(disp_est, disp_gt, mask, 1.0) for disp_est in disp_ests]\n",
        "    scalar_outputs[\"Thres2\"] = [Thres_metric(disp_est, disp_gt, mask, 2.0) for disp_est in disp_ests]\n",
        "    scalar_outputs[\"Thres3\"] = [Thres_metric(disp_est, disp_gt, mask, 3.0) for disp_est in disp_ests]\n",
        "\n",
        "    #if compute_metrics:\n",
        "        #image_outputs[\"errormap\"] = [disp_error_image_func.apply(disp_est, disp_gt) for disp_est in disp_ests]\n",
        "\n",
        "    return tensor2float(loss), tensor2float(scalar_outputs), image_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otKStDH765nk",
        "outputId": "039e14a0-8ca9-4d85-e9df-a7b8c623181b"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downscale epochs: [1, 2], downscale rate: 2.0\n",
            "setting learning rate to 0.001\n",
            "Epoch 0/2, Iter 0/9, train loss = 40.860\n",
            "Epoch 0/2, Iter 1/9, train loss = 30.813\n",
            "Epoch 0/2, Iter 2/9, train loss = 90.524\n",
            "Epoch 0/2, Iter 3/9, train loss = 64.898\n",
            "Epoch 0/2, Iter 4/9, train loss = 133.306\n",
            "Epoch 0/2, Iter 5/9, train loss = 19.360\n",
            "Epoch 0/2, Iter 6/9, train loss = 51.668\n",
            "Epoch 0/2, Iter 7/9, train loss = 158.464\n",
            "Epoch 0/2, Iter 8/9, train loss = 85.385\n",
            "Epoch 0/2, Iter 0/9, test loss = 22.941\n",
            "Epoch 0/2, Iter 1/9, test loss = 22.841\n",
            "Epoch 0/2, Iter 2/9, test loss = 22.840\n",
            "Epoch 0/2, Iter 3/9, test loss = 14.385\n",
            "Epoch 0/2, Iter 4/9, test loss = 13.056\n",
            "Epoch 0/2, Iter 5/9, test loss = 13.674\n",
            "Epoch 0/2, Iter 6/9, test loss = 9.964\n",
            "Epoch 0/2, Iter 7/9, test loss = 12.034\n",
            "Epoch 0/2, Iter 8/9, test loss = 13.644\n",
            "downscale epochs: [1, 2], downscale rate: 2.0\n",
            "setting learning rate to 0.0005\n",
            "Epoch 1/2, Iter 0/9, train loss = 55.692\n",
            "Epoch 1/2, Iter 1/9, train loss = 97.590\n",
            "Epoch 1/2, Iter 2/9, train loss = 94.793\n",
            "Epoch 1/2, Iter 3/9, train loss = 20.202\n",
            "Epoch 1/2, Iter 4/9, train loss = 41.386\n",
            "Epoch 1/2, Iter 5/9, train loss = 167.817\n",
            "Epoch 1/2, Iter 6/9, train loss = 73.920\n",
            "Epoch 1/2, Iter 7/9, train loss = 30.540\n",
            "Epoch 1/2, Iter 8/9, train loss = 63.703\n",
            "Epoch 1/2, Iter 0/9, test loss = 23.108\n",
            "Epoch 1/2, Iter 1/9, test loss = 23.015\n",
            "Epoch 1/2, Iter 2/9, test loss = 22.996\n",
            "Epoch 1/2, Iter 3/9, test loss = 14.621\n",
            "Epoch 1/2, Iter 4/9, test loss = 13.298\n",
            "Epoch 1/2, Iter 5/9, test loss = 13.927\n",
            "Epoch 1/2, Iter 6/9, test loss = 10.006\n",
            "Epoch 1/2, Iter 7/9, test loss = 12.075\n",
            "Epoch 1/2, Iter 8/9, test loss = 13.669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TApI7RrXKuAr"
      },
      "source": [
        "## prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp3wd5oABb5h"
      },
      "source": [
        "@make_iterative_func\n",
        "def tensor2numpy(vars):\n",
        "    if isinstance(vars, np.ndarray):\n",
        "        return vars\n",
        "    elif isinstance(vars, torch.Tensor):\n",
        "        return vars.data.cpu().numpy()\n",
        "    else:\n",
        "        raise NotImplementedError(\"invalid input type for tensor2numpy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVNHFzFKEoVF"
      },
      "source": [
        "def kitti_colormap(disparity, maxval=-1):\n",
        "    \"\"\"\n",
        "\tA utility function to reproduce KITTI fake colormap\n",
        "\tArguments:\n",
        "\t  - disparity: numpy float32 array of dimension HxW\n",
        "\t  - maxval: maximum disparity value for normalization (if equal to -1, the maximum value in disparity will be used)\n",
        "\t\n",
        "\tReturns a numpy uint8 array of shape HxWx3.\n",
        "\t\"\"\"\n",
        "    if maxval < 0:\n",
        "        maxval = np.max(disparity)\n",
        "\n",
        "    colormap = np.asarray(\n",
        "        [[0, 0, 0, 114], [0, 0, 1, 185], [1, 0, 0, 114], [1, 0, 1, 174], [0, 1, 0, 114], [0, 1, 1, 185], [1, 1, 0, 114],\n",
        "         [1, 1, 1, 0]])\n",
        "    weights = np.asarray([8.771929824561404, 5.405405405405405, 8.771929824561404, 5.747126436781609, 8.771929824561404,\n",
        "                          5.405405405405405, 8.771929824561404, 0])\n",
        "    cumsum = np.asarray([0, 0.114, 0.299, 0.413, 0.587, 0.701, 0.8859999999999999, 0.9999999999999999])\n",
        "\n",
        "    colored_disp = np.zeros([disparity.shape[0], disparity.shape[1], 3])\n",
        "    values = np.expand_dims(np.minimum(np.maximum(disparity / maxval, 0.), 1.), -1)\n",
        "    bins = np.repeat(np.repeat(np.expand_dims(np.expand_dims(cumsum, axis=0), axis=0), disparity.shape[1], axis=1),\n",
        "                     disparity.shape[0], axis=0)\n",
        "    diffs = np.where((np.repeat(values, 8, axis=-1) - bins) > 0, -1000, (np.repeat(values, 8, axis=-1) - bins))\n",
        "    index = np.argmax(diffs, axis=-1) - 1\n",
        "\n",
        "    w = 1 - (values[:, :, 0] - cumsum[index]) * np.asarray(weights)[index]\n",
        "\n",
        "    colored_disp[:, :, 2] = (w * colormap[index][:, :, 0] + (1. - w) * colormap[index + 1][:, :, 0])\n",
        "    colored_disp[:, :, 1] = (w * colormap[index][:, :, 1] + (1. - w) * colormap[index + 1][:, :, 1])\n",
        "    colored_disp[:, :, 0] = (w * colormap[index][:, :, 2] + (1. - w) * colormap[index + 1][:, :, 2])\n",
        "\n",
        "    return (colored_disp * np.expand_dims((disparity > 0), -1) * 255).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FroX3w86620"
      },
      "source": [
        "def test(colored=True):\n",
        "    print(\"Generating the disparity maps...\")\n",
        "\n",
        "    os.makedirs('./predictions_scene', exist_ok=True)\n",
        "\n",
        "    for batch_idx, sample in enumerate(TestImgLoader):\n",
        "\n",
        "\n",
        "        disp_est_tn = test_sample(sample)\n",
        "        disp_est_np = tensor2numpy(disp_est_tn)\n",
        "        top_pad_np = tensor2numpy(sample[\"top_pad\"])\n",
        "        right_pad_np = tensor2numpy(sample[\"right_pad\"])\n",
        "        left_filenames = sample[\"left_filename\"]\n",
        "\n",
        "        for disp_est, top_pad, right_pad, fn in zip(disp_est_np, top_pad_np, right_pad_np, left_filenames):\n",
        "\n",
        "            assert len(disp_est.shape) == 2\n",
        "\n",
        "            #disp_est = np.array(disp_est[top_pad:, :-right_pad], dtype=np.float32)\n",
        "\n",
        "            if disp_est != np.array([]):\n",
        "\n",
        "                name = fn.split('/')\n",
        "                fn = os.path.join(\"predictions_scene\", '_'.join(name[2:]))\n",
        "\n",
        "                if colored == True:\n",
        "                    disp_est = kitti_colormap(disp_est)\n",
        "                    cv2.imwrite(fn, disp_est)\n",
        "                else:\n",
        "                    disp_est = np.round(disp_est * 256).astype(np.uint16)\n",
        "                    io.imsave(fn, disp_est)\n",
        "\n",
        "    print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHnSX1bdK2bX"
      },
      "source": [
        "@make_nograd_func\n",
        "def test_sample(sample):\n",
        "    model.eval()\n",
        "    disp_ests = model(sample['left'].cuda(), sample['right'].cuda())\n",
        "    return disp_ests[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwJxTnpgIV2i",
        "outputId": "0390e1b5-404f-48c0-982a-e4778aa69904"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating the disparity maps...\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}