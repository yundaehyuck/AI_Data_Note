{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoTNet_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxBxSj8hpuHw"
      },
      "source": [
        "## 출처: https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/attention/CoTAttention.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F25xZus0Y2jT"
      },
      "source": [
        "## download DALI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNdscqV2Yr5h",
        "outputId": "e62819d3-95ff-48d4-c442-35fb86e91e23"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuxFaxORYs3r",
        "outputId": "eb98db3a-5f40-4860-dae2-c0afa78cdbf6"
      },
      "source": [
        "!pip install torchvision\n",
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda110"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Collecting nvidia-dali-cuda110\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.4.0-2575285-py3-none-manylinux2014_x86_64.whl (789.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 789.4 MB 8.3 kB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-dali-cuda110\n",
            "Successfully installed nvidia-dali-cuda110-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8TqhKdXY68e"
      },
      "source": [
        "## library load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyEUQnNRpzxy"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import flatten, nn\n",
        "from torch.nn import init\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "from torch.nn import functional as F\n",
        "from torchsummary import summary\n",
        "import math\n",
        "from torch import Tensor\n",
        "from torch.autograd import Function\n",
        "from torch.nn.modules.utils import _pair\n",
        "from string import Template\n",
        "import cupy\n",
        "from collections import namedtuple\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAlTtW6l0qdh"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo85ewXCZBNW"
      },
      "source": [
        "## download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ToCzpIZZDBp"
      },
      "source": [
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip \\\n",
        "  && unzip -qq flower_data.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9rYepoSZNup",
        "outputId": "689f3d4e-f6e1-4e53-f585-731ec22acb2c"
      },
      "source": [
        "import glob\n",
        "!ls ./flower_data/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train  valid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wmplgx9aTu8"
      },
      "source": [
        "from nvidia.dali.pipeline import Pipeline\n",
        "from nvidia.dali import pipeline_def\n",
        "import nvidia.dali.fn as fn\n",
        "import nvidia.dali.types as types\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoehQF_z236O"
      },
      "source": [
        "## CoT attention implementation\n",
        "## 출처:https://github.com/xmu-xiaoma666/External-Attention-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tdYKBcxphgF"
      },
      "source": [
        "class CoTAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=512,kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.dim=dim\n",
        "        self.kernel_size=kernel_size\n",
        "\n",
        "        self.key_embed=nn.Sequential(\n",
        "            nn.Conv2d(dim,dim,kernel_size=kernel_size,padding=kernel_size//2,groups=4,bias=False),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.value_embed=nn.Sequential(\n",
        "            nn.Conv2d(dim,dim,1,bias=False),\n",
        "            nn.BatchNorm2d(dim)\n",
        "        )\n",
        "\n",
        "        factor=4\n",
        "        self.attention_embed=nn.Sequential(\n",
        "            nn.Conv2d(2*dim,2*dim//factor,1,bias=False),\n",
        "            nn.BatchNorm2d(2*dim//factor),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(2*dim//factor,kernel_size*kernel_size*dim,1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,c,h,w=x.shape\n",
        "        k1=self.key_embed(x) #bs,c,h,w\n",
        "        v=self.value_embed(x).view(bs,c,-1) #bs,c,h,w\n",
        "\n",
        "        y=torch.cat([k1,x],dim=1) #bs,2c,h,w\n",
        "        att=self.attention_embed(y) #bs,c*k*k,h,w\n",
        "        att=att.reshape(bs,c,self.kernel_size*self.kernel_size,h,w)\n",
        "        att=att.mean(2,keepdim=False).view(bs,c,-1) #bs,c,h*w\n",
        "        k2=F.softmax(att,dim=-1)*v\n",
        "        k2=k2.view(bs,c,h,w)\n",
        "\n",
        "\n",
        "        return k1+k2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUfCpRutqOSr",
        "outputId": "93f1114e-bde4-487d-fc5e-cb03586ee822"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    input=torch.randn(50,512,7,7)\n",
        "    cot = CoTAttention(dim=512,kernel_size=3)\n",
        "    output=cot(input)\n",
        "    print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 512, 7, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbGPgNzu34N4"
      },
      "source": [
        "## ResNet50 implementation\n",
        "\n",
        "## 출처:https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py\n",
        "\n",
        "## 출처:https://deep-learning-study.tistory.com/534"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAxK0uvKBnBH"
      },
      "source": [
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"Residual block for resnet over 50 layers\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Ion8Rw0ICA"
      },
      "source": [
        "class Backbone(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_block, num_classes=102):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        \n",
        "        #we use a different inputsize than the original paper\n",
        "        #so conv2_x's stride is 1\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
        "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
        "        contain more than one residual block\n",
        "        Args:\n",
        "            block: block type, basic block or bottle neck block\n",
        "            out_channels: output depth channel number of this layer\n",
        "            num_blocks: how many blocks per layer\n",
        "            stride: the stride of the first block of this layer\n",
        "        Return:\n",
        "            return a resnet layer\n",
        "        \"\"\"\n",
        "\n",
        "        # we have num_block blocks per layer, the first block\n",
        "        # could be 1 or 2, other blocks would always be 1\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8-eIhLb0Xk_"
      },
      "source": [
        "def resnet50():\n",
        "    \"\"\" return a ResNet 50 object\n",
        "    \"\"\"\n",
        "    return Backbone(BottleNeck, [3, 4, 6, 3])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWt0j4dY0d0t",
        "outputId": "be3947a7-ab00-4b96-89dd-921dde864e2c"
      },
      "source": [
        "# check input\n",
        "model1 = resnet50().to(device)\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "output = model1(x)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI4iqBAj3o0V",
        "outputId": "2f6c54c9-d0d9-417d-a51e-9e34c8a74c76"
      },
      "source": [
        "summary(model1, (3, 224, 224), device=device.type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "       BottleNeck-15          [-1, 256, 56, 56]               0\n",
            "           Conv2d-16           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
            "             ReLU-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
            "             ReLU-21           [-1, 64, 56, 56]               0\n",
            "           Conv2d-22          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
            "       BottleNeck-24          [-1, 256, 56, 56]               0\n",
            "           Conv2d-25           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
            "             ReLU-27           [-1, 64, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
            "             ReLU-30           [-1, 64, 56, 56]               0\n",
            "           Conv2d-31          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-32          [-1, 256, 56, 56]             512\n",
            "       BottleNeck-33          [-1, 256, 56, 56]               0\n",
            "           Conv2d-34          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "             ReLU-36          [-1, 128, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "             ReLU-39          [-1, 128, 28, 28]               0\n",
            "           Conv2d-40          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-41          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-42          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-43          [-1, 512, 28, 28]           1,024\n",
            "       BottleNeck-44          [-1, 512, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "             ReLU-47          [-1, 128, 28, 28]               0\n",
            "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "             ReLU-50          [-1, 128, 28, 28]               0\n",
            "           Conv2d-51          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-52          [-1, 512, 28, 28]           1,024\n",
            "       BottleNeck-53          [-1, 512, 28, 28]               0\n",
            "           Conv2d-54          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
            "             ReLU-56          [-1, 128, 28, 28]               0\n",
            "           Conv2d-57          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
            "             ReLU-59          [-1, 128, 28, 28]               0\n",
            "           Conv2d-60          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-61          [-1, 512, 28, 28]           1,024\n",
            "       BottleNeck-62          [-1, 512, 28, 28]               0\n",
            "           Conv2d-63          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
            "             ReLU-65          [-1, 128, 28, 28]               0\n",
            "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
            "             ReLU-68          [-1, 128, 28, 28]               0\n",
            "           Conv2d-69          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n",
            "       BottleNeck-71          [-1, 512, 28, 28]               0\n",
            "           Conv2d-72          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-73          [-1, 256, 28, 28]             512\n",
            "             ReLU-74          [-1, 256, 28, 28]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "             ReLU-77          [-1, 256, 14, 14]               0\n",
            "           Conv2d-78         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-79         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-80         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-81         [-1, 1024, 14, 14]           2,048\n",
            "       BottleNeck-82         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-83          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "             ReLU-85          [-1, 256, 14, 14]               0\n",
            "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
            "             ReLU-88          [-1, 256, 14, 14]               0\n",
            "           Conv2d-89         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-90         [-1, 1024, 14, 14]           2,048\n",
            "       BottleNeck-91         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-92          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "             ReLU-94          [-1, 256, 14, 14]               0\n",
            "           Conv2d-95          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-96          [-1, 256, 14, 14]             512\n",
            "             ReLU-97          [-1, 256, 14, 14]               0\n",
            "           Conv2d-98         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-99         [-1, 1024, 14, 14]           2,048\n",
            "      BottleNeck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "      BottleNeck-109         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-110          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-111          [-1, 256, 14, 14]             512\n",
            "            ReLU-112          [-1, 256, 14, 14]               0\n",
            "          Conv2d-113          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-114          [-1, 256, 14, 14]             512\n",
            "            ReLU-115          [-1, 256, 14, 14]               0\n",
            "          Conv2d-116         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-117         [-1, 1024, 14, 14]           2,048\n",
            "      BottleNeck-118         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-119          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-120          [-1, 256, 14, 14]             512\n",
            "            ReLU-121          [-1, 256, 14, 14]               0\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "            ReLU-124          [-1, 256, 14, 14]               0\n",
            "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
            "      BottleNeck-127         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-128          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-129          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-130          [-1, 512, 14, 14]               0\n",
            "          Conv2d-131            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-132            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-133            [-1, 512, 7, 7]               0\n",
            "          Conv2d-134           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-135           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-136           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-137           [-1, 2048, 7, 7]           4,096\n",
            "      BottleNeck-138           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-139            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-140            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-141            [-1, 512, 7, 7]               0\n",
            "          Conv2d-142            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-143            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-144            [-1, 512, 7, 7]               0\n",
            "          Conv2d-145           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-146           [-1, 2048, 7, 7]           4,096\n",
            "      BottleNeck-147           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-148            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-149            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-150            [-1, 512, 7, 7]               0\n",
            "          Conv2d-151            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-152            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-153            [-1, 512, 7, 7]               0\n",
            "          Conv2d-154           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-155           [-1, 2048, 7, 7]           4,096\n",
            "      BottleNeck-156           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157           [-1, 2048, 1, 1]               0\n",
            "          Linear-158                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 23,528,522\n",
            "Trainable params: 23,528,522\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 244.44\n",
            "Params size (MB): 89.75\n",
            "Estimated Total Size (MB): 334.77\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Mdjg9X3DTd"
      },
      "source": [
        "## replace 3*3convolution with CoT attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsiPCqU0ima"
      },
      "source": [
        "class CoT_BottleNeck(nn.Module):\n",
        "\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            CoTAttention(out_channels,kernel_size=3),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * CoT_BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * CoT_BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if in_channels != out_channels * CoT_BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * CoT_BottleNeck.expansion, stride=1, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * CoT_BottleNeck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Q7bQR11Nxl"
      },
      "source": [
        "def cotnet50():\n",
        "    return Backbone(CoT_BottleNeck, [3, 4, 6, 3])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOp2R9Kv1aLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d518dd-910f-43d0-c5e9-24a5a225318a"
      },
      "source": [
        "#check input\n",
        "model2 = cotnet50().to(device)\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "output = model2(x)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhCBYlIR3glE",
        "outputId": "6b201b2e-4f16-4dc3-da6b-c0b40113f722"
      },
      "source": [
        "summary(model2, (3, 224, 224), device=device.type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]           9,216\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
            "           Conv2d-13           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-14           [-1, 32, 56, 56]              64\n",
            "             ReLU-15           [-1, 32, 56, 56]               0\n",
            "           Conv2d-16          [-1, 576, 56, 56]          19,008\n",
            "     CoTAttention-17           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-21          [-1, 256, 56, 56]             512\n",
            "           Conv2d-22          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
            "   CoT_BottleNeck-24          [-1, 256, 56, 56]               0\n",
            "           Conv2d-25           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
            "             ReLU-27           [-1, 64, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]           9,216\n",
            "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
            "             ReLU-30           [-1, 64, 56, 56]               0\n",
            "           Conv2d-31           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
            "           Conv2d-33           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-34           [-1, 32, 56, 56]              64\n",
            "             ReLU-35           [-1, 32, 56, 56]               0\n",
            "           Conv2d-36          [-1, 576, 56, 56]          19,008\n",
            "     CoTAttention-37           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "             ReLU-39           [-1, 64, 56, 56]               0\n",
            "           Conv2d-40          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-41          [-1, 256, 56, 56]             512\n",
            "   CoT_BottleNeck-42          [-1, 256, 56, 56]               0\n",
            "           Conv2d-43           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-44           [-1, 64, 56, 56]             128\n",
            "             ReLU-45           [-1, 64, 56, 56]               0\n",
            "           Conv2d-46           [-1, 64, 56, 56]           9,216\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "             ReLU-48           [-1, 64, 56, 56]               0\n",
            "           Conv2d-49           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-50           [-1, 64, 56, 56]             128\n",
            "           Conv2d-51           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-52           [-1, 32, 56, 56]              64\n",
            "             ReLU-53           [-1, 32, 56, 56]               0\n",
            "           Conv2d-54          [-1, 576, 56, 56]          19,008\n",
            "     CoTAttention-55           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-56           [-1, 64, 56, 56]             128\n",
            "             ReLU-57           [-1, 64, 56, 56]               0\n",
            "           Conv2d-58          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-59          [-1, 256, 56, 56]             512\n",
            "   CoT_BottleNeck-60          [-1, 256, 56, 56]               0\n",
            "           Conv2d-61          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-62          [-1, 128, 56, 56]             256\n",
            "             ReLU-63          [-1, 128, 56, 56]               0\n",
            "           Conv2d-64          [-1, 128, 56, 56]          36,864\n",
            "      BatchNorm2d-65          [-1, 128, 56, 56]             256\n",
            "             ReLU-66          [-1, 128, 56, 56]               0\n",
            "           Conv2d-67          [-1, 128, 56, 56]          16,384\n",
            "      BatchNorm2d-68          [-1, 128, 56, 56]             256\n",
            "           Conv2d-69           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-70           [-1, 64, 56, 56]             128\n",
            "             ReLU-71           [-1, 64, 56, 56]               0\n",
            "           Conv2d-72         [-1, 1152, 56, 56]          74,880\n",
            "     CoTAttention-73          [-1, 128, 56, 56]               0\n",
            "      BatchNorm2d-74          [-1, 128, 56, 56]             256\n",
            "             ReLU-75          [-1, 128, 56, 56]               0\n",
            "           Conv2d-76          [-1, 512, 56, 56]          65,536\n",
            "      BatchNorm2d-77          [-1, 512, 56, 56]           1,024\n",
            "           Conv2d-78          [-1, 512, 56, 56]         131,072\n",
            "      BatchNorm2d-79          [-1, 512, 56, 56]           1,024\n",
            "   CoT_BottleNeck-80          [-1, 512, 56, 56]               0\n",
            "           Conv2d-81          [-1, 128, 56, 56]          65,536\n",
            "      BatchNorm2d-82          [-1, 128, 56, 56]             256\n",
            "             ReLU-83          [-1, 128, 56, 56]               0\n",
            "           Conv2d-84          [-1, 128, 56, 56]          36,864\n",
            "      BatchNorm2d-85          [-1, 128, 56, 56]             256\n",
            "             ReLU-86          [-1, 128, 56, 56]               0\n",
            "           Conv2d-87          [-1, 128, 56, 56]          16,384\n",
            "      BatchNorm2d-88          [-1, 128, 56, 56]             256\n",
            "           Conv2d-89           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-90           [-1, 64, 56, 56]             128\n",
            "             ReLU-91           [-1, 64, 56, 56]               0\n",
            "           Conv2d-92         [-1, 1152, 56, 56]          74,880\n",
            "     CoTAttention-93          [-1, 128, 56, 56]               0\n",
            "      BatchNorm2d-94          [-1, 128, 56, 56]             256\n",
            "             ReLU-95          [-1, 128, 56, 56]               0\n",
            "           Conv2d-96          [-1, 512, 56, 56]          65,536\n",
            "      BatchNorm2d-97          [-1, 512, 56, 56]           1,024\n",
            "   CoT_BottleNeck-98          [-1, 512, 56, 56]               0\n",
            "           Conv2d-99          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-100          [-1, 128, 56, 56]             256\n",
            "            ReLU-101          [-1, 128, 56, 56]               0\n",
            "          Conv2d-102          [-1, 128, 56, 56]          36,864\n",
            "     BatchNorm2d-103          [-1, 128, 56, 56]             256\n",
            "            ReLU-104          [-1, 128, 56, 56]               0\n",
            "          Conv2d-105          [-1, 128, 56, 56]          16,384\n",
            "     BatchNorm2d-106          [-1, 128, 56, 56]             256\n",
            "          Conv2d-107           [-1, 64, 56, 56]          16,384\n",
            "     BatchNorm2d-108           [-1, 64, 56, 56]             128\n",
            "            ReLU-109           [-1, 64, 56, 56]               0\n",
            "          Conv2d-110         [-1, 1152, 56, 56]          74,880\n",
            "    CoTAttention-111          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-112          [-1, 128, 56, 56]             256\n",
            "            ReLU-113          [-1, 128, 56, 56]               0\n",
            "          Conv2d-114          [-1, 512, 56, 56]          65,536\n",
            "     BatchNorm2d-115          [-1, 512, 56, 56]           1,024\n",
            "  CoT_BottleNeck-116          [-1, 512, 56, 56]               0\n",
            "          Conv2d-117          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-118          [-1, 128, 56, 56]             256\n",
            "            ReLU-119          [-1, 128, 56, 56]               0\n",
            "          Conv2d-120          [-1, 128, 56, 56]          36,864\n",
            "     BatchNorm2d-121          [-1, 128, 56, 56]             256\n",
            "            ReLU-122          [-1, 128, 56, 56]               0\n",
            "          Conv2d-123          [-1, 128, 56, 56]          16,384\n",
            "     BatchNorm2d-124          [-1, 128, 56, 56]             256\n",
            "          Conv2d-125           [-1, 64, 56, 56]          16,384\n",
            "     BatchNorm2d-126           [-1, 64, 56, 56]             128\n",
            "            ReLU-127           [-1, 64, 56, 56]               0\n",
            "          Conv2d-128         [-1, 1152, 56, 56]          74,880\n",
            "    CoTAttention-129          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-130          [-1, 128, 56, 56]             256\n",
            "            ReLU-131          [-1, 128, 56, 56]               0\n",
            "          Conv2d-132          [-1, 512, 56, 56]          65,536\n",
            "     BatchNorm2d-133          [-1, 512, 56, 56]           1,024\n",
            "  CoT_BottleNeck-134          [-1, 512, 56, 56]               0\n",
            "          Conv2d-135          [-1, 256, 56, 56]         131,072\n",
            "     BatchNorm2d-136          [-1, 256, 56, 56]             512\n",
            "            ReLU-137          [-1, 256, 56, 56]               0\n",
            "          Conv2d-138          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-139          [-1, 256, 56, 56]             512\n",
            "            ReLU-140          [-1, 256, 56, 56]               0\n",
            "          Conv2d-141          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-142          [-1, 256, 56, 56]             512\n",
            "          Conv2d-143          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-144          [-1, 128, 56, 56]             256\n",
            "            ReLU-145          [-1, 128, 56, 56]               0\n",
            "          Conv2d-146         [-1, 2304, 56, 56]         297,216\n",
            "    CoTAttention-147          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-148          [-1, 256, 56, 56]             512\n",
            "            ReLU-149          [-1, 256, 56, 56]               0\n",
            "          Conv2d-150         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-151         [-1, 1024, 56, 56]           2,048\n",
            "          Conv2d-152         [-1, 1024, 56, 56]         524,288\n",
            "     BatchNorm2d-153         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_BottleNeck-154         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-155          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-156          [-1, 256, 56, 56]             512\n",
            "            ReLU-157          [-1, 256, 56, 56]               0\n",
            "          Conv2d-158          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-159          [-1, 256, 56, 56]             512\n",
            "            ReLU-160          [-1, 256, 56, 56]               0\n",
            "          Conv2d-161          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-162          [-1, 256, 56, 56]             512\n",
            "          Conv2d-163          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-164          [-1, 128, 56, 56]             256\n",
            "            ReLU-165          [-1, 128, 56, 56]               0\n",
            "          Conv2d-166         [-1, 2304, 56, 56]         297,216\n",
            "    CoTAttention-167          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-168          [-1, 256, 56, 56]             512\n",
            "            ReLU-169          [-1, 256, 56, 56]               0\n",
            "          Conv2d-170         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-171         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_BottleNeck-172         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-173          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-174          [-1, 256, 56, 56]             512\n",
            "            ReLU-175          [-1, 256, 56, 56]               0\n",
            "          Conv2d-176          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-177          [-1, 256, 56, 56]             512\n",
            "            ReLU-178          [-1, 256, 56, 56]               0\n",
            "          Conv2d-179          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-180          [-1, 256, 56, 56]             512\n",
            "          Conv2d-181          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-182          [-1, 128, 56, 56]             256\n",
            "            ReLU-183          [-1, 128, 56, 56]               0\n",
            "          Conv2d-184         [-1, 2304, 56, 56]         297,216\n",
            "    CoTAttention-185          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-186          [-1, 256, 56, 56]             512\n",
            "            ReLU-187          [-1, 256, 56, 56]               0\n",
            "          Conv2d-188         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-189         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_BottleNeck-190         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-191          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-192          [-1, 256, 56, 56]             512\n",
            "            ReLU-193          [-1, 256, 56, 56]               0\n",
            "          Conv2d-194          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-195          [-1, 256, 56, 56]             512\n",
            "            ReLU-196          [-1, 256, 56, 56]               0\n",
            "          Conv2d-197          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-198          [-1, 256, 56, 56]             512\n",
            "          Conv2d-199          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-200          [-1, 128, 56, 56]             256\n",
            "            ReLU-201          [-1, 128, 56, 56]               0\n",
            "          Conv2d-202         [-1, 2304, 56, 56]         297,216\n",
            "    CoTAttention-203          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-204          [-1, 256, 56, 56]             512\n",
            "            ReLU-205          [-1, 256, 56, 56]               0\n",
            "          Conv2d-206         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-207         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_BottleNeck-208         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-209          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-210          [-1, 256, 56, 56]             512\n",
            "            ReLU-211          [-1, 256, 56, 56]               0\n",
            "          Conv2d-212          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-213          [-1, 256, 56, 56]             512\n",
            "            ReLU-214          [-1, 256, 56, 56]               0\n",
            "          Conv2d-215          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-216          [-1, 256, 56, 56]             512\n",
            "          Conv2d-217          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-218          [-1, 128, 56, 56]             256\n",
            "            ReLU-219          [-1, 128, 56, 56]               0\n",
            "          Conv2d-220         [-1, 2304, 56, 56]         297,216\n",
            "    CoTAttention-221          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-222          [-1, 256, 56, 56]             512\n",
            "            ReLU-223          [-1, 256, 56, 56]               0\n",
            "          Conv2d-224         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-225         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_BottleNeck-226         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-227          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-228          [-1, 256, 56, 56]             512\n",
            "            ReLU-229          [-1, 256, 56, 56]               0\n",
            "          Conv2d-230          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-231          [-1, 256, 56, 56]             512\n",
            "            ReLU-232          [-1, 256, 56, 56]               0\n",
            "          Conv2d-233          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-234          [-1, 256, 56, 56]             512\n",
            "          Conv2d-235          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-236          [-1, 128, 56, 56]             256\n",
            "            ReLU-237          [-1, 128, 56, 56]               0\n",
            "          Conv2d-238         [-1, 2304, 56, 56]         297,216\n",
            "    CoTAttention-239          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-240          [-1, 256, 56, 56]             512\n",
            "            ReLU-241          [-1, 256, 56, 56]               0\n",
            "          Conv2d-242         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-243         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_BottleNeck-244         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-245          [-1, 512, 56, 56]         524,288\n",
            "     BatchNorm2d-246          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-247          [-1, 512, 56, 56]               0\n",
            "          Conv2d-248          [-1, 512, 56, 56]         589,824\n",
            "     BatchNorm2d-249          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-250          [-1, 512, 56, 56]               0\n",
            "          Conv2d-251          [-1, 512, 56, 56]         262,144\n",
            "     BatchNorm2d-252          [-1, 512, 56, 56]           1,024\n",
            "          Conv2d-253          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-254          [-1, 256, 56, 56]             512\n",
            "            ReLU-255          [-1, 256, 56, 56]               0\n",
            "          Conv2d-256         [-1, 4608, 56, 56]       1,184,256\n",
            "    CoTAttention-257          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-258          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-259          [-1, 512, 56, 56]               0\n",
            "          Conv2d-260         [-1, 2048, 56, 56]       1,048,576\n",
            "     BatchNorm2d-261         [-1, 2048, 56, 56]           4,096\n",
            "          Conv2d-262         [-1, 2048, 56, 56]       2,097,152\n",
            "     BatchNorm2d-263         [-1, 2048, 56, 56]           4,096\n",
            "  CoT_BottleNeck-264         [-1, 2048, 56, 56]               0\n",
            "          Conv2d-265          [-1, 512, 56, 56]       1,048,576\n",
            "     BatchNorm2d-266          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-267          [-1, 512, 56, 56]               0\n",
            "          Conv2d-268          [-1, 512, 56, 56]         589,824\n",
            "     BatchNorm2d-269          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-270          [-1, 512, 56, 56]               0\n",
            "          Conv2d-271          [-1, 512, 56, 56]         262,144\n",
            "     BatchNorm2d-272          [-1, 512, 56, 56]           1,024\n",
            "          Conv2d-273          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-274          [-1, 256, 56, 56]             512\n",
            "            ReLU-275          [-1, 256, 56, 56]               0\n",
            "          Conv2d-276         [-1, 4608, 56, 56]       1,184,256\n",
            "    CoTAttention-277          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-278          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-279          [-1, 512, 56, 56]               0\n",
            "          Conv2d-280         [-1, 2048, 56, 56]       1,048,576\n",
            "     BatchNorm2d-281         [-1, 2048, 56, 56]           4,096\n",
            "  CoT_BottleNeck-282         [-1, 2048, 56, 56]               0\n",
            "          Conv2d-283          [-1, 512, 56, 56]       1,048,576\n",
            "     BatchNorm2d-284          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-285          [-1, 512, 56, 56]               0\n",
            "          Conv2d-286          [-1, 512, 56, 56]         589,824\n",
            "     BatchNorm2d-287          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-288          [-1, 512, 56, 56]               0\n",
            "          Conv2d-289          [-1, 512, 56, 56]         262,144\n",
            "     BatchNorm2d-290          [-1, 512, 56, 56]           1,024\n",
            "          Conv2d-291          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-292          [-1, 256, 56, 56]             512\n",
            "            ReLU-293          [-1, 256, 56, 56]               0\n",
            "          Conv2d-294         [-1, 4608, 56, 56]       1,184,256\n",
            "    CoTAttention-295          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-296          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-297          [-1, 512, 56, 56]               0\n",
            "          Conv2d-298         [-1, 2048, 56, 56]       1,048,576\n",
            "     BatchNorm2d-299         [-1, 2048, 56, 56]           4,096\n",
            "  CoT_BottleNeck-300         [-1, 2048, 56, 56]               0\n",
            "AdaptiveAvgPool2d-301           [-1, 2048, 1, 1]               0\n",
            "          Linear-302                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 23,267,018\n",
            "Trainable params: 23,267,018\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 3230.19\n",
            "Params size (MB): 88.76\n",
            "Estimated Total Size (MB): 3319.52\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7ajLEUo2mhS"
      },
      "source": [
        "## official implementation\n",
        "## 출처:https://github.com/JDAI-CV/CoTNet/blob/master/models/cotnet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jvGW3FW0Hs3"
      },
      "source": [
        "## prepare requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGF3YtQsDWxH"
      },
      "source": [
        "CUDA_NUM_THREADS = 1024\n",
        "\n",
        "kernel_loop = '''\n",
        "#define CUDA_KERNEL_LOOP(i, n)                        \\\n",
        "  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
        "      i < (n);                                       \\\n",
        "      i += blockDim.x * gridDim.x)\n",
        "'''\n",
        "\n",
        "def GET_BLOCKS(N):\n",
        "    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\n",
        "\n",
        "_aggregation_zeropad_forward_kernel = kernel_loop + '''\n",
        "extern \"C\"\n",
        "__global__ void aggregation_zeropad_forward_kernel(\n",
        "const ${Dtype}* bottom_data, const ${Dtype}* weight_data, ${Dtype}* top_data) {\n",
        "  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
        "    const int n = index / ${weight_heads} / ${input_channels} / ${top_height} / ${top_width};\n",
        "    const int head = (index / ${top_width} / ${top_height} / ${input_channels}) % ${weight_heads};\n",
        "    const int c = (index / ${top_width} / ${top_height}) % ${input_channels};\n",
        "    const int h = (index / ${top_width}) % ${top_height};\n",
        "    const int w = index % ${top_width};\n",
        "    ${Dtype} value = 0;\n",
        "    for (int kh = 0; kh < ${kernel_h}; ++kh) {\n",
        "      for (int kw = 0; kw < ${kernel_w}; ++kw) {\n",
        "        const int h_in = -${pad_h} + h * ${stride_h} + kh * ${dilation_h};\n",
        "        const int w_in = -${pad_w} + w * ${stride_w} + kw * ${dilation_w};\n",
        "        if ((h_in >= 0) && (h_in < ${bottom_height}) && (w_in >= 0) && (w_in < ${bottom_width})) {\n",
        "          const int offset_bottom = ((n * ${input_channels} + c) * ${bottom_height} + h_in) * ${bottom_width} + w_in;\n",
        "          const int offset_weight = (((n * ${weight_heads} + head) * ${weight_channels} + c % ${weight_channels}) * ${kernel_h} * ${kernel_w} + (kh * ${kernel_w} + kw)) * ${top_height} * ${top_width} + h * ${top_width} + w;\n",
        "          value += weight_data[offset_weight] * bottom_data[offset_bottom];\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    top_data[index] = value;\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "_aggregation_zeropad_input_backward_kernel = kernel_loop + '''\n",
        "extern \"C\"\n",
        "__global__ void aggregation_zeropad_input_backward_kernel(\n",
        "    const ${Dtype}* const top_diff, const ${Dtype}* const weight_data, ${Dtype}* bottom_diff) {\n",
        "  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
        "    const int n = index / ${input_channels} / ${bottom_height} / ${bottom_width};\n",
        "    const int c = (index / ${bottom_height} / ${bottom_width}) % ${input_channels};\n",
        "    const int h = (index / ${bottom_width}) % ${bottom_height};\n",
        "    const int w = index % ${bottom_width};\n",
        "    ${Dtype} value = 0;\n",
        "    for (int head = 0; head < ${weight_heads}; ++head) {\n",
        "        for (int kh = 0; kh < ${kernel_h}; ++kh) {\n",
        "          for (int kw = 0; kw < ${kernel_w}; ++kw) {\n",
        "            const int h_out_s = h + ${pad_h} - kh * ${dilation_h};\n",
        "            const int w_out_s = w + ${pad_w} - kw * ${dilation_w};\n",
        "            if (((h_out_s % ${stride_h}) == 0) && ((w_out_s % ${stride_w}) == 0)) {\n",
        "              const int h_out = h_out_s / ${stride_h};\n",
        "              const int w_out = w_out_s / ${stride_w};\n",
        "              if ((h_out >= 0) && (h_out < ${top_height}) && (w_out >= 0) && (w_out < ${top_width})) {\n",
        "                const int offset_top = (((n * ${weight_heads} + head) * ${input_channels} + c) * ${top_height} + h_out) * ${top_width} + w_out;\n",
        "                const int offset_weight = (((n * ${weight_heads} + head) * ${weight_channels} + c % ${weight_channels}) * ${kernel_h} * ${kernel_w} + (kh * ${kernel_w} + kw)) * ${top_height} * ${top_width} + h_out * ${top_width} + w_out;\n",
        "                value += weight_data[offset_weight] * top_diff[offset_top];\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    }\n",
        "    bottom_diff[index] = value;\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "_aggregation_zeropad_weight_backward_kernel = kernel_loop + '''\n",
        "extern \"C\"\n",
        "__global__ void aggregation_zeropad_weight_backward_kernel(\n",
        "    const ${Dtype}* const top_diff, const ${Dtype}* const bottom_data, ${Dtype}* weight_diff) {\n",
        "  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
        "    const int n = index / ${weight_heads} / ${weight_channels} / ${top_height} / ${top_width};\n",
        "    const int head = (index / ${top_width} / ${top_height} / ${weight_channels}) % ${weight_heads};\n",
        "    const int c = (index / ${top_width} / ${top_height}) % ${weight_channels};\n",
        "    const int h = (index / ${top_width}) % ${top_height};\n",
        "    const int w = index % ${top_width};\n",
        "    for (int kh = 0; kh < ${kernel_h}; ++kh) {\n",
        "      for (int kw = 0; kw < ${kernel_w}; ++kw) {\n",
        "        const int h_in = -${pad_h} + h * ${stride_h} + kh * ${dilation_h};\n",
        "        const int w_in = -${pad_w} + w * ${stride_w} + kw * ${dilation_w};\n",
        "        const int offset_weight = (((n * ${weight_heads} + head) * ${weight_channels} + c) * ${kernel_h} * ${kernel_w} + (kh * ${kernel_w} + kw)) * ${top_height} * ${top_width} + h * ${top_width} + w;\n",
        "        ${Dtype} value = 0;\n",
        "        if ((h_in >= 0) && (h_in < ${bottom_height}) && (w_in >= 0) && (w_in < ${bottom_width})) {\n",
        "          for (int cc = c; cc < ${input_channels}; cc += ${weight_channels}) {\n",
        "            const int offset_bottom = ((n * ${input_channels} + cc) * ${bottom_height} + h_in) * ${bottom_width} + w_in;\n",
        "            const int offset_top = (((n * ${weight_heads} + head) * ${input_channels} + cc) * ${top_height} + h) * ${top_width} + w;\n",
        "            value += bottom_data[offset_bottom] * top_diff[offset_top];\n",
        "          }\n",
        "        }\n",
        "        weight_diff[offset_weight] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9c8HDkGDgk7"
      },
      "source": [
        "Stream = namedtuple('Stream', ['ptr'])\n",
        "\n",
        "def Dtype(t):\n",
        "    if isinstance(t, torch.cuda.FloatTensor):\n",
        "        return 'float'\n",
        "    elif isinstance(t, torch.cuda.DoubleTensor):\n",
        "        return 'double'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZyLC2TvC5xa"
      },
      "source": [
        "@cupy.memoize(for_each_device=True)\n",
        "def load_kernel(kernel_name, code, **kwargs):\n",
        "    code = Template(code).substitute(**kwargs)\n",
        "    kernel_code = cupy.cuda.compile_with_cache(code)\n",
        "    return kernel_code.get_function(kernel_name)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMlAj3vB0wO"
      },
      "source": [
        "class AggregationZeropad(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, kernel_size, stride, padding, dilation):\n",
        "        kernel_size, stride, padding, dilation = _pair(kernel_size), _pair(stride), _pair(padding), _pair(dilation)\n",
        "        ctx.kernel_size, ctx.stride, ctx.padding, ctx.dilation = kernel_size, stride, padding, dilation\n",
        "        assert input.dim() == 4 and input.is_cuda and weight.is_cuda\n",
        "        batch_size, input_channels, input_height, input_width = input.size()\n",
        "        _, weight_heads, weight_channels, weight_kernels, weight_height, weight_width = weight.size()\n",
        "        output_height = int((input_height + 2 * padding[0] - (dilation[0] * (kernel_size[0] - 1) + 1)) / stride[0] + 1)\n",
        "        output_width = int((input_width + 2 * padding[1] - (dilation[1] * (kernel_size[1] - 1) + 1)) / stride[1] + 1)\n",
        "        assert output_height * output_width == weight_height * weight_width\n",
        "        output = input.new(batch_size, weight_heads * input_channels, output_height, output_width)\n",
        "        n = output.numel()\n",
        "        if not input.is_contiguous():\n",
        "            input = input.detach().clone()\n",
        "        if not weight.is_contiguous():\n",
        "            weight = weight.detach().clone()\n",
        "\n",
        "        with torch.cuda.device_of(input):\n",
        "            f = load_kernel('aggregation_zeropad_forward_kernel', _aggregation_zeropad_forward_kernel, Dtype=Dtype(input), nthreads=n,\n",
        "                            num=batch_size, input_channels=input_channels, \n",
        "                            weight_heads=weight_heads, weight_channels=weight_channels,\n",
        "                            bottom_height=input_height, bottom_width=input_width,\n",
        "                            top_height=output_height, top_width=output_width,\n",
        "                            kernel_h=kernel_size[0], kernel_w=kernel_size[1],\n",
        "                            stride_h=stride[0], stride_w=stride[1],\n",
        "                            dilation_h=dilation[0], dilation_w=dilation[1],\n",
        "                            pad_h=padding[0], pad_w=padding[1])\n",
        "            f(block=(CUDA_NUM_THREADS, 1, 1),\n",
        "              grid=(GET_BLOCKS(n), 1, 1),\n",
        "              args=[input.data_ptr(), weight.data_ptr(), output.data_ptr()],\n",
        "              stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        kernel_size, stride, padding, dilation = ctx.kernel_size, ctx.stride, ctx.padding, ctx.dilation\n",
        "        input, weight = ctx.saved_tensors\n",
        "        assert grad_output.is_cuda\n",
        "        if not grad_output.is_contiguous():\n",
        "            grad_output = grad_output.contiguous()\n",
        "        batch_size, input_channels, input_height, input_width = input.size()\n",
        "        _, weight_heads, weight_channels, weight_kernels, weight_height, weight_width = weight.size()\n",
        "        output_height, output_width = grad_output.size()[2:]\n",
        "        grad_input, grad_weight = None, None\n",
        "        opt = dict(Dtype=Dtype(grad_output),\n",
        "                   num=batch_size, input_channels=input_channels, \n",
        "                   weight_heads=weight_heads, weight_channels=weight_channels,\n",
        "                   bottom_height=input_height, bottom_width=input_width,\n",
        "                   top_height=output_height, top_width=output_width,\n",
        "                   kernel_h=kernel_size[0], kernel_w=kernel_size[1],\n",
        "                   stride_h=stride[0], stride_w=stride[1],\n",
        "                   dilation_h=dilation[0], dilation_w=dilation[1],\n",
        "                   pad_h=padding[0], pad_w=padding[1])\n",
        "        with torch.cuda.device_of(input):\n",
        "            if ctx.needs_input_grad[0]:\n",
        "                grad_input = input.new(input.size())\n",
        "                n = grad_input.numel()\n",
        "                opt['nthreads'] = n\n",
        "                f = load_kernel('aggregation_zeropad_input_backward_kernel', _aggregation_zeropad_input_backward_kernel, **opt)\n",
        "                f(block=(CUDA_NUM_THREADS, 1, 1),\n",
        "                  grid=(GET_BLOCKS(n), 1, 1),\n",
        "                  args=[grad_output.data_ptr(), weight.data_ptr(), grad_input.data_ptr()],\n",
        "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
        "            if ctx.needs_input_grad[1]:\n",
        "                grad_weight = weight.new(weight.size())\n",
        "                n = grad_weight.numel() // weight.shape[3]\n",
        "                opt['nthreads'] = n\n",
        "                f = load_kernel('aggregation_zeropad_weight_backward_kernel', _aggregation_zeropad_weight_backward_kernel, **opt)\n",
        "                f(block=(CUDA_NUM_THREADS, 1, 1),\n",
        "                  grid=(GET_BLOCKS(n), 1, 1),\n",
        "                  args=[grad_output.data_ptr(), input.data_ptr(), grad_weight.data_ptr()],\n",
        "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
        "        return grad_input, grad_weight, None, None, None, None\n",
        "\n",
        "def aggregation_zeropad(input, weight, kernel_size=3, stride=1, padding=0, dilation=1):\n",
        "    assert input.shape[0] == weight.shape[0] and (input.shape[1] % weight.shape[2] == 0)\n",
        "    if input.is_cuda:\n",
        "        out = AggregationZeropad.apply(input, weight, kernel_size, stride, padding, dilation)\n",
        "    else:\n",
        "        #raise NotImplementedError\n",
        "        out = AggregationZeropad.apply(input.cuda(), weight.cuda(), kernel_size, stride, padding, dilation)\n",
        "        torch.cuda.synchronize()\n",
        "        out = out.cpu()\n",
        "    return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo7_zmGg5I7C"
      },
      "source": [
        "class LocalConvolution(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        stride: int = 1,\n",
        "        padding: int = 0,\n",
        "        dilation: int = 1,\n",
        "        pad_mode: int = 0,\n",
        "    ):\n",
        "        super(LocalConvolution, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.pad_mode = pad_mode\n",
        "\n",
        "    def forward(self, input: Tensor, weight: Tensor):\n",
        "        #if self.pad_mode == 0:\n",
        "        out = aggregation_zeropad(\n",
        "            input, \n",
        "            weight, \n",
        "            kernel_size=self.kernel_size, \n",
        "            stride=self.stride, \n",
        "            padding=self.padding, \n",
        "            dilation=self.dilation)\n",
        "        #else:\n",
        "        #  out = aggregation_refpad(\n",
        "        #    input, \n",
        "        #    weight, \n",
        "        #    kernel_size=self.kernel_size, \n",
        "        #    stride=self.stride, \n",
        "        #    padding=self.padding, \n",
        "        #    dilation=self.dilation)  \n",
        "        return out"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmcdeT9N_vNX"
      },
      "source": [
        "def swish(x, inplace: bool = False):\n",
        "    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n",
        "    \"\"\"\n",
        "    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self, inplace: bool = False):\n",
        "        super(Swish, self).__init__()\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        return swish(x, self.inplace)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3tph5MS0MKC"
      },
      "source": [
        "## define CoT Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba2P4LM31_29"
      },
      "source": [
        "class CotLayer(nn.Module):\n",
        "    def __init__(self, dim, kernel_size):\n",
        "        super(CotLayer, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.key_embed = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, self.kernel_size, stride=1, padding=self.kernel_size//2, groups=4, bias=False),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        share_planes = 8\n",
        "        factor = 2\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv2d(2*dim, dim//factor, 1, bias=False),\n",
        "            nn.BatchNorm2d(dim//factor),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(dim//factor, pow(kernel_size, 2) * dim // share_planes, kernel_size=1),\n",
        "            nn.GroupNorm(num_groups=dim // share_planes, num_channels=pow(kernel_size, 2) * dim // share_planes)\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, dilation=1, bias=False),\n",
        "            nn.BatchNorm2d(dim)\n",
        "        )\n",
        "\n",
        "        self.local_conv = LocalConvolution(dim, dim, kernel_size=self.kernel_size, stride=1, padding=(self.kernel_size - 1) // 2, dilation=1)\n",
        "        self.bn = nn.BatchNorm2d(dim)\n",
        "        self.act = Swish(inplace=True)\n",
        "\n",
        "        reduction_factor = 4\n",
        "        self.radix = 2\n",
        "        attn_chs = max(dim * self.radix // reduction_factor, 32)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.Conv2d(dim, attn_chs, 1),\n",
        "            nn.BatchNorm2d(attn_chs),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(attn_chs, self.radix*dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.key_embed(x)\n",
        "        qk = torch.cat([x, k], dim=1)\n",
        "        b, c, qk_hh, qk_ww = qk.size()\n",
        "\n",
        "        w = self.embed(qk)\n",
        "        w = w.view(b, 1, -1, self.kernel_size*self.kernel_size, qk_hh, qk_ww)\n",
        "        \n",
        "        x = self.conv1x1(x)\n",
        "        x = self.local_conv(x, w)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, 1, H, W)\n",
        "        k = k.view(B, C, 1, H, W)\n",
        "        x = torch.cat([x, k], dim=2)\n",
        "\n",
        "        x_gap = x.sum(dim=2)\n",
        "        x_gap = x_gap.mean((2, 3), keepdim=True)\n",
        "        x_attn = self.se(x_gap)\n",
        "        x_attn = x_attn.view(B, C, self.radix)\n",
        "        x_attn = F.softmax(x_attn, dim=2)\n",
        "        out = (x * x_attn.reshape((B, C, self.radix, 1, 1))).sum(dim=2)\n",
        "        \n",
        "        return out.contiguous()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Rs-V5M0bVV"
      },
      "source": [
        "## define bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR9IgQbHAWMp"
      },
      "source": [
        "#using ResNet bottleneck\n",
        "\n",
        "class CoT_Bottleneck(nn.Module):\n",
        "\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            CotLayer(out_channels,kernel_size=3),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * CoT_Bottleneck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * CoT_Bottleneck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if in_channels != out_channels * CoT_Bottleneck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * CoT_Bottleneck.expansion, stride=1, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * CoT_Bottleneck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdGHcPjH1dPZ"
      },
      "source": [
        "# official bottleneck\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, cardinality=1, base_width=64,\n",
        "                 reduce_first=1, dilation=1, first_dilation=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,\n",
        "                 attn_layer=None, aa_layer=None, drop_block=None, drop_path=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        width = int(math.floor(planes * (base_width / 64)) * cardinality)\n",
        "        first_planes = width // reduce_first\n",
        "        outplanes = planes * self.expansion\n",
        "        first_dilation = first_dilation or dilation\n",
        "        use_aa = aa_layer is not None and (stride == 2 or first_dilation != dilation)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, first_planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(first_planes)\n",
        "        self.act1 = act_layer(inplace=True)\n",
        "\n",
        "        if stride > 1:\n",
        "            self.avd = nn.AvgPool2d(3, 2, padding=1)\n",
        "        else:\n",
        "            self.avd = None\n",
        "        \n",
        "        self.conv2 = CotLayer(width, kernel_size=3) if cardinality == 1 else CoXtLayer(width, kernel_size=3)\n",
        "\n",
        "        #self.conv2 = nn.Conv2d(\n",
        "        #    first_planes, width, kernel_size=3, stride=1 if use_aa else stride,\n",
        "        #    padding=first_dilation, dilation=first_dilation, groups=cardinality, bias=False)\n",
        "        #self.bn2 = norm_layer(width)\n",
        "        #self.act2 = act_layer(inplace=True)\n",
        "        #self.aa = aa_layer(channels=width, stride=stride) if use_aa else None\n",
        "\n",
        "        self.conv3 = nn.Conv2d(width, outplanes, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(outplanes)\n",
        "\n",
        "        #self.se = create_attn(attn_layer, outplanes)\n",
        "        self.se = None\n",
        "\n",
        "        self.act3 = act_layer(inplace=True)\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or inplanes != outplanes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, outplanes, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(outplanes)\n",
        "            )\n",
        "\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.drop_block = drop_block\n",
        "        self.drop_path = drop_path\n",
        "\n",
        "    def zero_init_last_bn(self):\n",
        "        nn.init.zeros_(self.bn3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        if self.avd is not None:\n",
        "            x = self.avd(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        #x = self.bn2(x)\n",
        "        #if self.drop_block is not None:\n",
        "        #    x = self.drop_block(x)\n",
        "        #x = self.act2(x)\n",
        "        #if self.aa is not None:\n",
        "        #    x = self.aa(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "\n",
        "        if self.se is not None:\n",
        "            x = self.se(x)\n",
        "\n",
        "        if self.drop_path is not None:\n",
        "            x = self.drop_path(x)\n",
        "\n",
        "        residual = self.shortcut(residual)\n",
        "\n",
        "        x += residual\n",
        "        x = self.act3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUJ_p8Dq2J4L"
      },
      "source": [
        "def cotnet50v2(): #Resnet bottleneck + official CoTlayer\n",
        "    return Backbone(CoT_Bottleneck, [3, 4, 6, 3])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK1SaO6mH7jq"
      },
      "source": [
        "def cotnet50v3(): #official CoTNet50\n",
        "    return Backbone(Bottleneck, [3, 4, 6, 3])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whJCz0oI2dz0",
        "outputId": "d3cfe088-d0f4-4c70-97b9-ffaaaf1e5ebf"
      },
      "source": [
        "# check input\n",
        "\n",
        "model3 = cotnet50v2().to(device)\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "output = model3(x)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qRPLvuL2hOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32abb3a3-5246-4b1d-d3cb-4f699cf0f3e1"
      },
      "source": [
        "# check summary\n",
        "summary(model3, (3, 224, 224), device=device.type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]           9,216\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-12           [-1, 32, 56, 56]              64\n",
            "             ReLU-13           [-1, 32, 56, 56]               0\n",
            "           Conv2d-14           [-1, 72, 56, 56]           2,376\n",
            "        GroupNorm-15           [-1, 72, 56, 56]             144\n",
            "           Conv2d-16           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
            " LocalConvolution-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "            Swish-20           [-1, 64, 56, 56]               0\n",
            "           Conv2d-21             [-1, 32, 1, 1]           2,080\n",
            "      BatchNorm2d-22             [-1, 32, 1, 1]              64\n",
            "             ReLU-23             [-1, 32, 1, 1]               0\n",
            "           Conv2d-24            [-1, 128, 1, 1]           4,224\n",
            "         CotLayer-25           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
            "             ReLU-27           [-1, 64, 56, 56]               0\n",
            "           Conv2d-28          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-29          [-1, 256, 56, 56]             512\n",
            "           Conv2d-30          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-31          [-1, 256, 56, 56]             512\n",
            "   CoT_Bottleneck-32          [-1, 256, 56, 56]               0\n",
            "           Conv2d-33           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-34           [-1, 64, 56, 56]             128\n",
            "             ReLU-35           [-1, 64, 56, 56]               0\n",
            "           Conv2d-36           [-1, 64, 56, 56]           9,216\n",
            "      BatchNorm2d-37           [-1, 64, 56, 56]             128\n",
            "             ReLU-38           [-1, 64, 56, 56]               0\n",
            "           Conv2d-39           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-40           [-1, 32, 56, 56]              64\n",
            "             ReLU-41           [-1, 32, 56, 56]               0\n",
            "           Conv2d-42           [-1, 72, 56, 56]           2,376\n",
            "        GroupNorm-43           [-1, 72, 56, 56]             144\n",
            "           Conv2d-44           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-45           [-1, 64, 56, 56]             128\n",
            " LocalConvolution-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "            Swish-48           [-1, 64, 56, 56]               0\n",
            "           Conv2d-49             [-1, 32, 1, 1]           2,080\n",
            "      BatchNorm2d-50             [-1, 32, 1, 1]              64\n",
            "             ReLU-51             [-1, 32, 1, 1]               0\n",
            "           Conv2d-52            [-1, 128, 1, 1]           4,224\n",
            "         CotLayer-53           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-54           [-1, 64, 56, 56]             128\n",
            "             ReLU-55           [-1, 64, 56, 56]               0\n",
            "           Conv2d-56          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-57          [-1, 256, 56, 56]             512\n",
            "   CoT_Bottleneck-58          [-1, 256, 56, 56]               0\n",
            "           Conv2d-59           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-60           [-1, 64, 56, 56]             128\n",
            "             ReLU-61           [-1, 64, 56, 56]               0\n",
            "           Conv2d-62           [-1, 64, 56, 56]           9,216\n",
            "      BatchNorm2d-63           [-1, 64, 56, 56]             128\n",
            "             ReLU-64           [-1, 64, 56, 56]               0\n",
            "           Conv2d-65           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-66           [-1, 32, 56, 56]              64\n",
            "             ReLU-67           [-1, 32, 56, 56]               0\n",
            "           Conv2d-68           [-1, 72, 56, 56]           2,376\n",
            "        GroupNorm-69           [-1, 72, 56, 56]             144\n",
            "           Conv2d-70           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-71           [-1, 64, 56, 56]             128\n",
            " LocalConvolution-72           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-73           [-1, 64, 56, 56]             128\n",
            "            Swish-74           [-1, 64, 56, 56]               0\n",
            "           Conv2d-75             [-1, 32, 1, 1]           2,080\n",
            "      BatchNorm2d-76             [-1, 32, 1, 1]              64\n",
            "             ReLU-77             [-1, 32, 1, 1]               0\n",
            "           Conv2d-78            [-1, 128, 1, 1]           4,224\n",
            "         CotLayer-79           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-80           [-1, 64, 56, 56]             128\n",
            "             ReLU-81           [-1, 64, 56, 56]               0\n",
            "           Conv2d-82          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-83          [-1, 256, 56, 56]             512\n",
            "   CoT_Bottleneck-84          [-1, 256, 56, 56]               0\n",
            "           Conv2d-85          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-86          [-1, 128, 56, 56]             256\n",
            "             ReLU-87          [-1, 128, 56, 56]               0\n",
            "           Conv2d-88          [-1, 128, 56, 56]          36,864\n",
            "      BatchNorm2d-89          [-1, 128, 56, 56]             256\n",
            "             ReLU-90          [-1, 128, 56, 56]               0\n",
            "           Conv2d-91           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-92           [-1, 64, 56, 56]             128\n",
            "             ReLU-93           [-1, 64, 56, 56]               0\n",
            "           Conv2d-94          [-1, 144, 56, 56]           9,360\n",
            "        GroupNorm-95          [-1, 144, 56, 56]             288\n",
            "           Conv2d-96          [-1, 128, 56, 56]          16,384\n",
            "      BatchNorm2d-97          [-1, 128, 56, 56]             256\n",
            " LocalConvolution-98          [-1, 128, 56, 56]               0\n",
            "      BatchNorm2d-99          [-1, 128, 56, 56]             256\n",
            "           Swish-100          [-1, 128, 56, 56]               0\n",
            "          Conv2d-101             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-102             [-1, 64, 1, 1]             128\n",
            "            ReLU-103             [-1, 64, 1, 1]               0\n",
            "          Conv2d-104            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-105          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-106          [-1, 128, 56, 56]             256\n",
            "            ReLU-107          [-1, 128, 56, 56]               0\n",
            "          Conv2d-108          [-1, 512, 56, 56]          65,536\n",
            "     BatchNorm2d-109          [-1, 512, 56, 56]           1,024\n",
            "          Conv2d-110          [-1, 512, 56, 56]         131,072\n",
            "     BatchNorm2d-111          [-1, 512, 56, 56]           1,024\n",
            "  CoT_Bottleneck-112          [-1, 512, 56, 56]               0\n",
            "          Conv2d-113          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-114          [-1, 128, 56, 56]             256\n",
            "            ReLU-115          [-1, 128, 56, 56]               0\n",
            "          Conv2d-116          [-1, 128, 56, 56]          36,864\n",
            "     BatchNorm2d-117          [-1, 128, 56, 56]             256\n",
            "            ReLU-118          [-1, 128, 56, 56]               0\n",
            "          Conv2d-119           [-1, 64, 56, 56]          16,384\n",
            "     BatchNorm2d-120           [-1, 64, 56, 56]             128\n",
            "            ReLU-121           [-1, 64, 56, 56]               0\n",
            "          Conv2d-122          [-1, 144, 56, 56]           9,360\n",
            "       GroupNorm-123          [-1, 144, 56, 56]             288\n",
            "          Conv2d-124          [-1, 128, 56, 56]          16,384\n",
            "     BatchNorm2d-125          [-1, 128, 56, 56]             256\n",
            "LocalConvolution-126          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-127          [-1, 128, 56, 56]             256\n",
            "           Swish-128          [-1, 128, 56, 56]               0\n",
            "          Conv2d-129             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-130             [-1, 64, 1, 1]             128\n",
            "            ReLU-131             [-1, 64, 1, 1]               0\n",
            "          Conv2d-132            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-133          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-134          [-1, 128, 56, 56]             256\n",
            "            ReLU-135          [-1, 128, 56, 56]               0\n",
            "          Conv2d-136          [-1, 512, 56, 56]          65,536\n",
            "     BatchNorm2d-137          [-1, 512, 56, 56]           1,024\n",
            "  CoT_Bottleneck-138          [-1, 512, 56, 56]               0\n",
            "          Conv2d-139          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-140          [-1, 128, 56, 56]             256\n",
            "            ReLU-141          [-1, 128, 56, 56]               0\n",
            "          Conv2d-142          [-1, 128, 56, 56]          36,864\n",
            "     BatchNorm2d-143          [-1, 128, 56, 56]             256\n",
            "            ReLU-144          [-1, 128, 56, 56]               0\n",
            "          Conv2d-145           [-1, 64, 56, 56]          16,384\n",
            "     BatchNorm2d-146           [-1, 64, 56, 56]             128\n",
            "            ReLU-147           [-1, 64, 56, 56]               0\n",
            "          Conv2d-148          [-1, 144, 56, 56]           9,360\n",
            "       GroupNorm-149          [-1, 144, 56, 56]             288\n",
            "          Conv2d-150          [-1, 128, 56, 56]          16,384\n",
            "     BatchNorm2d-151          [-1, 128, 56, 56]             256\n",
            "LocalConvolution-152          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-153          [-1, 128, 56, 56]             256\n",
            "           Swish-154          [-1, 128, 56, 56]               0\n",
            "          Conv2d-155             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-156             [-1, 64, 1, 1]             128\n",
            "            ReLU-157             [-1, 64, 1, 1]               0\n",
            "          Conv2d-158            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-159          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-160          [-1, 128, 56, 56]             256\n",
            "            ReLU-161          [-1, 128, 56, 56]               0\n",
            "          Conv2d-162          [-1, 512, 56, 56]          65,536\n",
            "     BatchNorm2d-163          [-1, 512, 56, 56]           1,024\n",
            "  CoT_Bottleneck-164          [-1, 512, 56, 56]               0\n",
            "          Conv2d-165          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-166          [-1, 128, 56, 56]             256\n",
            "            ReLU-167          [-1, 128, 56, 56]               0\n",
            "          Conv2d-168          [-1, 128, 56, 56]          36,864\n",
            "     BatchNorm2d-169          [-1, 128, 56, 56]             256\n",
            "            ReLU-170          [-1, 128, 56, 56]               0\n",
            "          Conv2d-171           [-1, 64, 56, 56]          16,384\n",
            "     BatchNorm2d-172           [-1, 64, 56, 56]             128\n",
            "            ReLU-173           [-1, 64, 56, 56]               0\n",
            "          Conv2d-174          [-1, 144, 56, 56]           9,360\n",
            "       GroupNorm-175          [-1, 144, 56, 56]             288\n",
            "          Conv2d-176          [-1, 128, 56, 56]          16,384\n",
            "     BatchNorm2d-177          [-1, 128, 56, 56]             256\n",
            "LocalConvolution-178          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-179          [-1, 128, 56, 56]             256\n",
            "           Swish-180          [-1, 128, 56, 56]               0\n",
            "          Conv2d-181             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-182             [-1, 64, 1, 1]             128\n",
            "            ReLU-183             [-1, 64, 1, 1]               0\n",
            "          Conv2d-184            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-185          [-1, 128, 56, 56]               0\n",
            "     BatchNorm2d-186          [-1, 128, 56, 56]             256\n",
            "            ReLU-187          [-1, 128, 56, 56]               0\n",
            "          Conv2d-188          [-1, 512, 56, 56]          65,536\n",
            "     BatchNorm2d-189          [-1, 512, 56, 56]           1,024\n",
            "  CoT_Bottleneck-190          [-1, 512, 56, 56]               0\n",
            "          Conv2d-191          [-1, 256, 56, 56]         131,072\n",
            "     BatchNorm2d-192          [-1, 256, 56, 56]             512\n",
            "            ReLU-193          [-1, 256, 56, 56]               0\n",
            "          Conv2d-194          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-195          [-1, 256, 56, 56]             512\n",
            "            ReLU-196          [-1, 256, 56, 56]               0\n",
            "          Conv2d-197          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-198          [-1, 128, 56, 56]             256\n",
            "            ReLU-199          [-1, 128, 56, 56]               0\n",
            "          Conv2d-200          [-1, 288, 56, 56]          37,152\n",
            "       GroupNorm-201          [-1, 288, 56, 56]             576\n",
            "          Conv2d-202          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-203          [-1, 256, 56, 56]             512\n",
            "LocalConvolution-204          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-205          [-1, 256, 56, 56]             512\n",
            "           Swish-206          [-1, 256, 56, 56]               0\n",
            "          Conv2d-207            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-208            [-1, 128, 1, 1]             256\n",
            "            ReLU-209            [-1, 128, 1, 1]               0\n",
            "          Conv2d-210            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-211          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-212          [-1, 256, 56, 56]             512\n",
            "            ReLU-213          [-1, 256, 56, 56]               0\n",
            "          Conv2d-214         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-215         [-1, 1024, 56, 56]           2,048\n",
            "          Conv2d-216         [-1, 1024, 56, 56]         524,288\n",
            "     BatchNorm2d-217         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_Bottleneck-218         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-219          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-220          [-1, 256, 56, 56]             512\n",
            "            ReLU-221          [-1, 256, 56, 56]               0\n",
            "          Conv2d-222          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-223          [-1, 256, 56, 56]             512\n",
            "            ReLU-224          [-1, 256, 56, 56]               0\n",
            "          Conv2d-225          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-226          [-1, 128, 56, 56]             256\n",
            "            ReLU-227          [-1, 128, 56, 56]               0\n",
            "          Conv2d-228          [-1, 288, 56, 56]          37,152\n",
            "       GroupNorm-229          [-1, 288, 56, 56]             576\n",
            "          Conv2d-230          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-231          [-1, 256, 56, 56]             512\n",
            "LocalConvolution-232          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-233          [-1, 256, 56, 56]             512\n",
            "           Swish-234          [-1, 256, 56, 56]               0\n",
            "          Conv2d-235            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-236            [-1, 128, 1, 1]             256\n",
            "            ReLU-237            [-1, 128, 1, 1]               0\n",
            "          Conv2d-238            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-239          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-240          [-1, 256, 56, 56]             512\n",
            "            ReLU-241          [-1, 256, 56, 56]               0\n",
            "          Conv2d-242         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-243         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_Bottleneck-244         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-245          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-246          [-1, 256, 56, 56]             512\n",
            "            ReLU-247          [-1, 256, 56, 56]               0\n",
            "          Conv2d-248          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-249          [-1, 256, 56, 56]             512\n",
            "            ReLU-250          [-1, 256, 56, 56]               0\n",
            "          Conv2d-251          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-252          [-1, 128, 56, 56]             256\n",
            "            ReLU-253          [-1, 128, 56, 56]               0\n",
            "          Conv2d-254          [-1, 288, 56, 56]          37,152\n",
            "       GroupNorm-255          [-1, 288, 56, 56]             576\n",
            "          Conv2d-256          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-257          [-1, 256, 56, 56]             512\n",
            "LocalConvolution-258          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-259          [-1, 256, 56, 56]             512\n",
            "           Swish-260          [-1, 256, 56, 56]               0\n",
            "          Conv2d-261            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-262            [-1, 128, 1, 1]             256\n",
            "            ReLU-263            [-1, 128, 1, 1]               0\n",
            "          Conv2d-264            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-265          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-266          [-1, 256, 56, 56]             512\n",
            "            ReLU-267          [-1, 256, 56, 56]               0\n",
            "          Conv2d-268         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-269         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_Bottleneck-270         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-271          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-272          [-1, 256, 56, 56]             512\n",
            "            ReLU-273          [-1, 256, 56, 56]               0\n",
            "          Conv2d-274          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-275          [-1, 256, 56, 56]             512\n",
            "            ReLU-276          [-1, 256, 56, 56]               0\n",
            "          Conv2d-277          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-278          [-1, 128, 56, 56]             256\n",
            "            ReLU-279          [-1, 128, 56, 56]               0\n",
            "          Conv2d-280          [-1, 288, 56, 56]          37,152\n",
            "       GroupNorm-281          [-1, 288, 56, 56]             576\n",
            "          Conv2d-282          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-283          [-1, 256, 56, 56]             512\n",
            "LocalConvolution-284          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-285          [-1, 256, 56, 56]             512\n",
            "           Swish-286          [-1, 256, 56, 56]               0\n",
            "          Conv2d-287            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-288            [-1, 128, 1, 1]             256\n",
            "            ReLU-289            [-1, 128, 1, 1]               0\n",
            "          Conv2d-290            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-291          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-292          [-1, 256, 56, 56]             512\n",
            "            ReLU-293          [-1, 256, 56, 56]               0\n",
            "          Conv2d-294         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-295         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_Bottleneck-296         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-297          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-298          [-1, 256, 56, 56]             512\n",
            "            ReLU-299          [-1, 256, 56, 56]               0\n",
            "          Conv2d-300          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-301          [-1, 256, 56, 56]             512\n",
            "            ReLU-302          [-1, 256, 56, 56]               0\n",
            "          Conv2d-303          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-304          [-1, 128, 56, 56]             256\n",
            "            ReLU-305          [-1, 128, 56, 56]               0\n",
            "          Conv2d-306          [-1, 288, 56, 56]          37,152\n",
            "       GroupNorm-307          [-1, 288, 56, 56]             576\n",
            "          Conv2d-308          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-309          [-1, 256, 56, 56]             512\n",
            "LocalConvolution-310          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-311          [-1, 256, 56, 56]             512\n",
            "           Swish-312          [-1, 256, 56, 56]               0\n",
            "          Conv2d-313            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-314            [-1, 128, 1, 1]             256\n",
            "            ReLU-315            [-1, 128, 1, 1]               0\n",
            "          Conv2d-316            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-317          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-318          [-1, 256, 56, 56]             512\n",
            "            ReLU-319          [-1, 256, 56, 56]               0\n",
            "          Conv2d-320         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-321         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_Bottleneck-322         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-323          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-324          [-1, 256, 56, 56]             512\n",
            "            ReLU-325          [-1, 256, 56, 56]               0\n",
            "          Conv2d-326          [-1, 256, 56, 56]         147,456\n",
            "     BatchNorm2d-327          [-1, 256, 56, 56]             512\n",
            "            ReLU-328          [-1, 256, 56, 56]               0\n",
            "          Conv2d-329          [-1, 128, 56, 56]          65,536\n",
            "     BatchNorm2d-330          [-1, 128, 56, 56]             256\n",
            "            ReLU-331          [-1, 128, 56, 56]               0\n",
            "          Conv2d-332          [-1, 288, 56, 56]          37,152\n",
            "       GroupNorm-333          [-1, 288, 56, 56]             576\n",
            "          Conv2d-334          [-1, 256, 56, 56]          65,536\n",
            "     BatchNorm2d-335          [-1, 256, 56, 56]             512\n",
            "LocalConvolution-336          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-337          [-1, 256, 56, 56]             512\n",
            "           Swish-338          [-1, 256, 56, 56]               0\n",
            "          Conv2d-339            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-340            [-1, 128, 1, 1]             256\n",
            "            ReLU-341            [-1, 128, 1, 1]               0\n",
            "          Conv2d-342            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-343          [-1, 256, 56, 56]               0\n",
            "     BatchNorm2d-344          [-1, 256, 56, 56]             512\n",
            "            ReLU-345          [-1, 256, 56, 56]               0\n",
            "          Conv2d-346         [-1, 1024, 56, 56]         262,144\n",
            "     BatchNorm2d-347         [-1, 1024, 56, 56]           2,048\n",
            "  CoT_Bottleneck-348         [-1, 1024, 56, 56]               0\n",
            "          Conv2d-349          [-1, 512, 56, 56]         524,288\n",
            "     BatchNorm2d-350          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-351          [-1, 512, 56, 56]               0\n",
            "          Conv2d-352          [-1, 512, 56, 56]         589,824\n",
            "     BatchNorm2d-353          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-354          [-1, 512, 56, 56]               0\n",
            "          Conv2d-355          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-356          [-1, 256, 56, 56]             512\n",
            "            ReLU-357          [-1, 256, 56, 56]               0\n",
            "          Conv2d-358          [-1, 576, 56, 56]         148,032\n",
            "       GroupNorm-359          [-1, 576, 56, 56]           1,152\n",
            "          Conv2d-360          [-1, 512, 56, 56]         262,144\n",
            "     BatchNorm2d-361          [-1, 512, 56, 56]           1,024\n",
            "LocalConvolution-362          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-363          [-1, 512, 56, 56]           1,024\n",
            "           Swish-364          [-1, 512, 56, 56]               0\n",
            "          Conv2d-365            [-1, 256, 1, 1]         131,328\n",
            "     BatchNorm2d-366            [-1, 256, 1, 1]             512\n",
            "            ReLU-367            [-1, 256, 1, 1]               0\n",
            "          Conv2d-368           [-1, 1024, 1, 1]         263,168\n",
            "        CotLayer-369          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-370          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-371          [-1, 512, 56, 56]               0\n",
            "          Conv2d-372         [-1, 2048, 56, 56]       1,048,576\n",
            "     BatchNorm2d-373         [-1, 2048, 56, 56]           4,096\n",
            "          Conv2d-374         [-1, 2048, 56, 56]       2,097,152\n",
            "     BatchNorm2d-375         [-1, 2048, 56, 56]           4,096\n",
            "  CoT_Bottleneck-376         [-1, 2048, 56, 56]               0\n",
            "          Conv2d-377          [-1, 512, 56, 56]       1,048,576\n",
            "     BatchNorm2d-378          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-379          [-1, 512, 56, 56]               0\n",
            "          Conv2d-380          [-1, 512, 56, 56]         589,824\n",
            "     BatchNorm2d-381          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-382          [-1, 512, 56, 56]               0\n",
            "          Conv2d-383          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-384          [-1, 256, 56, 56]             512\n",
            "            ReLU-385          [-1, 256, 56, 56]               0\n",
            "          Conv2d-386          [-1, 576, 56, 56]         148,032\n",
            "       GroupNorm-387          [-1, 576, 56, 56]           1,152\n",
            "          Conv2d-388          [-1, 512, 56, 56]         262,144\n",
            "     BatchNorm2d-389          [-1, 512, 56, 56]           1,024\n",
            "LocalConvolution-390          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-391          [-1, 512, 56, 56]           1,024\n",
            "           Swish-392          [-1, 512, 56, 56]               0\n",
            "          Conv2d-393            [-1, 256, 1, 1]         131,328\n",
            "     BatchNorm2d-394            [-1, 256, 1, 1]             512\n",
            "            ReLU-395            [-1, 256, 1, 1]               0\n",
            "          Conv2d-396           [-1, 1024, 1, 1]         263,168\n",
            "        CotLayer-397          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-398          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-399          [-1, 512, 56, 56]               0\n",
            "          Conv2d-400         [-1, 2048, 56, 56]       1,048,576\n",
            "     BatchNorm2d-401         [-1, 2048, 56, 56]           4,096\n",
            "  CoT_Bottleneck-402         [-1, 2048, 56, 56]               0\n",
            "          Conv2d-403          [-1, 512, 56, 56]       1,048,576\n",
            "     BatchNorm2d-404          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-405          [-1, 512, 56, 56]               0\n",
            "          Conv2d-406          [-1, 512, 56, 56]         589,824\n",
            "     BatchNorm2d-407          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-408          [-1, 512, 56, 56]               0\n",
            "          Conv2d-409          [-1, 256, 56, 56]         262,144\n",
            "     BatchNorm2d-410          [-1, 256, 56, 56]             512\n",
            "            ReLU-411          [-1, 256, 56, 56]               0\n",
            "          Conv2d-412          [-1, 576, 56, 56]         148,032\n",
            "       GroupNorm-413          [-1, 576, 56, 56]           1,152\n",
            "          Conv2d-414          [-1, 512, 56, 56]         262,144\n",
            "     BatchNorm2d-415          [-1, 512, 56, 56]           1,024\n",
            "LocalConvolution-416          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-417          [-1, 512, 56, 56]           1,024\n",
            "           Swish-418          [-1, 512, 56, 56]               0\n",
            "          Conv2d-419            [-1, 256, 1, 1]         131,328\n",
            "     BatchNorm2d-420            [-1, 256, 1, 1]             512\n",
            "            ReLU-421            [-1, 256, 1, 1]               0\n",
            "          Conv2d-422           [-1, 1024, 1, 1]         263,168\n",
            "        CotLayer-423          [-1, 512, 56, 56]               0\n",
            "     BatchNorm2d-424          [-1, 512, 56, 56]           1,024\n",
            "            ReLU-425          [-1, 512, 56, 56]               0\n",
            "          Conv2d-426         [-1, 2048, 56, 56]       1,048,576\n",
            "     BatchNorm2d-427         [-1, 2048, 56, 56]           4,096\n",
            "  CoT_Bottleneck-428         [-1, 2048, 56, 56]               0\n",
            "AdaptiveAvgPool2d-429           [-1, 2048, 1, 1]               0\n",
            "          Linear-430                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 20,201,458\n",
            "Trainable params: 20,201,458\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 2891.50\n",
            "Params size (MB): 77.06\n",
            "Estimated Total Size (MB): 2969.14\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKXMNnYnH-OL",
        "outputId": "aa2a64c9-0447-4cb9-d5cf-45a521e112ea"
      },
      "source": [
        "# check input\n",
        "\n",
        "model4 = cotnet50v3().to(device)\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "output = model4(x)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DypyWLwlHbit",
        "outputId": "a0b9ecdb-f491-42e7-e259-eb9d3af179e6"
      },
      "source": [
        "# check summary\n",
        "summary(model4, (3, 224, 224), device=device.type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]           9,216\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-12           [-1, 32, 56, 56]              64\n",
            "             ReLU-13           [-1, 32, 56, 56]               0\n",
            "           Conv2d-14           [-1, 72, 56, 56]           2,376\n",
            "        GroupNorm-15           [-1, 72, 56, 56]             144\n",
            "           Conv2d-16           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
            " LocalConvolution-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "            Swish-20           [-1, 64, 56, 56]               0\n",
            "           Conv2d-21             [-1, 32, 1, 1]           2,080\n",
            "      BatchNorm2d-22             [-1, 32, 1, 1]              64\n",
            "             ReLU-23             [-1, 32, 1, 1]               0\n",
            "           Conv2d-24            [-1, 128, 1, 1]           4,224\n",
            "         CotLayer-25           [-1, 64, 56, 56]               0\n",
            "           Conv2d-26          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-27          [-1, 256, 56, 56]             512\n",
            "           Conv2d-28          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-29          [-1, 256, 56, 56]             512\n",
            "             ReLU-30          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-31          [-1, 256, 56, 56]               0\n",
            "           Conv2d-32           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "             ReLU-34           [-1, 64, 56, 56]               0\n",
            "           Conv2d-35           [-1, 64, 56, 56]           9,216\n",
            "      BatchNorm2d-36           [-1, 64, 56, 56]             128\n",
            "             ReLU-37           [-1, 64, 56, 56]               0\n",
            "           Conv2d-38           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-39           [-1, 32, 56, 56]              64\n",
            "             ReLU-40           [-1, 32, 56, 56]               0\n",
            "           Conv2d-41           [-1, 72, 56, 56]           2,376\n",
            "        GroupNorm-42           [-1, 72, 56, 56]             144\n",
            "           Conv2d-43           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-44           [-1, 64, 56, 56]             128\n",
            " LocalConvolution-45           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-46           [-1, 64, 56, 56]             128\n",
            "            Swish-47           [-1, 64, 56, 56]               0\n",
            "           Conv2d-48             [-1, 32, 1, 1]           2,080\n",
            "      BatchNorm2d-49             [-1, 32, 1, 1]              64\n",
            "             ReLU-50             [-1, 32, 1, 1]               0\n",
            "           Conv2d-51            [-1, 128, 1, 1]           4,224\n",
            "         CotLayer-52           [-1, 64, 56, 56]               0\n",
            "           Conv2d-53          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-54          [-1, 256, 56, 56]             512\n",
            "             ReLU-55          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-56          [-1, 256, 56, 56]               0\n",
            "           Conv2d-57           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-58           [-1, 64, 56, 56]             128\n",
            "             ReLU-59           [-1, 64, 56, 56]               0\n",
            "           Conv2d-60           [-1, 64, 56, 56]           9,216\n",
            "      BatchNorm2d-61           [-1, 64, 56, 56]             128\n",
            "             ReLU-62           [-1, 64, 56, 56]               0\n",
            "           Conv2d-63           [-1, 32, 56, 56]           4,096\n",
            "      BatchNorm2d-64           [-1, 32, 56, 56]              64\n",
            "             ReLU-65           [-1, 32, 56, 56]               0\n",
            "           Conv2d-66           [-1, 72, 56, 56]           2,376\n",
            "        GroupNorm-67           [-1, 72, 56, 56]             144\n",
            "           Conv2d-68           [-1, 64, 56, 56]           4,096\n",
            "      BatchNorm2d-69           [-1, 64, 56, 56]             128\n",
            " LocalConvolution-70           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-71           [-1, 64, 56, 56]             128\n",
            "            Swish-72           [-1, 64, 56, 56]               0\n",
            "           Conv2d-73             [-1, 32, 1, 1]           2,080\n",
            "      BatchNorm2d-74             [-1, 32, 1, 1]              64\n",
            "             ReLU-75             [-1, 32, 1, 1]               0\n",
            "           Conv2d-76            [-1, 128, 1, 1]           4,224\n",
            "         CotLayer-77           [-1, 64, 56, 56]               0\n",
            "           Conv2d-78          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-79          [-1, 256, 56, 56]             512\n",
            "             ReLU-80          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-81          [-1, 256, 56, 56]               0\n",
            "           Conv2d-82          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-83          [-1, 128, 56, 56]             256\n",
            "             ReLU-84          [-1, 128, 56, 56]               0\n",
            "        AvgPool2d-85          [-1, 128, 28, 28]               0\n",
            "           Conv2d-86          [-1, 128, 28, 28]          36,864\n",
            "      BatchNorm2d-87          [-1, 128, 28, 28]             256\n",
            "             ReLU-88          [-1, 128, 28, 28]               0\n",
            "           Conv2d-89           [-1, 64, 28, 28]          16,384\n",
            "      BatchNorm2d-90           [-1, 64, 28, 28]             128\n",
            "             ReLU-91           [-1, 64, 28, 28]               0\n",
            "           Conv2d-92          [-1, 144, 28, 28]           9,360\n",
            "        GroupNorm-93          [-1, 144, 28, 28]             288\n",
            "           Conv2d-94          [-1, 128, 28, 28]          16,384\n",
            "      BatchNorm2d-95          [-1, 128, 28, 28]             256\n",
            " LocalConvolution-96          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-97          [-1, 128, 28, 28]             256\n",
            "            Swish-98          [-1, 128, 28, 28]               0\n",
            "           Conv2d-99             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-100             [-1, 64, 1, 1]             128\n",
            "            ReLU-101             [-1, 64, 1, 1]               0\n",
            "          Conv2d-102            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-103          [-1, 128, 28, 28]               0\n",
            "          Conv2d-104          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-105          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-106          [-1, 512, 28, 28]         131,072\n",
            "     BatchNorm2d-107          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-108          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-109          [-1, 512, 28, 28]               0\n",
            "          Conv2d-110          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-111          [-1, 128, 28, 28]             256\n",
            "            ReLU-112          [-1, 128, 28, 28]               0\n",
            "          Conv2d-113          [-1, 128, 28, 28]          36,864\n",
            "     BatchNorm2d-114          [-1, 128, 28, 28]             256\n",
            "            ReLU-115          [-1, 128, 28, 28]               0\n",
            "          Conv2d-116           [-1, 64, 28, 28]          16,384\n",
            "     BatchNorm2d-117           [-1, 64, 28, 28]             128\n",
            "            ReLU-118           [-1, 64, 28, 28]               0\n",
            "          Conv2d-119          [-1, 144, 28, 28]           9,360\n",
            "       GroupNorm-120          [-1, 144, 28, 28]             288\n",
            "          Conv2d-121          [-1, 128, 28, 28]          16,384\n",
            "     BatchNorm2d-122          [-1, 128, 28, 28]             256\n",
            "LocalConvolution-123          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-124          [-1, 128, 28, 28]             256\n",
            "           Swish-125          [-1, 128, 28, 28]               0\n",
            "          Conv2d-126             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-127             [-1, 64, 1, 1]             128\n",
            "            ReLU-128             [-1, 64, 1, 1]               0\n",
            "          Conv2d-129            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-130          [-1, 128, 28, 28]               0\n",
            "          Conv2d-131          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-132          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-133          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-134          [-1, 512, 28, 28]               0\n",
            "          Conv2d-135          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-136          [-1, 128, 28, 28]             256\n",
            "            ReLU-137          [-1, 128, 28, 28]               0\n",
            "          Conv2d-138          [-1, 128, 28, 28]          36,864\n",
            "     BatchNorm2d-139          [-1, 128, 28, 28]             256\n",
            "            ReLU-140          [-1, 128, 28, 28]               0\n",
            "          Conv2d-141           [-1, 64, 28, 28]          16,384\n",
            "     BatchNorm2d-142           [-1, 64, 28, 28]             128\n",
            "            ReLU-143           [-1, 64, 28, 28]               0\n",
            "          Conv2d-144          [-1, 144, 28, 28]           9,360\n",
            "       GroupNorm-145          [-1, 144, 28, 28]             288\n",
            "          Conv2d-146          [-1, 128, 28, 28]          16,384\n",
            "     BatchNorm2d-147          [-1, 128, 28, 28]             256\n",
            "LocalConvolution-148          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-149          [-1, 128, 28, 28]             256\n",
            "           Swish-150          [-1, 128, 28, 28]               0\n",
            "          Conv2d-151             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-152             [-1, 64, 1, 1]             128\n",
            "            ReLU-153             [-1, 64, 1, 1]               0\n",
            "          Conv2d-154            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-155          [-1, 128, 28, 28]               0\n",
            "          Conv2d-156          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-157          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-158          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-159          [-1, 512, 28, 28]               0\n",
            "          Conv2d-160          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-161          [-1, 128, 28, 28]             256\n",
            "            ReLU-162          [-1, 128, 28, 28]               0\n",
            "          Conv2d-163          [-1, 128, 28, 28]          36,864\n",
            "     BatchNorm2d-164          [-1, 128, 28, 28]             256\n",
            "            ReLU-165          [-1, 128, 28, 28]               0\n",
            "          Conv2d-166           [-1, 64, 28, 28]          16,384\n",
            "     BatchNorm2d-167           [-1, 64, 28, 28]             128\n",
            "            ReLU-168           [-1, 64, 28, 28]               0\n",
            "          Conv2d-169          [-1, 144, 28, 28]           9,360\n",
            "       GroupNorm-170          [-1, 144, 28, 28]             288\n",
            "          Conv2d-171          [-1, 128, 28, 28]          16,384\n",
            "     BatchNorm2d-172          [-1, 128, 28, 28]             256\n",
            "LocalConvolution-173          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-174          [-1, 128, 28, 28]             256\n",
            "           Swish-175          [-1, 128, 28, 28]               0\n",
            "          Conv2d-176             [-1, 64, 1, 1]           8,256\n",
            "     BatchNorm2d-177             [-1, 64, 1, 1]             128\n",
            "            ReLU-178             [-1, 64, 1, 1]               0\n",
            "          Conv2d-179            [-1, 256, 1, 1]          16,640\n",
            "        CotLayer-180          [-1, 128, 28, 28]               0\n",
            "          Conv2d-181          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-182          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-183          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-184          [-1, 512, 28, 28]               0\n",
            "          Conv2d-185          [-1, 256, 28, 28]         131,072\n",
            "     BatchNorm2d-186          [-1, 256, 28, 28]             512\n",
            "            ReLU-187          [-1, 256, 28, 28]               0\n",
            "       AvgPool2d-188          [-1, 256, 14, 14]               0\n",
            "          Conv2d-189          [-1, 256, 14, 14]         147,456\n",
            "     BatchNorm2d-190          [-1, 256, 14, 14]             512\n",
            "            ReLU-191          [-1, 256, 14, 14]               0\n",
            "          Conv2d-192          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-193          [-1, 128, 14, 14]             256\n",
            "            ReLU-194          [-1, 128, 14, 14]               0\n",
            "          Conv2d-195          [-1, 288, 14, 14]          37,152\n",
            "       GroupNorm-196          [-1, 288, 14, 14]             576\n",
            "          Conv2d-197          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-198          [-1, 256, 14, 14]             512\n",
            "LocalConvolution-199          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-200          [-1, 256, 14, 14]             512\n",
            "           Swish-201          [-1, 256, 14, 14]               0\n",
            "          Conv2d-202            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-203            [-1, 128, 1, 1]             256\n",
            "            ReLU-204            [-1, 128, 1, 1]               0\n",
            "          Conv2d-205            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-206          [-1, 256, 14, 14]               0\n",
            "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-209         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-210         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-211         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-212         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-213          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-214          [-1, 256, 14, 14]             512\n",
            "            ReLU-215          [-1, 256, 14, 14]               0\n",
            "          Conv2d-216          [-1, 256, 14, 14]         147,456\n",
            "     BatchNorm2d-217          [-1, 256, 14, 14]             512\n",
            "            ReLU-218          [-1, 256, 14, 14]               0\n",
            "          Conv2d-219          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-220          [-1, 128, 14, 14]             256\n",
            "            ReLU-221          [-1, 128, 14, 14]               0\n",
            "          Conv2d-222          [-1, 288, 14, 14]          37,152\n",
            "       GroupNorm-223          [-1, 288, 14, 14]             576\n",
            "          Conv2d-224          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
            "LocalConvolution-226          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-227          [-1, 256, 14, 14]             512\n",
            "           Swish-228          [-1, 256, 14, 14]               0\n",
            "          Conv2d-229            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-230            [-1, 128, 1, 1]             256\n",
            "            ReLU-231            [-1, 128, 1, 1]               0\n",
            "          Conv2d-232            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-233          [-1, 256, 14, 14]               0\n",
            "          Conv2d-234         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-235         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-236         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-237         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-238          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-239          [-1, 256, 14, 14]             512\n",
            "            ReLU-240          [-1, 256, 14, 14]               0\n",
            "          Conv2d-241          [-1, 256, 14, 14]         147,456\n",
            "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
            "            ReLU-243          [-1, 256, 14, 14]               0\n",
            "          Conv2d-244          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-245          [-1, 128, 14, 14]             256\n",
            "            ReLU-246          [-1, 128, 14, 14]               0\n",
            "          Conv2d-247          [-1, 288, 14, 14]          37,152\n",
            "       GroupNorm-248          [-1, 288, 14, 14]             576\n",
            "          Conv2d-249          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-250          [-1, 256, 14, 14]             512\n",
            "LocalConvolution-251          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
            "           Swish-253          [-1, 256, 14, 14]               0\n",
            "          Conv2d-254            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-255            [-1, 128, 1, 1]             256\n",
            "            ReLU-256            [-1, 128, 1, 1]               0\n",
            "          Conv2d-257            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-258          [-1, 256, 14, 14]               0\n",
            "          Conv2d-259         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-260         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-261         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-262         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-263          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-264          [-1, 256, 14, 14]             512\n",
            "            ReLU-265          [-1, 256, 14, 14]               0\n",
            "          Conv2d-266          [-1, 256, 14, 14]         147,456\n",
            "     BatchNorm2d-267          [-1, 256, 14, 14]             512\n",
            "            ReLU-268          [-1, 256, 14, 14]               0\n",
            "          Conv2d-269          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-270          [-1, 128, 14, 14]             256\n",
            "            ReLU-271          [-1, 128, 14, 14]               0\n",
            "          Conv2d-272          [-1, 288, 14, 14]          37,152\n",
            "       GroupNorm-273          [-1, 288, 14, 14]             576\n",
            "          Conv2d-274          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
            "LocalConvolution-276          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-277          [-1, 256, 14, 14]             512\n",
            "           Swish-278          [-1, 256, 14, 14]               0\n",
            "          Conv2d-279            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-280            [-1, 128, 1, 1]             256\n",
            "            ReLU-281            [-1, 128, 1, 1]               0\n",
            "          Conv2d-282            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-283          [-1, 256, 14, 14]               0\n",
            "          Conv2d-284         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-285         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-286         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-287         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-288          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-289          [-1, 256, 14, 14]             512\n",
            "            ReLU-290          [-1, 256, 14, 14]               0\n",
            "          Conv2d-291          [-1, 256, 14, 14]         147,456\n",
            "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
            "            ReLU-293          [-1, 256, 14, 14]               0\n",
            "          Conv2d-294          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-295          [-1, 128, 14, 14]             256\n",
            "            ReLU-296          [-1, 128, 14, 14]               0\n",
            "          Conv2d-297          [-1, 288, 14, 14]          37,152\n",
            "       GroupNorm-298          [-1, 288, 14, 14]             576\n",
            "          Conv2d-299          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-300          [-1, 256, 14, 14]             512\n",
            "LocalConvolution-301          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
            "           Swish-303          [-1, 256, 14, 14]               0\n",
            "          Conv2d-304            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-305            [-1, 128, 1, 1]             256\n",
            "            ReLU-306            [-1, 128, 1, 1]               0\n",
            "          Conv2d-307            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-308          [-1, 256, 14, 14]               0\n",
            "          Conv2d-309         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-310         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-311         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-312         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-313          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-314          [-1, 256, 14, 14]             512\n",
            "            ReLU-315          [-1, 256, 14, 14]               0\n",
            "          Conv2d-316          [-1, 256, 14, 14]         147,456\n",
            "     BatchNorm2d-317          [-1, 256, 14, 14]             512\n",
            "            ReLU-318          [-1, 256, 14, 14]               0\n",
            "          Conv2d-319          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-320          [-1, 128, 14, 14]             256\n",
            "            ReLU-321          [-1, 128, 14, 14]               0\n",
            "          Conv2d-322          [-1, 288, 14, 14]          37,152\n",
            "       GroupNorm-323          [-1, 288, 14, 14]             576\n",
            "          Conv2d-324          [-1, 256, 14, 14]          65,536\n",
            "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
            "LocalConvolution-326          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-327          [-1, 256, 14, 14]             512\n",
            "           Swish-328          [-1, 256, 14, 14]               0\n",
            "          Conv2d-329            [-1, 128, 1, 1]          32,896\n",
            "     BatchNorm2d-330            [-1, 128, 1, 1]             256\n",
            "            ReLU-331            [-1, 128, 1, 1]               0\n",
            "          Conv2d-332            [-1, 512, 1, 1]          66,048\n",
            "        CotLayer-333          [-1, 256, 14, 14]               0\n",
            "          Conv2d-334         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-335         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-336         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-337         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-338          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-339          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-340          [-1, 512, 14, 14]               0\n",
            "       AvgPool2d-341            [-1, 512, 7, 7]               0\n",
            "          Conv2d-342            [-1, 512, 7, 7]         589,824\n",
            "     BatchNorm2d-343            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-344            [-1, 512, 7, 7]               0\n",
            "          Conv2d-345            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-346            [-1, 256, 7, 7]             512\n",
            "            ReLU-347            [-1, 256, 7, 7]               0\n",
            "          Conv2d-348            [-1, 576, 7, 7]         148,032\n",
            "       GroupNorm-349            [-1, 576, 7, 7]           1,152\n",
            "          Conv2d-350            [-1, 512, 7, 7]         262,144\n",
            "     BatchNorm2d-351            [-1, 512, 7, 7]           1,024\n",
            "LocalConvolution-352            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-353            [-1, 512, 7, 7]           1,024\n",
            "           Swish-354            [-1, 512, 7, 7]               0\n",
            "          Conv2d-355            [-1, 256, 1, 1]         131,328\n",
            "     BatchNorm2d-356            [-1, 256, 1, 1]             512\n",
            "            ReLU-357            [-1, 256, 1, 1]               0\n",
            "          Conv2d-358           [-1, 1024, 1, 1]         263,168\n",
            "        CotLayer-359            [-1, 512, 7, 7]               0\n",
            "          Conv2d-360           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-361           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-362           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-363           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-364           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-365           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-366            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-367            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-368            [-1, 512, 7, 7]               0\n",
            "          Conv2d-369            [-1, 512, 7, 7]         589,824\n",
            "     BatchNorm2d-370            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-371            [-1, 512, 7, 7]               0\n",
            "          Conv2d-372            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-373            [-1, 256, 7, 7]             512\n",
            "            ReLU-374            [-1, 256, 7, 7]               0\n",
            "          Conv2d-375            [-1, 576, 7, 7]         148,032\n",
            "       GroupNorm-376            [-1, 576, 7, 7]           1,152\n",
            "          Conv2d-377            [-1, 512, 7, 7]         262,144\n",
            "     BatchNorm2d-378            [-1, 512, 7, 7]           1,024\n",
            "LocalConvolution-379            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-380            [-1, 512, 7, 7]           1,024\n",
            "           Swish-381            [-1, 512, 7, 7]               0\n",
            "          Conv2d-382            [-1, 256, 1, 1]         131,328\n",
            "     BatchNorm2d-383            [-1, 256, 1, 1]             512\n",
            "            ReLU-384            [-1, 256, 1, 1]               0\n",
            "          Conv2d-385           [-1, 1024, 1, 1]         263,168\n",
            "        CotLayer-386            [-1, 512, 7, 7]               0\n",
            "          Conv2d-387           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-388           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-389           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-390           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-391            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-392            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-393            [-1, 512, 7, 7]               0\n",
            "          Conv2d-394            [-1, 512, 7, 7]         589,824\n",
            "     BatchNorm2d-395            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-396            [-1, 512, 7, 7]               0\n",
            "          Conv2d-397            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-398            [-1, 256, 7, 7]             512\n",
            "            ReLU-399            [-1, 256, 7, 7]               0\n",
            "          Conv2d-400            [-1, 576, 7, 7]         148,032\n",
            "       GroupNorm-401            [-1, 576, 7, 7]           1,152\n",
            "          Conv2d-402            [-1, 512, 7, 7]         262,144\n",
            "     BatchNorm2d-403            [-1, 512, 7, 7]           1,024\n",
            "LocalConvolution-404            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-405            [-1, 512, 7, 7]           1,024\n",
            "           Swish-406            [-1, 512, 7, 7]               0\n",
            "          Conv2d-407            [-1, 256, 1, 1]         131,328\n",
            "     BatchNorm2d-408            [-1, 256, 1, 1]             512\n",
            "            ReLU-409            [-1, 256, 1, 1]               0\n",
            "          Conv2d-410           [-1, 1024, 1, 1]         263,168\n",
            "        CotLayer-411            [-1, 512, 7, 7]               0\n",
            "          Conv2d-412           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-413           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-414           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-415           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-416           [-1, 2048, 1, 1]               0\n",
            "          Linear-417                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 20,193,906\n",
            "Trainable params: 20,193,906\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 390.63\n",
            "Params size (MB): 77.03\n",
            "Estimated Total Size (MB): 468.24\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_5OHPiGZq2J"
      },
      "source": [
        "## train using DALI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDcCZ3aaJkpz"
      },
      "source": [
        "# define model\n",
        "\n",
        "model1 = resnet50().to(device) #ResNet50\n",
        "model2 = cotnet50v3().to(device) #official CoTNet50"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0wmrg43aEzs"
      },
      "source": [
        "# define DALI pipeline\n",
        "\n",
        "@pipeline_def\n",
        "def dali_mixed_pipeline():\n",
        "    image_dir = \"./flower_data/train\"\n",
        "    jpegs, labels = fn.readers.file(name=\"Reader\", file_root=image_dir, random_shuffle=True)\n",
        "    images = fn.decoders.image(jpegs, device=\"mixed\")\n",
        "    images= fn.normalize(images.gpu(), mean=0, stddev=255)  # same as divide by 255\n",
        "    images = fn.resize(images.gpu(), resize_x=224, resize_y=224)\n",
        "    return images, labels.gpu()\n",
        "\n",
        "torch_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Resize((224, 224)),\n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br3JIyuIaoOG"
      },
      "source": [
        "#build pipeline\n",
        "\n",
        "batch_size=4\n",
        "num_threads=2\n",
        "pipe = dali_mixed_pipeline(batch_size=4,num_threads=2,device_id=0)\n",
        "pipe.build()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBeeagSba2YH",
        "outputId": "e457fa66-f21f-4d51-ef1c-b09dcc172828"
      },
      "source": [
        "#DALI dataloader creation\n",
        "\n",
        "dali_dl = DALIGenericIterator(\n",
        "  pipelines=pipe, \n",
        "  output_map=['image', 'label'], \n",
        "  size=pipe.epoch_size(\"Reader\"), \n",
        "  last_batch_padded=True,\n",
        "  last_batch_policy=LastBatchPolicy.PARTIAL, \n",
        "  auto_reset=True\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nvidia/dali/plugin/base_iterator.py:162: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.\n",
            "  _iterator_deprecation_warning()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKb1ymT0bAGP"
      },
      "source": [
        "#define basic train function\n",
        "\n",
        "def train(dl, model, num_epochs=5):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  start = time.time()\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dl):\n",
        "      # DALI\n",
        "      if type(dl) == DALIGenericIterator:\n",
        "        inputs = data[0][\"image\"].permute(0, 3, 1, 2).to(device)\n",
        "        labels = data[0][\"label\"].long().view(-1).to(device)\n",
        "      # Torch\n",
        "      else:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      \n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      \n",
        "    print('epoch : %d loss: %.3f' %(epoch + 1, running_loss / (i+1)))\n",
        "  print('Finished Training')\n",
        "  print(np.round(time.time() - start, 2), '(seconds)')\n",
        "\n",
        "#define basic test function\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(dl, model):\n",
        "  start = time.time()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, data in enumerate(dl):\n",
        "    # DALI\n",
        "    if type(dl) == DALIGenericIterator:\n",
        "      inputs = data[0][\"image\"].permute(0, 3, 1, 2).to(device)\n",
        "      labels = data[0][\"label\"].long().view(-1).to(device)\n",
        "    # Torch\n",
        "    else:\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    correct += (preds == labels).sum()\n",
        "    total += labels.shape[0]\n",
        "  print(f\"Acc: {correct/total*100.0:.2f}%\")\n",
        "  print(f\"Elapsed time: {(time.time() - start):.2f} sec\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbtW9wpGbIjJ",
        "outputId": "4f5a4175-363f-4827-db65-f4ce14afeb5b"
      },
      "source": [
        "#train\n",
        "print(\"ResNet50\")\n",
        "train(dali_dl, model1)\n",
        "print(\"CoTNet50\")\n",
        "train(dali_dl, model2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch : 1 loss: 4.179\n",
            "epoch : 2 loss: 3.520\n",
            "epoch : 3 loss: 3.190\n",
            "epoch : 4 loss: 2.911\n",
            "epoch : 5 loss: 2.640\n",
            "Finished Training\n",
            "1868.76 (seconds)\n",
            "CoTNet50\n",
            "epoch : 1 loss: 4.317\n",
            "epoch : 2 loss: 3.555\n",
            "epoch : 3 loss: 3.057\n",
            "epoch : 4 loss: 2.686\n",
            "epoch : 5 loss: 2.276\n",
            "Finished Training\n",
            "1108.34 (seconds)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlyW8fzIcKfV",
        "outputId": "7ac52af7-ba7e-40c7-fde1-e9b7f9378318"
      },
      "source": [
        "# Test\n",
        "print(\"ResNet50\")\n",
        "test(dali_dl, model1)\n",
        "print(\"CoTNet50\")\n",
        "test(dali_dl, model2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet50\n",
            "Acc: 32.74%\n",
            "Elapsed time: 149.35 sec\n",
            "CoTNet50\n",
            "Acc: 44.83%\n",
            "Elapsed time: 68.27 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}