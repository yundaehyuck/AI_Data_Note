{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiWulKAdmAHX"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH0VdC4uJJVG"
      },
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다.\n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "outputs": [],
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "outputs": [],
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3e8FiNvgX60",
        "outputId": "7d5546f7-e812-4f5d-8555-9dd37c6c58c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 24995.85it/s]\n"
          ]
        }
      ],
      "source": [
        "data, max_len = padding(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af5d0df-ce06-42d9-961c-3accdc17ef6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0]\n",
            "[60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10]\n",
            "[70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0]\n",
            "[20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "for d in data:\n",
        "\n",
        "    print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "outputs": [],
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fb5332-eeaf-4d24-f1a2-7af7e3c717e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.0694, -0.2390,  0.1662,  ..., -1.9457,  0.0275,  2.1537],\n",
            "         [-0.5915, -0.0119, -0.1197,  ..., -0.2442,  0.0591,  0.6294],\n",
            "         [ 1.6521,  0.2287,  0.0495,  ..., -0.1128, -0.1242,  0.3815],\n",
            "         ...,\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996]],\n",
            "\n",
            "        [[-1.0447, -0.0719,  0.1134,  ...,  0.8984,  0.1344, -0.0302],\n",
            "         [ 0.7037,  2.7984, -0.7042,  ..., -0.1699,  0.1612, -0.4142],\n",
            "         [-1.2631,  0.6691,  0.0374,  ..., -1.6806,  0.2675,  0.1831],\n",
            "         ...,\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996]],\n",
            "\n",
            "        [[ 2.5548, -1.0154,  0.7586,  ...,  0.0159,  1.5879,  0.4343],\n",
            "         [ 0.0770,  0.6727,  1.5715,  ..., -0.7994,  0.4170, -0.2782],\n",
            "         [ 0.7205, -0.4941,  0.9013,  ..., -1.3814, -1.2703, -0.1441],\n",
            "         ...,\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4409, -1.0802,  0.6363,  ...,  0.2827,  0.4761,  0.2845],\n",
            "         [ 0.6459,  0.0103, -0.7734,  ...,  0.2758, -0.9612,  0.1922],\n",
            "         [-1.2631,  0.6691,  0.0374,  ..., -1.6806,  0.2675,  0.1831],\n",
            "         ...,\n",
            "         [ 0.7708, -0.5485,  0.0612,  ..., -0.2599,  0.0446, -1.6321],\n",
            "         [-0.6018,  0.9487,  0.4275,  ...,  0.5346,  0.1684, -0.6455],\n",
            "         [ 0.5375, -2.8091, -0.8052,  ..., -0.4761, -1.4856, -1.3453]],\n",
            "\n",
            "        [[-0.5928,  0.2490,  0.3497,  ..., -1.6491, -1.4605,  1.7212],\n",
            "         [ 0.8843, -1.5932, -0.9083,  ..., -0.1485,  0.8318, -1.4777],\n",
            "         [ 0.3258,  1.4572, -0.3862,  ...,  0.4370,  0.5598, -2.2042],\n",
            "         ...,\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996]],\n",
            "\n",
            "        [[-0.3904,  0.2150, -0.1377,  ...,  0.8961,  0.4269,  0.0901],\n",
            "         [ 0.8843, -1.5932, -0.9083,  ..., -0.1485,  0.8318, -1.4777],\n",
            "         [ 0.7246, -1.0251, -0.7295,  ...,  0.6507, -0.7829,  1.8227],\n",
            "         ...,\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996],\n",
            "         [ 0.7478, -1.0914, -1.9780,  ...,  0.1879, -0.8365, -0.0996]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "outputs": [],
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "outputs": [],
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-vSL7PwnV6k",
        "outputId": "66b201ba-fbeb-4554-9627-6edec575d4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tiOKAv9nEli",
        "outputId": "2bfefd95-0d41-4aaf-f46a-5cb86b48120b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNb2isfn5Cx",
        "outputId": "a7da7b13-936c-420b-bf7f-aa2ce3510bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd86a1c-b632-4648-92e9-3a3d2b1b05d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0321, 0.0377, 0.0567,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0532, 0.0586, 0.0367,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0503, 0.0501, 0.0560,  ..., 0.0578, 0.0578, 0.0578],\n",
            "          ...,\n",
            "          [0.0939, 0.0711, 0.0937,  ..., 0.0272, 0.0272, 0.0272],\n",
            "          [0.0939, 0.0711, 0.0937,  ..., 0.0272, 0.0272, 0.0272],\n",
            "          [0.0939, 0.0711, 0.0937,  ..., 0.0272, 0.0272, 0.0272]],\n",
            "\n",
            "         [[0.0914, 0.0527, 0.0437,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0405, 0.0613, 0.0348,  ..., 0.0277, 0.0277, 0.0277],\n",
            "          [0.0641, 0.0758, 0.0420,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          ...,\n",
            "          [0.0516, 0.0497, 0.0692,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          [0.0516, 0.0497, 0.0692,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          [0.0516, 0.0497, 0.0692,  ..., 0.0541, 0.0541, 0.0541]],\n",
            "\n",
            "         [[0.0291, 0.0744, 0.0439,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0710, 0.0954, 0.0466,  ..., 0.0309, 0.0309, 0.0309],\n",
            "          [0.0601, 0.0342, 0.0635,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          ...,\n",
            "          [0.0203, 0.0478, 0.0366,  ..., 0.0387, 0.0387, 0.0387],\n",
            "          [0.0203, 0.0478, 0.0366,  ..., 0.0387, 0.0387, 0.0387],\n",
            "          [0.0203, 0.0478, 0.0366,  ..., 0.0387, 0.0387, 0.0387]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0376, 0.0394, 0.0348,  ..., 0.0786, 0.0786, 0.0786],\n",
            "          [0.0773, 0.0634, 0.0384,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0812, 0.0299, 0.0232,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          ...,\n",
            "          [0.0410, 0.0486, 0.0479,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0410, 0.0486, 0.0479,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0410, 0.0486, 0.0479,  ..., 0.0632, 0.0632, 0.0632]],\n",
            "\n",
            "         [[0.0559, 0.0464, 0.0594,  ..., 0.0498, 0.0498, 0.0498],\n",
            "          [0.0313, 0.0382, 0.0494,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0706, 0.0391, 0.0563,  ..., 0.0381, 0.0381, 0.0381],\n",
            "          ...,\n",
            "          [0.0338, 0.0529, 0.0473,  ..., 0.0416, 0.0416, 0.0416],\n",
            "          [0.0338, 0.0529, 0.0473,  ..., 0.0416, 0.0416, 0.0416],\n",
            "          [0.0338, 0.0529, 0.0473,  ..., 0.0416, 0.0416, 0.0416]],\n",
            "\n",
            "         [[0.0686, 0.0378, 0.0835,  ..., 0.0344, 0.0344, 0.0344],\n",
            "          [0.0200, 0.0260, 0.0516,  ..., 0.0557, 0.0557, 0.0557],\n",
            "          [0.0703, 0.0385, 0.0620,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          ...,\n",
            "          [0.0773, 0.0626, 0.0438,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0773, 0.0626, 0.0438,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0773, 0.0626, 0.0438,  ..., 0.0393, 0.0393, 0.0393]]],\n",
            "\n",
            "\n",
            "        [[[0.0253, 0.0402, 0.0314,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0490, 0.0799, 0.0421,  ..., 0.0435, 0.0435, 0.0435],\n",
            "          [0.0556, 0.0512, 0.0218,  ..., 0.0508, 0.0508, 0.0508],\n",
            "          ...,\n",
            "          [0.1064, 0.0646, 0.1719,  ..., 0.0364, 0.0364, 0.0364],\n",
            "          [0.1064, 0.0646, 0.1719,  ..., 0.0364, 0.0364, 0.0364],\n",
            "          [0.1064, 0.0646, 0.1719,  ..., 0.0364, 0.0364, 0.0364]],\n",
            "\n",
            "         [[0.0265, 0.0343, 0.0320,  ..., 0.0579, 0.0579, 0.0579],\n",
            "          [0.0942, 0.0618, 0.0615,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          [0.0869, 0.0656, 0.0439,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          ...,\n",
            "          [0.0319, 0.0562, 0.0334,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0319, 0.0562, 0.0334,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0319, 0.0562, 0.0334,  ..., 0.0532, 0.0532, 0.0532]],\n",
            "\n",
            "         [[0.0581, 0.0383, 0.0947,  ..., 0.0464, 0.0464, 0.0464],\n",
            "          [0.0518, 0.0697, 0.0968,  ..., 0.0438, 0.0438, 0.0438],\n",
            "          [0.0315, 0.0508, 0.0449,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          ...,\n",
            "          [0.0727, 0.0642, 0.0592,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0727, 0.0642, 0.0592,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0727, 0.0642, 0.0592,  ..., 0.0455, 0.0455, 0.0455]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0406, 0.0297, 0.0433,  ..., 0.0549, 0.0549, 0.0549],\n",
            "          [0.1070, 0.0947, 0.0628,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0444, 0.0323, 0.0290,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          ...,\n",
            "          [0.0370, 0.0421, 0.0362,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0370, 0.0421, 0.0362,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0370, 0.0421, 0.0362,  ..., 0.0527, 0.0527, 0.0527]],\n",
            "\n",
            "         [[0.1187, 0.0233, 0.0822,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0602, 0.0967, 0.0501,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0314, 0.0432, 0.0639,  ..., 0.0523, 0.0523, 0.0523],\n",
            "          ...,\n",
            "          [0.0481, 0.0472, 0.0343,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0481, 0.0472, 0.0343,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0481, 0.0472, 0.0343,  ..., 0.0450, 0.0450, 0.0450]],\n",
            "\n",
            "         [[0.0874, 0.0729, 0.0503,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0321, 0.0275, 0.0296,  ..., 0.0583, 0.0583, 0.0583],\n",
            "          [0.0423, 0.0273, 0.0407,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          ...,\n",
            "          [0.0265, 0.0633, 0.0797,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0265, 0.0633, 0.0797,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0265, 0.0633, 0.0797,  ..., 0.0449, 0.0449, 0.0449]]],\n",
            "\n",
            "\n",
            "        [[[0.0617, 0.0700, 0.0304,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0491, 0.0593, 0.0181,  ..., 0.0594, 0.0594, 0.0594],\n",
            "          [0.0585, 0.0486, 0.1091,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          ...,\n",
            "          [0.0422, 0.0502, 0.0639,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0422, 0.0502, 0.0639,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0422, 0.0502, 0.0639,  ..., 0.0348, 0.0348, 0.0348]],\n",
            "\n",
            "         [[0.0276, 0.0184, 0.0292,  ..., 0.0786, 0.0786, 0.0786],\n",
            "          [0.0486, 0.0528, 0.0325,  ..., 0.0578, 0.0578, 0.0578],\n",
            "          [0.0634, 0.0349, 0.0345,  ..., 0.0547, 0.0547, 0.0547],\n",
            "          ...,\n",
            "          [0.0601, 0.0314, 0.0759,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0601, 0.0314, 0.0759,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0601, 0.0314, 0.0759,  ..., 0.0486, 0.0486, 0.0486]],\n",
            "\n",
            "         [[0.0240, 0.0259, 0.0400,  ..., 0.0595, 0.0595, 0.0595],\n",
            "          [0.0259, 0.0570, 0.0371,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          [0.0630, 0.0343, 0.0456,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          ...,\n",
            "          [0.0474, 0.1049, 0.0299,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0474, 0.1049, 0.0299,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0474, 0.1049, 0.0299,  ..., 0.0427, 0.0427, 0.0427]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0354, 0.0439, 0.0392,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0329, 0.0472, 0.0472,  ..., 0.0621, 0.0621, 0.0621],\n",
            "          [0.0519, 0.0291, 0.0824,  ..., 0.0340, 0.0340, 0.0340],\n",
            "          ...,\n",
            "          [0.0424, 0.0490, 0.0246,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0424, 0.0490, 0.0246,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0424, 0.0490, 0.0246,  ..., 0.0532, 0.0532, 0.0532]],\n",
            "\n",
            "         [[0.0398, 0.0536, 0.0518,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0344, 0.0368, 0.0405,  ..., 0.0666, 0.0666, 0.0666],\n",
            "          [0.0458, 0.0703, 0.0538,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          ...,\n",
            "          [0.0394, 0.0430, 0.0501,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0394, 0.0430, 0.0501,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0394, 0.0430, 0.0501,  ..., 0.0465, 0.0465, 0.0465]],\n",
            "\n",
            "         [[0.0686, 0.0636, 0.0650,  ..., 0.0412, 0.0412, 0.0412],\n",
            "          [0.0738, 0.0362, 0.0391,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0497, 0.0376, 0.0398,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          ...,\n",
            "          [0.0642, 0.0344, 0.0434,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0642, 0.0344, 0.0434,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0642, 0.0344, 0.0434,  ..., 0.0443, 0.0443, 0.0443]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0594, 0.0316, 0.0544,  ..., 0.0403, 0.0378, 0.0429],\n",
            "          [0.0837, 0.0353, 0.0374,  ..., 0.0414, 0.0361, 0.0278],\n",
            "          [0.0564, 0.0303, 0.0272,  ..., 0.0596, 0.0727, 0.0712],\n",
            "          ...,\n",
            "          [0.0548, 0.0440, 0.0337,  ..., 0.0717, 0.0537, 0.0554],\n",
            "          [0.0543, 0.0574, 0.0867,  ..., 0.0575, 0.0271, 0.0585],\n",
            "          [0.0377, 0.0358, 0.0433,  ..., 0.0376, 0.0584, 0.0410]],\n",
            "\n",
            "         [[0.0523, 0.0374, 0.0455,  ..., 0.0506, 0.0810, 0.0382],\n",
            "          [0.0372, 0.0431, 0.0420,  ..., 0.0537, 0.0657, 0.0340],\n",
            "          [0.0305, 0.0719, 0.0336,  ..., 0.0364, 0.0415, 0.0503],\n",
            "          ...,\n",
            "          [0.0412, 0.0471, 0.0524,  ..., 0.0457, 0.0308, 0.0405],\n",
            "          [0.0857, 0.0594, 0.0516,  ..., 0.0582, 0.0390, 0.0362],\n",
            "          [0.0351, 0.0733, 0.0635,  ..., 0.0410, 0.0391, 0.0581]],\n",
            "\n",
            "         [[0.0481, 0.0496, 0.0644,  ..., 0.0527, 0.0705, 0.0471],\n",
            "          [0.0721, 0.0496, 0.0580,  ..., 0.0615, 0.0383, 0.0673],\n",
            "          [0.0502, 0.0510, 0.0369,  ..., 0.0720, 0.0364, 0.0739],\n",
            "          ...,\n",
            "          [0.0392, 0.0667, 0.0667,  ..., 0.0310, 0.0596, 0.0408],\n",
            "          [0.0649, 0.0314, 0.0596,  ..., 0.0405, 0.0559, 0.0427],\n",
            "          [0.0577, 0.0365, 0.0551,  ..., 0.0460, 0.0578, 0.0313]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0820, 0.0596, 0.0320,  ..., 0.0328, 0.0550, 0.0410],\n",
            "          [0.0465, 0.0304, 0.0378,  ..., 0.0556, 0.0497, 0.1030],\n",
            "          [0.0764, 0.0424, 0.0243,  ..., 0.0855, 0.0536, 0.0501],\n",
            "          ...,\n",
            "          [0.0601, 0.0375, 0.0479,  ..., 0.0478, 0.0416, 0.0428],\n",
            "          [0.0524, 0.0493, 0.0516,  ..., 0.0601, 0.0501, 0.0368],\n",
            "          [0.0766, 0.0508, 0.0694,  ..., 0.0330, 0.0469, 0.0438]],\n",
            "\n",
            "         [[0.0749, 0.0575, 0.0911,  ..., 0.0291, 0.0500, 0.0417],\n",
            "          [0.0467, 0.1171, 0.0811,  ..., 0.0406, 0.0282, 0.0458],\n",
            "          [0.0492, 0.0482, 0.0528,  ..., 0.0428, 0.0360, 0.0739],\n",
            "          ...,\n",
            "          [0.0564, 0.0489, 0.0506,  ..., 0.0622, 0.0440, 0.0533],\n",
            "          [0.0336, 0.0401, 0.0187,  ..., 0.0384, 0.0989, 0.0281],\n",
            "          [0.0512, 0.0447, 0.0307,  ..., 0.0437, 0.0463, 0.0334]],\n",
            "\n",
            "         [[0.0443, 0.0490, 0.0432,  ..., 0.0760, 0.1080, 0.0616],\n",
            "          [0.0633, 0.0337, 0.0436,  ..., 0.0291, 0.0517, 0.0616],\n",
            "          [0.0587, 0.0527, 0.0536,  ..., 0.0504, 0.0834, 0.0341],\n",
            "          ...,\n",
            "          [0.0471, 0.0389, 0.0628,  ..., 0.0199, 0.0571, 0.0595],\n",
            "          [0.0367, 0.0339, 0.0326,  ..., 0.0605, 0.0857, 0.0475],\n",
            "          [0.0507, 0.0389, 0.0515,  ..., 0.0422, 0.0627, 0.0591]]],\n",
            "\n",
            "\n",
            "        [[[0.0692, 0.0428, 0.0247,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0537, 0.0804, 0.0540,  ..., 0.0340, 0.0340, 0.0340],\n",
            "          [0.0474, 0.0468, 0.0582,  ..., 0.0432, 0.0432, 0.0432],\n",
            "          ...,\n",
            "          [0.0879, 0.0751, 0.0536,  ..., 0.0259, 0.0259, 0.0259],\n",
            "          [0.0879, 0.0751, 0.0536,  ..., 0.0259, 0.0259, 0.0259],\n",
            "          [0.0879, 0.0751, 0.0536,  ..., 0.0259, 0.0259, 0.0259]],\n",
            "\n",
            "         [[0.0715, 0.0231, 0.0572,  ..., 0.0397, 0.0397, 0.0397],\n",
            "          [0.0356, 0.0861, 0.0576,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0487, 0.0501, 0.0317,  ..., 0.0608, 0.0608, 0.0608],\n",
            "          ...,\n",
            "          [0.0247, 0.0793, 0.0403,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0247, 0.0793, 0.0403,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0247, 0.0793, 0.0403,  ..., 0.0514, 0.0514, 0.0514]],\n",
            "\n",
            "         [[0.0487, 0.0508, 0.0511,  ..., 0.0230, 0.0230, 0.0230],\n",
            "          [0.0241, 0.0500, 0.0797,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0332, 0.0706, 0.0522,  ..., 0.0336, 0.0336, 0.0336],\n",
            "          ...,\n",
            "          [0.0404, 0.0403, 0.0431,  ..., 0.0385, 0.0385, 0.0385],\n",
            "          [0.0404, 0.0403, 0.0431,  ..., 0.0385, 0.0385, 0.0385],\n",
            "          [0.0404, 0.0403, 0.0431,  ..., 0.0385, 0.0385, 0.0385]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0922, 0.0367, 0.0381,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0422, 0.0402, 0.0513,  ..., 0.0711, 0.0711, 0.0711],\n",
            "          [0.0447, 0.0374, 0.0468,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          ...,\n",
            "          [0.0550, 0.0270, 0.0646,  ..., 0.0603, 0.0603, 0.0603],\n",
            "          [0.0550, 0.0270, 0.0646,  ..., 0.0603, 0.0603, 0.0603],\n",
            "          [0.0550, 0.0270, 0.0646,  ..., 0.0603, 0.0603, 0.0603]],\n",
            "\n",
            "         [[0.0561, 0.0311, 0.0432,  ..., 0.0380, 0.0380, 0.0380],\n",
            "          [0.0477, 0.0583, 0.0410,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0417, 0.0375, 0.0397,  ..., 0.0794, 0.0794, 0.0794],\n",
            "          ...,\n",
            "          [0.0516, 0.0498, 0.0550,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0516, 0.0498, 0.0550,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0516, 0.0498, 0.0550,  ..., 0.0454, 0.0454, 0.0454]],\n",
            "\n",
            "         [[0.0418, 0.0352, 0.0482,  ..., 0.0628, 0.0628, 0.0628],\n",
            "          [0.0471, 0.0334, 0.0702,  ..., 0.0406, 0.0406, 0.0406],\n",
            "          [0.0551, 0.0595, 0.0470,  ..., 0.0489, 0.0489, 0.0489],\n",
            "          ...,\n",
            "          [0.0386, 0.0416, 0.0640,  ..., 0.0360, 0.0360, 0.0360],\n",
            "          [0.0386, 0.0416, 0.0640,  ..., 0.0360, 0.0360, 0.0360],\n",
            "          [0.0386, 0.0416, 0.0640,  ..., 0.0360, 0.0360, 0.0360]]],\n",
            "\n",
            "\n",
            "        [[[0.0457, 0.0649, 0.0561,  ..., 0.0332, 0.0332, 0.0332],\n",
            "          [0.0395, 0.0805, 0.0438,  ..., 0.0341, 0.0341, 0.0341],\n",
            "          [0.0693, 0.0526, 0.0358,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          ...,\n",
            "          [0.0306, 0.0792, 0.0860,  ..., 0.0274, 0.0274, 0.0274],\n",
            "          [0.0306, 0.0792, 0.0860,  ..., 0.0274, 0.0274, 0.0274],\n",
            "          [0.0306, 0.0792, 0.0860,  ..., 0.0274, 0.0274, 0.0274]],\n",
            "\n",
            "         [[0.0672, 0.0872, 0.0543,  ..., 0.0327, 0.0327, 0.0327],\n",
            "          [0.0576, 0.0810, 0.0792,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          [0.0788, 0.0917, 0.0593,  ..., 0.0326, 0.0326, 0.0326],\n",
            "          ...,\n",
            "          [0.0844, 0.0754, 0.0272,  ..., 0.0489, 0.0489, 0.0489],\n",
            "          [0.0844, 0.0754, 0.0272,  ..., 0.0489, 0.0489, 0.0489],\n",
            "          [0.0844, 0.0754, 0.0272,  ..., 0.0489, 0.0489, 0.0489]],\n",
            "\n",
            "         [[0.0457, 0.0467, 0.0535,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0340, 0.0483, 0.0557,  ..., 0.0593, 0.0593, 0.0593],\n",
            "          [0.0405, 0.0502, 0.0658,  ..., 0.0397, 0.0397, 0.0397],\n",
            "          ...,\n",
            "          [0.0629, 0.0430, 0.0537,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.0629, 0.0430, 0.0537,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.0629, 0.0430, 0.0537,  ..., 0.0410, 0.0410, 0.0410]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0500, 0.0285, 0.0397,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0366, 0.0385, 0.0358,  ..., 0.0681, 0.0681, 0.0681],\n",
            "          [0.0629, 0.0606, 0.0455,  ..., 0.0388, 0.0388, 0.0388],\n",
            "          ...,\n",
            "          [0.0322, 0.0259, 0.0271,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          [0.0322, 0.0259, 0.0271,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          [0.0322, 0.0259, 0.0271,  ..., 0.0580, 0.0580, 0.0580]],\n",
            "\n",
            "         [[0.0344, 0.0418, 0.0646,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0691, 0.0517, 0.0567,  ..., 0.0460, 0.0460, 0.0460],\n",
            "          [0.0456, 0.0285, 0.0391,  ..., 0.0500, 0.0500, 0.0500],\n",
            "          ...,\n",
            "          [0.0466, 0.0537, 0.0329,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0466, 0.0537, 0.0329,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0466, 0.0537, 0.0329,  ..., 0.0488, 0.0488, 0.0488]],\n",
            "\n",
            "         [[0.0550, 0.0559, 0.0307,  ..., 0.0602, 0.0602, 0.0602],\n",
            "          [0.0496, 0.0348, 0.0594,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          [0.0359, 0.0620, 0.0655,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          ...,\n",
            "          [0.0660, 0.0460, 0.0800,  ..., 0.0398, 0.0398, 0.0398],\n",
            "          [0.0660, 0.0460, 0.0800,  ..., 0.0398, 0.0398, 0.0398],\n",
            "          [0.0660, 0.0460, 0.0800,  ..., 0.0398, 0.0398, 0.0398]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ],
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7megouWpgCck",
        "outputId": "3095f0bc-e282-4d31-f668-bc2e7b995b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaK0bpMGhQZ2",
        "outputId": "8e661087-1287-4938-8413-26923eda8524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5e3ce6-eb03-444f-f4fc-da7ac59a64bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1165,  0.1298,  0.0666,  ..., -0.1141,  0.0476,  0.0787],\n",
            "         [-0.1516,  0.1034,  0.0816,  ..., -0.1275,  0.0539,  0.1626],\n",
            "         [-0.1476,  0.0405, -0.0229,  ..., -0.0741,  0.0741,  0.0630],\n",
            "         ...,\n",
            "         [-0.1154,  0.0567,  0.0332,  ..., -0.0294,  0.0969,  0.0368],\n",
            "         [-0.1154,  0.0567,  0.0332,  ..., -0.0294,  0.0969,  0.0368],\n",
            "         [-0.1154,  0.0567,  0.0332,  ..., -0.0294,  0.0969,  0.0368]],\n",
            "\n",
            "        [[-0.5060,  0.2339,  0.2490,  ..., -0.4742,  0.0375,  0.5349],\n",
            "         [-0.5093,  0.2369,  0.1996,  ..., -0.4750,  0.0638,  0.5631],\n",
            "         [-0.5939,  0.2865,  0.2446,  ..., -0.4908,  0.0479,  0.5590],\n",
            "         ...,\n",
            "         [-0.4932,  0.2491,  0.2009,  ..., -0.4722,  0.0540,  0.5183],\n",
            "         [-0.4932,  0.2491,  0.2009,  ..., -0.4722,  0.0540,  0.5183],\n",
            "         [-0.4932,  0.2491,  0.2009,  ..., -0.4722,  0.0540,  0.5183]],\n",
            "\n",
            "        [[-0.3577,  0.2886,  0.0669,  ..., -0.3673,  0.0200,  0.2743],\n",
            "         [-0.3638,  0.3230,  0.1383,  ..., -0.4000, -0.0045,  0.3291],\n",
            "         [-0.2927,  0.2468,  0.0286,  ..., -0.4200, -0.0023,  0.2401],\n",
            "         ...,\n",
            "         [-0.2557,  0.2971,  0.0189,  ..., -0.3427,  0.0164,  0.1991],\n",
            "         [-0.2557,  0.2971,  0.0189,  ..., -0.3427,  0.0164,  0.1991],\n",
            "         [-0.2557,  0.2971,  0.0189,  ..., -0.3427,  0.0164,  0.1991]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0611, -0.0421,  0.0651,  ..., -0.1268, -0.2571, -0.2146],\n",
            "         [-0.0334, -0.0107,  0.0473,  ..., -0.1291, -0.2135, -0.2150],\n",
            "         [-0.0121,  0.0051,  0.0569,  ..., -0.1396, -0.2093, -0.2034],\n",
            "         ...,\n",
            "         [-0.0566, -0.0393,  0.0499,  ..., -0.1256, -0.2513, -0.1948],\n",
            "         [-0.0524, -0.0598,  0.0966,  ..., -0.0993, -0.2154, -0.2558],\n",
            "         [-0.0581, -0.0241,  0.0332,  ..., -0.0905, -0.2313, -0.2570]],\n",
            "\n",
            "        [[-0.1666,  0.0812,  0.0154,  ..., -0.2514, -0.0458,  0.0152],\n",
            "         [-0.2124,  0.0447, -0.0040,  ..., -0.1854, -0.0077,  0.0332],\n",
            "         [-0.1920,  0.0956,  0.0524,  ..., -0.2374, -0.0509,  0.0635],\n",
            "         ...,\n",
            "         [-0.1452,  0.0565,  0.0164,  ..., -0.1620, -0.0445, -0.0357],\n",
            "         [-0.1452,  0.0565,  0.0164,  ..., -0.1620, -0.0445, -0.0357],\n",
            "         [-0.1452,  0.0565,  0.0164,  ..., -0.1620, -0.0445, -0.0357]],\n",
            "\n",
            "        [[-0.3336,  0.0782,  0.0937,  ..., -0.3741, -0.1436,  0.0844],\n",
            "         [-0.2445,  0.0694,  0.0894,  ..., -0.2804, -0.1188,  0.0728],\n",
            "         [-0.2222,  0.0488,  0.1127,  ..., -0.3389, -0.1649,  0.0928],\n",
            "         ...,\n",
            "         [-0.2562,  0.0324,  0.0818,  ..., -0.2915, -0.1231,  0.0135],\n",
            "         [-0.2562,  0.0324,  0.0818,  ..., -0.2915, -0.1231,  0.0135],\n",
            "         [-0.2562,  0.0324,  0.0818,  ..., -0.2915, -0.1231,  0.0135]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    q = w_q(q)  # (B, L, d_model)\n",
        "    k = w_k(k)  # (B, L, d_model)\n",
        "    v = w_v(v)  # (B, L, d_model)\n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    d_k = d_model // num_heads\n",
        "\n",
        "    # num_heads * d_k로 쪼갠다\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "outputs": [],
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb3331c-5f70-4144-94e4-e1ee0fc3bb17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0084,  0.0697,  0.0066,  ...,  0.0762, -0.1163, -0.1509],\n",
            "         [ 0.0095,  0.0896,  0.0087,  ...,  0.0376, -0.0603, -0.2201],\n",
            "         [-0.0065,  0.0732,  0.0640,  ...,  0.0287, -0.1010, -0.2239],\n",
            "         ...,\n",
            "         [-0.0384,  0.1396,  0.1162,  ...,  0.1014, -0.1325, -0.2189],\n",
            "         [-0.0384,  0.1396,  0.1162,  ...,  0.1014, -0.1325, -0.2189],\n",
            "         [-0.0384,  0.1396,  0.1162,  ...,  0.1014, -0.1325, -0.2189]],\n",
            "\n",
            "        [[-0.2871,  0.2953, -0.3290,  ...,  0.0831,  0.1059, -0.2935],\n",
            "         [-0.1951,  0.2030, -0.2173,  ...,  0.0829,  0.1585, -0.2751],\n",
            "         [-0.2699,  0.3090, -0.2791,  ...,  0.1213,  0.1017, -0.2839],\n",
            "         ...,\n",
            "         [-0.2669,  0.2389, -0.2369,  ...,  0.0907,  0.0859, -0.2880],\n",
            "         [-0.2669,  0.2389, -0.2369,  ...,  0.0907,  0.0859, -0.2880],\n",
            "         [-0.2669,  0.2389, -0.2369,  ...,  0.0907,  0.0859, -0.2880]],\n",
            "\n",
            "        [[-0.2609,  0.3197, -0.1531,  ...,  0.1359, -0.0250, -0.2770],\n",
            "         [-0.1726,  0.2895, -0.1467,  ...,  0.2336,  0.0393, -0.3033],\n",
            "         [-0.1338,  0.2933, -0.0587,  ...,  0.1355, -0.0040, -0.3053],\n",
            "         ...,\n",
            "         [-0.1362,  0.3282, -0.0905,  ...,  0.1764,  0.0348, -0.3555],\n",
            "         [-0.1362,  0.3282, -0.0905,  ...,  0.1764,  0.0348, -0.3555],\n",
            "         [-0.1362,  0.3282, -0.0905,  ...,  0.1764,  0.0348, -0.3555]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0437,  0.1636, -0.0092,  ...,  0.0398, -0.0615, -0.1066],\n",
            "         [-0.0510,  0.1717, -0.0178,  ..., -0.0048, -0.0306, -0.1238],\n",
            "         [-0.0345,  0.1283, -0.0250,  ...,  0.0340, -0.0873, -0.0951],\n",
            "         ...,\n",
            "         [-0.0460,  0.1343, -0.0106,  ...,  0.0011, -0.0934, -0.0668],\n",
            "         [-0.1030,  0.1625, -0.0084,  ...,  0.1174, -0.0822, -0.1604],\n",
            "         [-0.0691,  0.1475, -0.0348,  ...,  0.0576, -0.0485, -0.0713]],\n",
            "\n",
            "        [[ 0.0537,  0.1530, -0.0107,  ...,  0.0820,  0.0898, -0.0601],\n",
            "         [ 0.0129,  0.2198, -0.0769,  ...,  0.1222,  0.0196, -0.0402],\n",
            "         [ 0.0480,  0.2321, -0.0855,  ...,  0.1042,  0.0925, -0.0773],\n",
            "         ...,\n",
            "         [ 0.0680,  0.2053,  0.0048,  ...,  0.1374,  0.0376, -0.0863],\n",
            "         [ 0.0680,  0.2053,  0.0048,  ...,  0.1374,  0.0376, -0.0863],\n",
            "         [ 0.0680,  0.2053,  0.0048,  ...,  0.1374,  0.0376, -0.0863]],\n",
            "\n",
            "        [[-0.1762,  0.2517,  0.0263,  ...,  0.2144, -0.0453, -0.0936],\n",
            "         [-0.2115,  0.1757,  0.0309,  ...,  0.2458, -0.0643, -0.0384],\n",
            "         [-0.1102,  0.1644,  0.0375,  ...,  0.1578, -0.0074,  0.0197],\n",
            "         ...,\n",
            "         [-0.1863,  0.2045,  0.0377,  ...,  0.2464, -0.0190, -0.0596],\n",
            "         [-0.1863,  0.2045,  0.0377,  ...,  0.2464, -0.0190, -0.0596],\n",
            "         [-0.1863,  0.2045,  0.0377,  ...,  0.2464, -0.0190, -0.0596]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "og7EoqqzLSRk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}