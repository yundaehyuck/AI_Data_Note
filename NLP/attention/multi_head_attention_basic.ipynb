{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Headed Attention"
      ],
      "metadata": {
        "id": "_pRTDb-ZKqDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J0NXRS6KlhF",
        "outputId": "bbd6f32e-17ca-4ce8-bca6-f2fa4f8e23ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version:[2.0.1+cu118].\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "print(\"PyTorch version:[%s].\" %(torch.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"device:[%s].\"%(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVV0QmZcLAxB",
        "outputId": "e6b18e84-4bbf-4c0c-b2f1-6c6275fec098"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:[cpu].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCM-jCX-LJ3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot-Product Attention (SDPA)\n",
        "- Data $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the number data and $d$ is the data dimension\n",
        "- Query and Key $Q, K \\in \\mathbb{R}^{n \\times d_K}$\n",
        "- Value $V \\in \\mathbb{R}^{n \\times d_V} $\n",
        "\n",
        "$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right)V \\in \\mathbb{R}^{n \\times d_V} $"
      ],
      "metadata": {
        "id": "OMYK6wz2LMjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#init이 없어도 module이 되긴하네..?\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "\n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "\n",
        "        d_K = K.size()[-1] # key dimension\n",
        "\n",
        "        scores = Q.matmul(K.transpose(-2,-1))/np.sqrt(d_K) #normalize q and k inner product\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = F.softmax(scores, dim = -1)\n",
        "\n",
        "        out = attention.matmul(V)\n",
        "\n",
        "        return out, attention"
      ],
      "metadata": {
        "id": "BzUIdtxrLM24"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sh(x):\n",
        "\n",
        "    return str(x.shape)[11:-1]"
      ],
      "metadata": {
        "id": "lZochDEeMWNu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Demo run of scaled dot product attention\n",
        "\n",
        "SDPA = ScaledDotProductAttention()\n",
        "\n",
        "n_batch, d_K, d_V = 3, 128, 256 #d_K( = d_Q) does not necessarily be equal to d_V\n",
        "\n",
        "n_Q, n_K, n_V = 30, 50, 50 #인코더 - 디코더 attention에서는 q,k의 차원이 달라도 계산이 된다\n",
        "\n",
        "Q = torch.rand(n_batch, n_Q, d_K)\n",
        "K = torch.rand(n_batch, n_K, d_K)\n",
        "V = torch.rand(n_batch, n_V, d_V)\n",
        "\n",
        "out, attention = SDPA.forward(Q, K, V, mask = None)\n",
        "\n",
        "print(\"SDPA: Q%s K%s V%s => out%s attention%s\"%(\n",
        "    sh(Q), sh(K), sh(V), sh(out), sh(attention)\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXsbc6bqLxle",
        "outputId": "c5d2cf41-b980-4cae-825c-37db44919689"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDPA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#it supports 'multi-headed' attention\n",
        "\n",
        "n_batch, n_head, d_K, d_V = 3, 5, 128, 256\n",
        "\n",
        "n_Q, n_K, n_V = 30, 50, 50 # n_K and n_V should be the same\n",
        "\n",
        "Q = torch.rand(n_batch, n_head, n_Q, d_K)\n",
        "K = torch.rand(n_batch, n_head, n_K, d_K)\n",
        "V = torch.rand(n_batch, n_head, n_V, d_V)\n",
        "\n",
        "out, attention = SDPA.forward(Q, K, V, mask = None)\n",
        "\n",
        "# out: [n_batch x n_head x n_Q x d_V]\n",
        "# attention: [n_batch x n_head x n_Q x n_K]\n",
        "\n",
        "print(\"(Multi-Headed) SDPA: Q%s K%s V%s => out%s attention%s\"%(\n",
        "    sh(Q), sh(K), sh(V), sh(out), sh(attention)\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH1U7gUfMsPb",
        "outputId": "eb91103f-a436-40f7-d504-fdfe3c0dbabe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Multi-Headed) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 5, 30, 256] attention[3, 5, 30, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNfra213NTey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Headed Attention (MHA)\n",
        "\n",
        "$\\text{head}_{\\color{red}i} = \\text{Attention}(Q {\\color{green}W}^Q_{\\color{red}i},K {\\color{green}W}^K_{\\color{red}i}, V {\\color{green}W}^V_{\\color{red}i}) $"
      ],
      "metadata": {
        "id": "MYlxs8vLNhNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_feat = 128, n_head = 5, actv = F.relu, USE_BIAS = True, dropout_p = 0.1, device = None):\n",
        "\n",
        "        \"\"\"\n",
        "        :param d_feat: feature dimension\n",
        "        :param n_head: number of heads\n",
        "        :param actv: activation after each linear layer\n",
        "        :param USE_BIAS: whether to use bias\n",
        "        :param dropout_p: dropout rate\n",
        "        :device: which device to use (e.g., cuda:0)\n",
        "        \"\"\"\n",
        "\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "\n",
        "        if (d_feat % n_head) != 0:\n",
        "\n",
        "            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\" %(d_feat, n_head))\n",
        "\n",
        "        self.d_feat = d_feat\n",
        "        self.n_head = n_head\n",
        "        self.d_head = self.d_feat // self.n_head\n",
        "        self.actv = actv\n",
        "        self.USE_BIAS = USE_BIAS\n",
        "        self.dropout_p = dropout_p #prob. of zeroed\n",
        "\n",
        "        self.lin_Q = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n",
        "        self.lin_K = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n",
        "        self.lin_V = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n",
        "        self.lin_O = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = self.dropout_p)\n",
        "\n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "\n",
        "        \"\"\"\n",
        "        :param Q: [n_batch, n_Q, d_feat]\n",
        "        :param K: [n_batch, n_K, d_feat]\n",
        "        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same\n",
        "        :param mask:\n",
        "        \"\"\"\n",
        "\n",
        "        n_batch = Q.shape[0]\n",
        "\n",
        "        Q_feat = self.lin_Q(Q)\n",
        "        K_feat = self.lin_K(K)\n",
        "        V_feat = self.lin_V(V)\n",
        "\n",
        "        # Q_feat: [n_batch, n_Q, d_feat]\n",
        "        # K_feat: [n_batch, n_K, d_feat]\n",
        "        # V_feat: [n_batch, n_V, d_feat]\n",
        "\n",
        "        #원래는 k개 head로 k개 만들어서 aggregate하는건데\n",
        "        #그냥 하나의 결과를 k개로 쪼개서 하나본데?? 편하니까?\n",
        "        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\n",
        "\n",
        "        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q_split: [n_batch, n_head, n_Q, d_head]\n",
        "        # K_split: [n_batch, n_head, n_K, d_head]\n",
        "        # V_split: [n_batch, n_head, n_V, d_head]\n",
        "\n",
        "        # Multi-Headed Attention\n",
        "        d_K = K.size()[-1] #key dimension\n",
        "\n",
        "        scores = torch.matmul(Q_split, K_split.permute(0,1,3,2))/np.sqrt(d_K)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = torch.softmax(scores, dim = -1)\n",
        "        x_raw = torch.matmul(self.dropout(attention), V_split) #dropout is NOT mentioned in the paper\n",
        "\n",
        "        # attention: [n_batch, n_head, n_Q, n_K]\n",
        "        # x_raw: [n_batch, n_head, n_Q, d_head]\n",
        "\n",
        "        # Reshape x\n",
        "        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n",
        "        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n",
        "\n",
        "        x_rsh2 = x_rsh1.view(n_batch, -1, self.d_feat)\n",
        "        # x_rsh2: [n_batch, n_Q, d_feat]\n",
        "\n",
        "        #Linear\n",
        "        x = self.lin_O(x_rsh2)\n",
        "        #x:[n_batch, n_Q, d_feat]\n",
        "\n",
        "        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n",
        "        'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n",
        "        'scores':scores,'attention':attention,\n",
        "        'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "I2yfB737Nhfo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#self attention layer\n",
        "\n",
        "n_batch = 128\n",
        "n_src = 32\n",
        "d_feat = 200\n",
        "n_head = 5\n",
        "\n",
        "src = torch.rand(n_batch, n_src, d_feat)\n",
        "\n",
        "self_attention = MultiHeadedAttention(\n",
        "    d_feat = d_feat, n_head = n_head, actv = F.relu, USE_BIAS = True, dropout_p = 0.1, device = device\n",
        ")\n",
        "\n",
        "out = self_attention.forward(src, src, src, mask = None)"
      ],
      "metadata": {
        "id": "yllU16eZQy0C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\n",
        "Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\n",
        "scores,attention = out['scores'],out['attention']\n",
        "x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\n",
        "\n",
        "# Print out shapes\n",
        "def sh(_x): return str(_x.shape)[11:-1]\n",
        "print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\n",
        "print ()\n",
        "print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\n",
        "print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\n",
        "print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\n",
        "print ()\n",
        "print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\n",
        "print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\n",
        "print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\n",
        "print ()\n",
        "print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\n",
        "print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\n",
        "print ()\n",
        "print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\n",
        "print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\n",
        "print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\n",
        "print ()\n",
        "print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrX6DFUqRIy8",
        "outputId": "983527d9-5f26-4944-b3e9-e4f5e1be4452"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input src:\t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "\n",
            "Q_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "K_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "V_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "\n",
            "Q_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "K_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "V_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "\n",
            "scores:   \t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n",
            "attention:\t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n",
            "\n",
            "x_raw:    \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "x_rsh1:   \t[128, 32, 5, 40]  \t= [n_batch, n_src, n_head, d_head]\n",
            "x_rsh2:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "\n",
            "Output x: \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgfpS2Y6RNcq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}