{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 Tokenizing"
      ],
      "metadata": {
        "id": "2LrsPUw0KJRH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxEPOzL-HofH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "한국어에서의 다양한 tokenizing 방식을 실습해보겠습니다.   \n",
        "\n",
        "한국어는 다음의 단계로 tokenizing이 가능합니다.\n",
        "\n",
        "1. 어절 단위\n",
        "2. 형태소 단위\n",
        "3. 음절 단위\n",
        "4. 자소 단위\n",
        "5. WordPiece 단위"
      ],
      "metadata": {
        "id": "-jwxlLa2KLTo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajq9gruKKLhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. 실습용 데이터 준비"
      ],
      "metadata": {
        "id": "NaHRziWsKOlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습을 위해 한국어 wikipedia 파일을 가져오도록 하겠습니다.   \n",
        "본 wikipedia 파일은 앞선 전처리 실습을 통해 전처리가 완료된 파일입니다.   \n"
      ],
      "metadata": {
        "id": "JGss7Xv2KM8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir my_data"
      ],
      "metadata": {
        "id": "IX8abgOdKNMt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iaWdG8zKR2u",
        "outputId": "5235bf20-e1c0-4f78-ecb4-9eb59fe305d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1323k  100 1323k    0     0  1077k      0  0:00:01  0:00:01 --:--:-- 1077k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 확인해보겠습니다."
      ],
      "metadata": {
        "id": "WzlXMOfQKXL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('my_data/wiki_20190620_small.txt', 'r', encoding = 'utf-8')\n",
        "\n",
        "# 'r' 은 read를 의미합니다.\n",
        "# 본 파일은 encoding format을 UTF-8로 저장했기 때문에, UTF-8로 읽겠습니다.\n",
        "# 한국어는 특히 encoding format이 맞지 않으면, 글자가 깨지는 현상이 나타납니다."
      ],
      "metadata": {
        "id": "v1KFS36oKUvc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = data.readlines() #전체 문장을 list에 저장하는 함수입니다."
      ],
      "metadata": {
        "id": "c7T5HECaKgzu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in lines[0:10]:\n",
        "\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RiztXcWKkxv",
        "outputId": "ba6a161b-6436-4ce5-ad0e-58d44ed9d3de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.\n",
            "\n",
            "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
            "\n",
            "조지아 공과대학교를 졸업하였다.\n",
            "\n",
            "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다.\n",
            "\n",
            "1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.\n",
            "\n",
            "그의 별명이 \"땅콩 농부\" 로 알려졌다.\n",
            "\n",
            "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다.\n",
            "\n",
            "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다.\n",
            "\n",
            "조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
            "\n",
            "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 어절 단위 tokenizing"
      ],
      "metadata": {
        "id": "EFr3HZ3AKq1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어절 단위 tokenizing은 모든 문장을 띄어쓰기 단위로 분리하는 것을 의미합니다.\n",
        "\n",
        "\"이순신은 조선 중기의 무신이다.\" -> [\"이순신은\", \"조선\", \"중기의\", \"무신이다.\"]"
      ],
      "metadata": {
        "id": "V1VmAO-vKsoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "\n",
        "tokenized_text = text.split(\" \") #split 함수는 입력 string에 대해 특정 string을 기반으로 분리\n",
        "\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRrRZZjwKo7p",
        "outputId": "9674d2c7-dffa-4289-f3c0-68443412110b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신은', '조선', '중기의', '무신이다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing의 목적은 크게 두 가지입니다.  \n",
        "1. 의미를 지닌 단위로 자연어를 분절\n",
        "2. Model의 학습 시, 동일한 size로 입력\n",
        "\n",
        "따라서, tokenizer는 특정 사이즈로 token의 개수를 조절하는 함수가 필수로 포함되어야 합니다."
      ],
      "metadata": {
        "id": "cxA1K34gK7kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 위해, token의 개수가 부족할 때는 padding 처리를 해주고,    \n",
        "개수가 많을 때는 token을 잘라서 반환하는 함수를 구현하겠습니다.   "
      ],
      "metadata": {
        "id": "J-vGZRv4LBgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 10\n",
        "\n",
        "#padding\n",
        "\n",
        "tokenized_text += [\"padding\"] * (max_seq_length - len(tokenized_text))\n",
        "\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5v8xPujK5Ci",
        "outputId": "5b00fcfa-5607-4a4f-9a26-6297eda604d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신은', '조선', '중기의', '무신이다.', 'padding', 'padding', 'padding', 'padding', 'padding', 'padding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2\n",
        "\n",
        "#filtering\n",
        "\n",
        "tokenized_text = tokenized_text[0:max_seq_length]\n",
        "\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRqrP3flLPb8",
        "outputId": "43cf0b1d-9fca-4f18-98ce-7a758fa93aed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신은', '조선']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드를 이용해 tokenizer class를 만들어보겠습니다."
      ],
      "metadata": {
        "id": "WHsz8LbtLg0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokenizer_type_list = [\"word\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "\n",
        "    def tokenize(self, text, tokenizer_type):\n",
        "\n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "\n",
        "        if tokenizer_type == \"word\": #띄어쓰기 단위로 분리\n",
        "\n",
        "            tokenized_text = text.split(\" \")\n",
        "\n",
        "        if self.padding:\n",
        "\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "        else:\n",
        "\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "\n",
        "        return texts\n"
      ],
      "metadata": {
        "id": "EKRlw2rqLa_U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 10\n",
        "my_tokenizer.padding = True"
      ],
      "metadata": {
        "id": "3YU2nFCTMg8T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"word\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"word\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IURripTkMpx2",
        "outputId": "06019a1d-bd70-49c4-a3bf-df80ed8c873a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신은', '조선', '중기의', '무신이다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[['이순신은', '조선', '중기의', '무신이다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['그는', '임진왜란을', '승리로', '이끌었다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBOWdxQlM3U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 형태소 단위 tokenizing"
      ],
      "metadata": {
        "id": "afjg-UtjM_te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "형태소 분석기로는 mecab을 사용하겠습니다."
      ],
      "metadata": {
        "id": "nLQ-5TaQM_mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh #install_mecab-ko_on_colab_light_xxxxxx.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx2snio7NB9j",
        "outputId": "ec8b8336-9f55-48f5-d1f8-4ae4be0a8a3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (138/138), 1.72 MiB | 24.74 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2023-07-26 17:19:00--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c5:2ef4, 2406:da00:ff00::22e9:9f55, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNH3QVYWES&Signature=fAlFPpvn5AYn6Vl6LedBhMjw6to%3D&x-amz-security-token=FwoGZXIvYXdzEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDIhm%2FVEiMZ6hAS3ZnSK%2BAe7rPcG1D0GuZil%2F3PYPeA2nbqnf6l%2BHlQ5ebXc4H24vmT%2F1U624AbJZhre85t9oTnUnA2JFA%2B%2FDBxDz1yf0thsuskOqL5pEcrPNfD67yEwJha6cJMco8omPJu4SXEoYvkRAFROpcLWyhaE2BwdakdovsW16Z7ZZE9fd3m4jhk2NeQnb3%2FMe96nSNfWE%2BL6y%2F42AiejkKF1ZMPWGzxckreG8sQQ%2FR%2BnUMIxgKFoAiSimwIB%2Ff55bn0oSnUgdIT4ohquFpgYyLQNtJ3iHM2s7jT4UuviQNAoZzvi2OWb%2Fj0IfjIvzJZ7SfsHTaO4iPhnTl%2FoL6g%3D%3D&Expires=1690393742 [following]\n",
            "--2023-07-26 17:19:02--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNH3QVYWES&Signature=fAlFPpvn5AYn6Vl6LedBhMjw6to%3D&x-amz-security-token=FwoGZXIvYXdzEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDIhm%2FVEiMZ6hAS3ZnSK%2BAe7rPcG1D0GuZil%2F3PYPeA2nbqnf6l%2BHlQ5ebXc4H24vmT%2F1U624AbJZhre85t9oTnUnA2JFA%2B%2FDBxDz1yf0thsuskOqL5pEcrPNfD67yEwJha6cJMco8omPJu4SXEoYvkRAFROpcLWyhaE2BwdakdovsW16Z7ZZE9fd3m4jhk2NeQnb3%2FMe96nSNfWE%2BL6y%2F42AiejkKF1ZMPWGzxckreG8sQQ%2FR%2BnUMIxgKFoAiSimwIB%2Ff55bn0oSnUgdIT4ohquFpgYyLQNtJ3iHM2s7jT4UuviQNAoZzvi2OWb%2Fj0IfjIvzJZ7SfsHTaO4iPhnTl%2FoL6g%3D%3D&Expires=1690393742\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.123.241, 3.5.25.162, 3.5.28.132, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.123.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.06MB/s    in 0.4s    \n",
            "\n",
            "2023-07-26 17:19:03 (3.06 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2023-07-26 17:21:08--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22c0:3470, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNILJYXEHY&Signature=2Qf7uzd635OP3CLR5ulmrnJtiB0%3D&x-amz-security-token=FwoGZXIvYXdzEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDPQlQjcn3Xu89uMwEiK%2BAUZBDHxTC4eFgSOTM7WqwWiUNCbSpebDJOJppdsSK4FiRu9qoRjMpYITkZFmTtPNffvuqQQq5a8RTfQ1UcU0hUNTBhePpBRZ3ji5Vsvw0jSMAPBjnvtgSFUFkAN%2BtD%2FsXVMzysh6v3Ywfw3Wxf8dAqoRFG1YpHHxzBSBio1gRLizE3bbk17oPTRpB9JPaIguIWkeK4XzaQk8B7cBT4qCn%2F09plyTr5ZYGjPbH%2BUW2T4zAan0rTQV5j7WoSgdOBcohayFpgYyLdKIxy4LLPv6u8yuOvN4IOwxWun7e8tLL%2BzSTNSlckDryqQRtIaCoENYPaNXwA%3D%3D&Expires=1690393869 [following]\n",
            "--2023-07-26 17:21:09--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNILJYXEHY&Signature=2Qf7uzd635OP3CLR5ulmrnJtiB0%3D&x-amz-security-token=FwoGZXIvYXdzEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDPQlQjcn3Xu89uMwEiK%2BAUZBDHxTC4eFgSOTM7WqwWiUNCbSpebDJOJppdsSK4FiRu9qoRjMpYITkZFmTtPNffvuqQQq5a8RTfQ1UcU0hUNTBhePpBRZ3ji5Vsvw0jSMAPBjnvtgSFUFkAN%2BtD%2FsXVMzysh6v3Ywfw3Wxf8dAqoRFG1YpHHxzBSBio1gRLizE3bbk17oPTRpB9JPaIguIWkeK4XzaQk8B7cBT4qCn%2F09plyTr5ZYGjPbH%2BUW2T4zAan0rTQV5j7WoSgdOBcohayFpgYyLdKIxy4LLPv6u8yuOvN4IOwxWun7e8tLL%2BzSTNSlckDryqQRtIaCoENYPaNXwA%3D%3D&Expires=1690393869\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.131.163, 52.217.231.209, 54.231.137.105, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.131.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  26.8MB/s    in 1.8s    \n",
            "\n",
            "2023-07-26 17:21:11 (26.8 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)\n",
            "https://github.com/konlpy/konlpy/issues/395#issue-1099168405 - 2022.01.11\n",
            "Done\n",
            "Install mecab-python\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n",
            "light 버전 작성 : Dogdriip님 ( https://github.com/Dogdriip )\n",
            "문제를 해결해주신 combacsa님 감사합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()\n",
        "print(mecab.pos(\"아버지가방에들어가신다.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be_oNYh9Nebr",
        "outputId": "f1c7aa55-ca8a-44a8-d0f0-57d8e3b360b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EF'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "\n",
        "# 이순신 -> PS\n",
        "# 조선 -> DT TI\n",
        "# 중기 -> TI\n",
        "# 무신 -> OC\n",
        "# 이순신 - 직업 - 무신\n",
        "# 이순신 - 출생지 - 조선\n",
        "\n",
        "tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi4zUFI5NxvG",
        "outputId": "c2593ea4-7480-4595-88f4-72a6547b9132"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "형태소 tokenizer도 class에 추가하겠습니다."
      ],
      "metadata": {
        "id": "k7lx_WsyR7bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "\n",
        "    def tokenize(self, text, tokenizer_type):\n",
        "\n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "\n",
        "        if tokenizer_type == \"word\": #띄어쓰기 단위\n",
        "\n",
        "            tokenized_text = text.split(\" \")\n",
        "\n",
        "        elif tokenizer_type == \"morph\": #형태소 단위\n",
        "\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "\n",
        "        if self.padding:\n",
        "\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "        else:\n",
        "\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "\n",
        "        return texts"
      ],
      "metadata": {
        "id": "nMu9GsPNR2Fb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 10\n",
        "my_tokenizer.padding = True"
      ],
      "metadata": {
        "id": "V3voa9r7S1g2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"morph\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\",\"그는 임진왜란을 승리로 이끌었다.\"], \"morph\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBt303XNS_jh",
        "outputId": "6ea1a5bf-6070-4003-a0ba-ace002f70148"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.', '[PAD]']\n",
            "[['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.', '[PAD]'], ['그', '는', '임진왜란', '을', '승리', '로', '이끌', '었', '다', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqeOAFTBTMzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 음절 단위 tokenizing"
      ],
      "metadata": {
        "id": "sjNfuCcPTRAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "음절 단위 tokenizing은 한 자연어를 한 글자씩 분리합니다."
      ],
      "metadata": {
        "id": "mmS3kGEbTSwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "\n",
        "tokenized_text = list(text)\n",
        "\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0d8xvqsTROu",
        "outputId": "512cdca8-d2db-45c0-b65c-71d3fe0f1513"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "\n",
        "    def tokenize(self, text, tokenizer_type):\n",
        "\n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "\n",
        "        if tokenizer_type == \"word\": #띄어쓰기 단위\n",
        "            tokenized_text = text.split(\" \")\n",
        "\n",
        "        elif tokenizer_type == \"morph\": #형태소 단위\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "\n",
        "        elif tokenizer_type == \"syllable\": #글자 단위\n",
        "            tokenized_text = list(text)\n",
        "\n",
        "        if self.padding:\n",
        "\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "        else:\n",
        "\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "\n",
        "        return texts\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "kM-TIM1_TahI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 20\n",
        "my_tokenizer.padding = True"
      ],
      "metadata": {
        "id": "Ug65GDYCUW9l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"syllable\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"syllable\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XyREUEoUrUj",
        "outputId": "eab85ea5-12dc-40b8-dc35-1071d8c30ced"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.', '[PAD]', '[PAD]', '[PAD]']\n",
            "[['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기', '의', ' ', '무', '신', '이', '다', '.', '[PAD]', '[PAD]', '[PAD]'], ['그', '는', ' ', '임', '진', '왜', '란', '을', ' ', '승', '리', '로', ' ', '이', '끌', '었', '다', '.', '[PAD]', '[PAD]']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBsqoE8CU2kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자소 단위 tokenizing"
      ],
      "metadata": {
        "id": "JFNKeEIXU5eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한글은 하나의 문자도 최대 초성, 중성, 종성, 총 3개의 자소로 분리가 가능합니다.   \n",
        "실습에서는 자소 분리를 위해 hgtk 라이브러리를 사용하겠습니다."
      ],
      "metadata": {
        "id": "h49CUoP2U7Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hgtk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juNScM4pU5qF",
        "outputId": "8c8a1080-e60c-4fa8-c0ac-60ceb0223ea4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hgtk\n",
            "  Downloading hgtk-0.2.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: hgtk\n",
            "Successfully installed hgtk-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hgtk"
      ],
      "metadata": {
        "id": "6_YCq-8rVHnv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = list(hgtk.text.decompose(text))\n",
        "print(tokenized_text)\n",
        "# ㅇ ㅣ ㅅ ㅜ ㄴ ㅅ ㅣ ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syY2HCdVLwO",
        "outputId": "483a47fd-d445-413b-b35d-6ec30d29de88"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ', 'ㅓ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅜ', 'ㅇ', 'ᴥ', 'ㄱ', 'ㅣ', 'ᴥ', 'ㅇ', 'ㅢ', 'ᴥ', ' ', 'ㅁ', 'ㅜ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅣ', 'ᴥ', 'ㄷ', 'ㅏ', 'ᴥ', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\", \"jaso\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "\n",
        "    def tokenize(self, text, tokenizer_type):\n",
        "\n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "\n",
        "        if tokenizer_type == \"word\": #띄어쓰기 단위\n",
        "            tokenized_text = text.split(\" \")\n",
        "\n",
        "        elif tokenizer_type == \"morph\": #형태소 단위\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "\n",
        "        elif tokenizer_type == \"syllable\": #글자 단위\n",
        "            tokenized_text = list(text)\n",
        "\n",
        "        elif tokenizer_type == \"jaso\": #자소 단위\n",
        "            tokenized_text = list(hgtk.text.decompose(text))\n",
        "\n",
        "        if self.padding:\n",
        "\n",
        "            tokenized_text += [self.pad_token]*(self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "        else:\n",
        "\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "\n",
        "            texts[i] = self.tokenize(text,tokenizer_type)\n",
        "\n",
        "        return texts"
      ],
      "metadata": {
        "id": "2LGEnbqeVWrL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 20\n",
        "my_tokenizer.padding = True"
      ],
      "metadata": {
        "id": "GCYMom9iWRNT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\",\"jaso\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"],\"jaso\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9QxUvi5WYBh",
        "outputId": "d2d4af08-88b7-49f8-b2f7-bc35b5abe846"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ']\n",
            "[['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ'], ['ㄱ', 'ㅡ', 'ᴥ', 'ㄴ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅇ', 'ㅣ', 'ㅁ', 'ᴥ', 'ㅈ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅙ', 'ᴥ', 'ㄹ']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QYvOnbcWpuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordPiece tokenizing"
      ],
      "metadata": {
        "id": "wLkkUMZcW1Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61SRYkm6W1Ri",
        "outputId": "99a954d5-33aa-41a8-fe55-7196d6ee9fe4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir wordPieceTokenizer"
      ],
      "metadata": {
        "id": "yIClpY_wW3G5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "#initialize an empty tokenizer\n",
        "wp_tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text = True, #[이순신, ##은, ' ', 조선]\n",
        "    handle_chinese_chars = True,\n",
        "    strip_accents = False, #True: [YepHamza] -> [Yep, Hamza]\n",
        "    lowercase = False\n",
        ")"
      ],
      "metadata": {
        "id": "5hbTJk4fW8v_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and then train\n",
        "wp_tokenizer.train(\n",
        "    files = \"/content/my_data/wiki_20190620_small.txt\",\n",
        "    vocab_size = 10000,\n",
        "    min_frequency = 2,\n",
        "    show_progress = True,\n",
        "    special_tokens =[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"],\n",
        "    limit_alphabet = 1000,\n",
        "    wordpieces_prefix=\"##\",\n",
        ")"
      ],
      "metadata": {
        "id": "fbO58Jd2XWc6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the files\n",
        "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iO4asqrXtUt",
        "outputId": "4898720c-f337-4934-bbb2-c9363c28988b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wordPieceTokenizer/my_tokenizer-vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wp_tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPMUvqoQX-vc",
        "outputId": "af47c2ce-cb36-491a-beec-d6fe7c969367"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = wp_tokenizer.encode(text)\n",
        "print(tokenized_text)\n",
        "print(tokenized_text.tokens)\n",
        "print(tokenized_text.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so3-Vr_oYVtJ",
        "outputId": "8a94a820-1ddb-4b42-e010-e52bd45a6aee"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n",
            "[705, 1187, 7631, 2002, 753, 2606, 452, 8524, 1066, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JVGXKOxYeTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final tokenizer"
      ],
      "metadata": {
        "id": "8hXSXG-RYg1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\", \"jaso\", \"wordPiece\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "\n",
        "    def tokenize(self, text, tokenizer_type):\n",
        "\n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "\n",
        "        if tokenizer_type == \"word\": #띄어쓰기 단위\n",
        "            tokenized_text = text.split(\" \")\n",
        "\n",
        "        elif tokenizer_type == \"morph\": #형태소 단위\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "\n",
        "        elif tokenizer_type == \"syllable\": #글자 단위\n",
        "            tokenized_text = list(text)\n",
        "\n",
        "        elif tokenizer_type == \"jaso\": #자소 단위\n",
        "            tokenized_text = list(hgtk.text.decompose(text))\n",
        "\n",
        "        elif tokenizer_type == \"wordPiece\": #custom wordpiece tokenizer\n",
        "            tokenized_text = wp_tokenizer.encode(text).tokens\n",
        "\n",
        "        if self.padding:\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "        else:\n",
        "\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "\n",
        "\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "\n",
        "        return texts"
      ],
      "metadata": {
        "id": "-iFKzpXaYhk6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 10\n",
        "my_tokenizer.padding = True"
      ],
      "metadata": {
        "id": "UNUmSV-AZmNi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"wordPiece\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"wordPiece\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KeAhzNQZwA7",
        "outputId": "bbf034db-6c07-4585-a0f1-2eeb89094a08"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n",
            "[['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.'], ['그는', '임진', '##왜', '##란을', '승리', '##로', '이끌었다', '.', '[PAD]', '[PAD]']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNWU6QkmZ6Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "구현된 tokenizing 함수들을 모두 확인해보겠습니다."
      ],
      "metadata": {
        "id": "-fKPmkqMZ-Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"word\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"morph\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"syllable\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"jaso\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"wordPiece\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-1lQGuoZ-XI",
        "outputId": "08677a9b-d329-43ad-e0f2-25dea7e96521"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이순신은', '조선', '중기의', '무신이다.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "['이순신', '은', '조선', '중기', '의', '무신', '이', '다', '.', '[PAD]']\n",
            "['이', '순', '신', '은', ' ', '조', '선', ' ', '중', '기']\n",
            "['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ']\n",
            "['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NzUrIT80aARi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}