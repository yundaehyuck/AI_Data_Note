{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1u25xNNJZYwE"
      },
      "outputs": [],
      "source": [
        "#library\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_AH1z71ZeMm",
        "outputId": "a84b1cda-dd28-42b0-9d01-3fa62bf9daaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f62f296d290>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 4\n",
        "hidden_size = 2"
      ],
      "metadata": {
        "id": "Mlq7bpwSZfiw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = [1,0,0,0]\n",
        "e = [0,1,0,0]\n",
        "l = [0,0,1,0]\n",
        "o = [0,0,0,1]\n",
        "\n",
        "input_data_np = np.array([[h,e,l,l,o],[e,o,l,l,l],[l,l,e,e,l]],dtype = np.float32)"
      ],
      "metadata": {
        "id": "T5UOAkVsZiJH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.Tensor(input_data_np)"
      ],
      "metadata": {
        "id": "5l5UqN4fZtjI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHWRw2-VZwNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch RNN"
      ],
      "metadata": {
        "id": "7nNWmLGSZxSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = torch.nn.RNN(input_size, hidden_size)\n",
        "\n",
        "outputs, _status = rnn(input_data)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyEfxp5BZyEM",
        "outputId": "2de5b90f-67ac-4020-c269-76d5dc635ff9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.7497, -0.6135],\n",
            "         [-0.5282, -0.2473],\n",
            "         [-0.9136, -0.4269],\n",
            "         [-0.9136, -0.4269],\n",
            "         [-0.9028,  0.1180]],\n",
            "\n",
            "        [[-0.5753, -0.0070],\n",
            "         [-0.9052,  0.2597],\n",
            "         [-0.9173, -0.1989],\n",
            "         [-0.9173, -0.1989],\n",
            "         [-0.8996, -0.2725]],\n",
            "\n",
            "        [[-0.9077, -0.3205],\n",
            "         [-0.8944, -0.2902],\n",
            "         [-0.5134, -0.0288],\n",
            "         [-0.5134, -0.0288],\n",
            "         [-0.9127, -0.2222]]], grad_fn=<StackBackward0>)\n",
            "torch.Size([3, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status) #hidden vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8fBC6H1Z5uh",
        "outputId": "f392a69d-396f-497d-9bf3-c40de5bb4ab1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.9077, -0.3205],\n",
            "         [-0.8944, -0.2902],\n",
            "         [-0.5134, -0.0288],\n",
            "         [-0.5134, -0.0288],\n",
            "         [-0.9127, -0.2222]]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ls6SmRylZ7wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch RNN training"
      ],
      "metadata": {
        "id": "E5xBHsMqaquq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = ['h','i','e','l','o']\n",
        "\n",
        "input_size = len(char_set)\n",
        "\n",
        "hidden_size = len(char_set)\n",
        "\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "YccnGMs6aKvr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [[0,1,0,2,3,3]]\n",
        "\n",
        "x_one_hot = [[[1,0,0,0,0],\n",
        "              [0,1,0,0,0],\n",
        "              [1,0,0,0,0],\n",
        "              [0,0,1,0,0],\n",
        "              [0,0,0,1,0],\n",
        "              [0,0,0,1,0]]]\n",
        "\n",
        "y_data = [[1,0,2,3,3,4]]"
      ],
      "metadata": {
        "id": "g5hABkT0aReh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor(x_one_hot)\n",
        "y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "6EEWptVtakJN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = torch.nn.RNN(input_size, hidden_size, batch_first = True)"
      ],
      "metadata": {
        "id": "TPr_BG3Ban5p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "5aoDTxnkawZK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs, _status = rnn(x)\n",
        "\n",
        "    loss = criterion(outputs.view(-1,input_size), y.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis = 2)\n",
        "\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "\n",
        "    print(i,'loss:', loss.item(), 'prediction:', result, 'true y:', y_data, 'prediction str:', result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg0b9qoKa1sc",
        "outputId": "d91e3130-672f-4842-94bf-de8dbeb6342a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 1.6659356355667114 prediction: [[3 2 3 2 2 2]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: leleee\n",
            "1 loss: 1.3435620069503784 prediction: [[3 3 3 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llllll\n",
            "2 loss: 1.1607609987258911 prediction: [[3 3 3 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llllll\n",
            "3 loss: 1.0613101720809937 prediction: [[3 3 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llelll\n",
            "4 loss: 1.0018833875656128 prediction: [[3 3 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llelll\n",
            "5 loss: 0.9612741470336914 prediction: [[3 3 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llelll\n",
            "6 loss: 0.9296206831932068 prediction: [[3 3 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llelll\n",
            "7 loss: 0.900706946849823 prediction: [[3 3 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llello\n",
            "8 loss: 0.8716586232185364 prediction: [[3 3 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llello\n",
            "9 loss: 0.840173065662384 prediction: [[3 3 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llello\n",
            "10 loss: 0.8042361736297607 prediction: [[1 3 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llello\n",
            "11 loss: 0.7646371722221375 prediction: [[1 3 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: llelll\n",
            "12 loss: 0.7293186187744141 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "13 loss: 0.7165681719779968 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "14 loss: 0.6902549266815186 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "15 loss: 0.6629135608673096 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "16 loss: 0.6484732627868652 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "17 loss: 0.6375202536582947 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "18 loss: 0.6257078647613525 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "19 loss: 0.6134440898895264 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "20 loss: 0.6095907092094421 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "21 loss: 0.5962169766426086 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "22 loss: 0.585158109664917 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "23 loss: 0.5790695548057556 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "24 loss: 0.5743274688720703 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "25 loss: 0.5704931616783142 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "26 loss: 0.5668087005615234 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "27 loss: 0.5621005892753601 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "28 loss: 0.5574932098388672 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "29 loss: 0.5542095303535461 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "30 loss: 0.5517210364341736 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "31 loss: 0.5493166446685791 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "32 loss: 0.5470219254493713 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "33 loss: 0.5452892780303955 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "34 loss: 0.5440242886543274 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "35 loss: 0.5425464510917664 prediction: [[1 0 2 3 3 3]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhelll\n",
            "36 loss: 0.5410192608833313 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "37 loss: 0.5398886799812317 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "38 loss: 0.538909375667572 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "39 loss: 0.5377857089042664 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "40 loss: 0.5366470813751221 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "41 loss: 0.5357503294944763 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "42 loss: 0.5349513292312622 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "43 loss: 0.5339869260787964 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "44 loss: 0.5330380797386169 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "45 loss: 0.5322182178497314 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "46 loss: 0.5313381552696228 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "47 loss: 0.5302885174751282 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "48 loss: 0.5291419625282288 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "49 loss: 0.5278935432434082 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "50 loss: 0.5263164639472961 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "51 loss: 0.5241955518722534 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "52 loss: 0.5213882327079773 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "53 loss: 0.5177200436592102 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "54 loss: 0.5134819746017456 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "55 loss: 0.5102964043617249 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "56 loss: 0.5095341205596924 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "57 loss: 0.5085729956626892 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "58 loss: 0.5051264762878418 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "59 loss: 0.5012237429618835 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "60 loss: 0.4976901710033417 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "61 loss: 0.4940367639064789 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "62 loss: 0.49089956283569336 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "63 loss: 0.4887775480747223 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "64 loss: 0.4872687757015228 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "65 loss: 0.48538243770599365 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "66 loss: 0.4828859567642212 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "67 loss: 0.4805332124233246 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "68 loss: 0.47891685366630554 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "69 loss: 0.47726985812187195 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "70 loss: 0.47513461112976074 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "71 loss: 0.47355711460113525 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "72 loss: 0.47258591651916504 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "73 loss: 0.47143879532814026 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "74 loss: 0.47027587890625 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "75 loss: 0.4696432054042816 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "76 loss: 0.4689140021800995 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "77 loss: 0.4678760766983032 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "78 loss: 0.46722695231437683 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "79 loss: 0.4666273891925812 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "80 loss: 0.4658105671405792 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "81 loss: 0.4652121067047119 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "82 loss: 0.46482744812965393 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "83 loss: 0.46424272656440735 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "84 loss: 0.4638115167617798 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "85 loss: 0.4635121822357178 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "86 loss: 0.4630613625049591 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "87 loss: 0.4626632630825043 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "88 loss: 0.46239134669303894 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "89 loss: 0.46198198199272156 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "90 loss: 0.4616352617740631 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "91 loss: 0.4613693952560425 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "92 loss: 0.4610212743282318 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "93 loss: 0.4607293903827667 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "94 loss: 0.46050524711608887 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "95 loss: 0.4602121412754059 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "96 loss: 0.4599829614162445 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "97 loss: 0.4597799777984619 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "98 loss: 0.45953118801116943 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n",
            "99 loss: 0.45933184027671814 prediction: [[1 0 2 3 3 4]] true y: [[1, 0, 2, 3, 3, 4]] prediction str: lhello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# long sentence"
      ],
      "metadata": {
        "id": "isto0pcCc72a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = ' if you want you'"
      ],
      "metadata": {
        "id": "_Z8bM6PtbW5N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sample))\n",
        "\n",
        "char_dic = {c: i for i,c in enumerate(char_set)}\n",
        "\n",
        "print(char_dic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfkbNkJRbi71",
        "outputId": "1924c921-5264-4542-d283-74553dc51e7e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'t': 0, 'o': 1, 'u': 2, 'a': 3, 'y': 4, 'i': 5, 'n': 6, 'f': 7, 'w': 8, ' ': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "hidden_size = len(char_dic)\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "rCUa-R2GbniQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = [char_dic[c] for c in sample]\n",
        "\n",
        "x_data = [sample_idx[:-1]]\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "y_data = [sample_idx[1:]]"
      ],
      "metadata": {
        "id": "UKQJDNqGbrsH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor(x_one_hot)\n",
        "y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXrLHqlXb1mg",
        "outputId": "3ac5b1d8-a3bd-43cf-9bfb-dcacf6dc9049"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-0520fb99e116>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  x = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = torch.nn.RNN(dic_size, hidden_size, batch_first = True)"
      ],
      "metadata": {
        "id": "Iqmy-ES0b8lT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "PV4YD3lKcDCm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "for i in range(50):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs, _status = rnn(x)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, dic_size), y.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis = 2)\n",
        "\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "\n",
        "    print(i, 'loss:', loss.item(), 'prediction:', result, 'true y:', y_data, 'prediction str:', result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fblBSzk6cHud",
        "outputId": "8b9bdb46-1eeb-4251-e739-04517d9c5e7c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 2.2741920948028564 prediction: [[9 0 7 9 9 7 0 9 7 6 0 7 9 2 7]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str:  tf  ft fntf uf\n",
            "1 loss: 2.0447263717651367 prediction: [[9 2 9 9 9 2 9 9 3 9 2 9 9 9 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str:  u   u  a u   u\n",
            "2 loss: 1.7993568181991577 prediction: [[9 2 9 4 1 2 9 2 3 9 2 9 4 9 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str:  u you ua u y u\n",
            "3 loss: 1.6393059492111206 prediction: [[4 2 9 4 1 2 9 4 3 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: yu you ya t you\n",
            "4 loss: 1.4978060722351074 prediction: [[4 1 9 4 1 2 9 4 3 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: yo you ya t you\n",
            "5 loss: 1.3884620666503906 prediction: [[4 1 9 4 1 2 9 8 3 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: yo you wa t you\n",
            "6 loss: 1.2922332286834717 prediction: [[4 1 9 4 1 2 9 8 1 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: yo you wo t you\n",
            "7 loss: 1.2138755321502686 prediction: [[5 1 9 4 1 2 9 8 1 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: io you wo t you\n",
            "8 loss: 1.1405367851257324 prediction: [[5 1 9 4 1 2 9 8 1 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: io you wo t you\n",
            "9 loss: 1.090843677520752 prediction: [[5 7 9 4 1 2 9 8 3 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you wa t you\n",
            "10 loss: 1.0408360958099365 prediction: [[5 7 9 4 1 2 9 8 3 9 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you wa t you\n",
            "11 loss: 1.0045870542526245 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "12 loss: 0.9755343794822693 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "13 loss: 0.9485270380973816 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "14 loss: 0.9281152486801147 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "15 loss: 0.9132986664772034 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "16 loss: 0.9013258814811707 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "17 loss: 0.8930460214614868 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "18 loss: 0.8850751519203186 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "19 loss: 0.8765869140625 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "20 loss: 0.8686766028404236 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "21 loss: 0.8613290190696716 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "22 loss: 0.8541983366012573 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "23 loss: 0.847614586353302 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "24 loss: 0.8421689867973328 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "25 loss: 0.8390255570411682 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "26 loss: 0.8366023898124695 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "27 loss: 0.8325966000556946 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "28 loss: 0.8313268423080444 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "29 loss: 0.830905020236969 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "30 loss: 0.8294092416763306 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "31 loss: 0.8272984027862549 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "32 loss: 0.826208233833313 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "33 loss: 0.8251383304595947 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "34 loss: 0.8230296969413757 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "35 loss: 0.822070837020874 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "36 loss: 0.8214303255081177 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "37 loss: 0.8203542232513428 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "38 loss: 0.8192527890205383 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "39 loss: 0.818848729133606 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "40 loss: 0.8184306025505066 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "41 loss: 0.8175459504127502 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "42 loss: 0.8171268701553345 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "43 loss: 0.8168736100196838 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "44 loss: 0.816381573677063 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "45 loss: 0.8158447742462158 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "46 loss: 0.8156306147575378 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "47 loss: 0.8154051303863525 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "48 loss: 0.814957857131958 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n",
            "49 loss: 0.8147245049476624 prediction: [[5 7 9 4 1 2 9 8 3 6 0 9 4 1 2]] true y: [[5, 7, 9, 4, 1, 2, 9, 8, 3, 6, 0, 9, 4, 1, 2]] prediction str: if you want you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# very long sentence"
      ],
      "metadata": {
        "id": "nVKCA93lc4e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "224jbDIvci6n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6FTQrDXycuFA",
        "outputId": "72c40ffb-b224-4639-cf75-ca464db2255b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence))\n",
        "\n",
        "char_dic = {c: i for i,c in enumerate(char_set)}"
      ],
      "metadata": {
        "id": "JnUkJ2jhcvT9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "\n",
        "hidden_size = len(char_dic)\n",
        "\n",
        "sequence_length = 10\n",
        "\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "jE8mIbdudCe8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i+1:i + sequence_length+1]\n",
        "\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])\n",
        "    y_data.append([char_dic[c] for c in y_str])\n",
        "\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oac-1fnudHfz",
        "outputId": "18195aa8-cf08-421a-d7d9-dffb5670fb34"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor(x_one_hot)\n",
        "\n",
        "y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "udFtXRJxdkCD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "\n",
        "        super(Net,self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers = layers,\n",
        "                                batch_first = True)\n",
        "\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias = True)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x,_status = self.rnn(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "80vJDlPVdqju"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "zCpvojf7eI67"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(x)\n",
        "\n",
        "    loss = criterion(outputs.view(-1,dic_size), y.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    results = outputs.argmax(dim = 2)\n",
        "\n",
        "    predict_str = ''\n",
        "\n",
        "    for j, result in enumerate(results):\n",
        "\n",
        "        if j == 0:\n",
        "\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "\n",
        "        else:\n",
        "\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7OJFD4feOQ3",
        "outputId": "66f2fdc0-dba3-4d7f-95cc-2391f608fe30"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",nn,nnnnnn,n,nnnn, nnnnnn,nnnnnnnnnnnnnnnnnn,nnntn,nnnnn,nn,nnnnnntnnnnnnnnnnnnnnnnnnnnnnnnn,n,nn,nnnnnnnnnnnnnnnnn,nnn,n,nn,,nnnnnn,nn,nnnnnnnnnnn,n,ntnnnnnnn,nnnnnnnnnnnnnn,nnnn\n",
            "                                                                                                                                                                                   \n",
            "mm.oem.otmmmmemmmemmmmmmmmemmmmmmmmmmmmmemmmmmmmmmppmmmyemyyemmyemmmmmmmmmmmemmmemmytemmmmemmymmmmemm.tmmmmmemm.emmmmmmmmmmmyemmytmmmmmyemmyemmmmmmmeemypmmymmmmmmmmemm.mmmmmmmmmmm\n",
            "thherhdhorrrerrreeeeereueeeeeerreeerreeeereerrreearererereerrreeeeeereereeeerrreeredeerreeererreeerrureerureareueereeereeureeerreerereeerrreeerereeereeerreureeeeeeerreearreerreerr\n",
            "t oaret oaaaaaaaaaaaaaaraaooaaaraaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaraaaaaaaaaaaaaaaaaaaaraaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "  o cle                        d                  o        o  o      o     o                 d              o o     o  o          o   ooo              o     o    o  n  o    o     \n",
            "   tht  th   tttdl   ttte    ttt    th   ttttl   ttn   t     tt t  t  tt    tl   t    th    tth   tth    tt  t     t   t     tth   thh   tt l   ttt tt   ttt   t    tt   t t ht   t\n",
            "  ttm  ee   e       t       t                              t              te   t    t            e             u     t                 t           t   u                    t    e \n",
            "  te  te   t                     e                        t                                  e                       e             e   t           e                               \n",
            "  t   to                   t                                                                                                                                                       \n",
            "  to  ton  to  t   too     too  ton     o  t   tooo   o   to  t      tt   to  tto  too     oo   ton   o   tt    oo  ton    oo         to  t   to  ton  tn   o  t       t   to   to \n",
            "  to  tos  to       oo     too  too    to       o      s  to         t s  tos t s t o           tos   o         to  t n    to    t    to      to  t s t s   o          t   t   t o \n",
            "  to  tos  to t n   tooe   ttos t oo   to t t   oth    r  to tto     tos  tos thr thooe   tths  too  ttts tto  tths tht    ton   t s  to t    tor tos t     t tt  t t  tt  t e tto \n",
            "  ta  toe  toetha   toaee  ttoe thaoee to tot   ttoe  he  toetha  t  toe  toe thr toaoee  ttoe  toeettthe tor  tthe toth   toe hethe  toetha  tor the toe ttthtee   tt tteethe toa \n",
            "  ta  toe  to   e    ht      h    t    t          h       to         to   to      tht       he  toe   th   o      e t      to      e  to      to  the t       t         t   he t   \n",
            "m toeeoose toetoaehttoaoe  taoe toaoae ta toa e aaoe toe  toetoaeoaootoe  toe toa toaoaee ttoe  toshaoaoe toe taaos toshe  tosehetoe  toetoae toe toe tos taoaeoo ttotoaaooeoe tose\n",
            "m tor tos  tortote  toshe  ttoe toaoe  teet ar ttaoe toe  tod oeltte toe  toe tor toaoe   ttoe  toe ttaoe toe tttae tos e ttoe   toe  tod oel toe toe toe ttastee tttt tae toe toe \n",
            "g tod toe  todt el  tnshe  ttoe thtoe  te t tr t the  he  todl dl    tod  tod tor thtse    the  toe   tsd tor   tse toe e  toe   toe  todl d  tod the toe t sst        ttr toe toe \n",
            "g tor tosl torl i   tnshe   tod theod  ti t t  t tod  hi  torlhr     tor  tnd tor thens    the  tos   tod tor   t i tos e  tod h the  torl d  tor thi thd t s t        tar toe tod \n",
            "g tor tosd torthi   tnshi   ton thtos  tn t    t tor   s  torth    s tor  tn  ton thtnss   tos  toshs tod tor   t i toshi  tonsh ths  torth   tor ths t   t sstn        ar tos ton \n",
            "g ton ton  to t tm  tnshem  ton t to   tn ton  t to    i  to th  e s ton  tn  ton t anssm  tos  tosssstt  ton   t t toshi  tonsh thi  to ton  ton thi t   t sstn        an toe tonc\n",
            "g to  tong to t tl  tnshel  to  t a    tn ton  t to    e  to th  e   to   tn  to  t anssm  toe  tos ssts  to    t t tothe  to ch them to te ' to  toe eo  e shtn  t     an toemeonc\n",
            "g to 'ton' to l tlt tnshel  ao 't a    tn toa lt to   hel to lh le t to ' tn  to 't anssm  the  tos shts  to '  but t thel to ch the  to le ' ton the tog e ssts  t     an toe tonc\n",
            "g to 'tos' to l tlt tsshem  aoe'thaou  tn toa lt th   he  to lh le e to ' ts  ton'thasssm  the  tossssts  to '  tut toshe  toeth the  to le ' ton the eog esssts  t    uan the eonc\n",
            "g ton'tant tonlutlt tsshem  ton't aoum tn toanlt thn the  tonlh le t ton' tsg ton't asssm mthe  tosssstsg ton   tut tnthe  tonth the  to len' ton the tag essstn  t  t uan toe tons\n",
            "g to ' att ao l tl  tsshem  to 't aou' tn eoa  e th   he  to lh  e i to   as  to 't asss   the  tos huist to    but tothe  theth the  ao len' tor the eod essstn  et t uan the eost\n",
            "g ton'tant to butnt anshem  ton't aou' tn toanltsthn ther to bhnle t ton' tnd ton't ansim tthe  tosk uand ton   but tnther tonth the  to ben' tor the tad essstn  nt m uan toe tont\n",
            "g ton'tast to built asshim  ton't aoug tn toanlesthn thel to bh less tor' tst ton't anssd tthe  aossssand tor   bui tnthel tonthdthem to ben' tor the tnd essstn  ntst uan toeleoss\n",
            "g ton' ant to luilt tnshim  lon't aoug tp pon lesth  thel to lh le t to g tnt ton't anssm tthe  tosk  and to k  bui t them touch lhem to lenm tor the end essstn  ntit  an themeost\n",
            "g to ' ant to luilt anshem  lon't aoug tp pon le thg ther to lh le t to g tnd ton't anss  tthe  tosk  and to k  bui t the  to ch bhe  to leng tor the end esssin  nsit uar themeosc\n",
            "d ton',ast to builc anshem  don't aoug tp peonlesthg ther to ch lest to ' tnt ton't anssd tthem tosi  and to k, bui t the  to ch bhem to ben' ton themend esssip  nss  uan themeost\n",
            "p ton',ast to cuilc anshem  ton't arug tp peonle thg ther thgth le t ton' tnt ton't ansit tthem tosi  and ton , bui t ther thsth them to ben' ton themend ess ip  nsit  an themeest\n",
            "p ton',ant to luill asshim  don't arug tp peo le to  ther to lh le t tong asd ton't assig ttoe  aosks and ton , bui tosher to ch them ao len' tor toe endlesssin entit eon toemeosc\n",
            "p tog'wapt to cuild tnshem  t n't aru',tp penglesthg ther thgch le t to ' tnt ton't assit tthem tosks tnd to ', bui t ther thdth them to ben' ton themtnd essit    t t  af themthtt\n",
            "p ton'want uo build asship  don't arug tp pen lesto  ther to lhlle t wo g ast ton't assig  them aosks and to k, bui t ther tosch them ao len' tor themendlesssil entit  an whemeosc\n",
            "plton'want to luill anshep  don't arug tp peonle to kther to bonlecs wong andlton't ansig  toemeaosks andlton , bui tosher torchltoemeao len' tor toemen less inlens   eon woemeosc\n",
            "p ton'wast to cuild anship  don't arug tp people the ther to co le t wo ' tnt ton't ansig  them tosks tnd to k, bui t ther touch them to beng tor themend ess in  ntit  an themeosc\n",
            "p tougwast to cuild anship, ton't arug tp pendlesthe ther to co le t torg tnt ton't ansig  them tosks tnt tork, bui t ther th ch them to ce g tor themend ess im  ctit  om themeo t\n",
            "p,tougwast to cuild asship, don't arug tp peoplesthg ther to collect word ast ton't assig  them tosks and tork, bui t ther toach them to long tor themend ess ilm ntit  of themeoac\n",
            "p,tondwart to luild ars ep, oon't arug tp people to kther to bollect worg art tordt ansig  the  tosks and tork, bui t ther torchether to long tor toe endless ilmentit  on whe eorc\n",
            "p tougwast to build anship, don't arug tp people thg ther to bollect wor' and ton't ansig  the  tosks and tork, bui t ther toach the  to long tor the end ess immensit  of the eaat\n",
            "p wougwast to build anship, don't arug tp peoplesthg them to bollect wo ' and ton't ansign them tosks and to k, bui t ther toach them to long tor themend ess imm ntit  ap themeaat\n",
            "g wougwand to cuild anshipl don't arum up people to  ther to bolle t wor' and won't assign the  tosks and work, bui uather toach the  to long uor the end ess immensit  af the eaac\n",
            "glwoumwand to luimd anshipl don't arum up peonle to  ther to lollect wor' and won't ansign them tosks and work, bui uather toach them to long uor the endless immensit  af the eoac\n",
            "g woumwant to luild anship, don't arum up peonle tog ther to lollect wor' and won't ansign them tosks and work, bui father toach them to long uor themendless immensit  of themeoac\n",
            "g waumwant to luimd anship, don't arum up people to  ther to lollect wor' and ton't assign them tosks and tork, bui father toach them to lon  uor the endless immensit  of the eaac\n",
            "p waumwant to cuild anship, don't arum up people tog ther to collect wor' ant uon't assign them tosks and work, but father toach the  to cong uor the endless immensit  of the eoat\n",
            "p waumwant to cuild anship, don't arum up people to  ther to collect wor' ast won't assign the  tosks and work, but uather toach the  to cong uor the endless immensity of the eoac\n",
            "p waumwant to cuild anship, don't arum up people thg ther to collect wor' ant won't ansign them tosks and work, but father toach them to cong uor themendless immensity of themsoac\n",
            "p waumwant to cuild asthip, don't arum up people to  ther to collect wor' ant won't ansign the  tosks and work, but rather toach the  to cong uor the endless immensity af the soac\n",
            "p waumwant to build a ship, don't arum up people tog ther to lollect wor' and ton't ansign them tosks and work, but rather toach them to long uor the endless immensity of themeaac\n",
            "g roumwant to build a ship, don't arum up people tog ther to lollect wor' and ton't assign the  tosks and work, but father toach the  to long uor the endless immensity of the saac\n",
            "g roumwant to build a ship, don't arum up peoplestog ther to collect wor' and ton't assign them tosks and work, but father toach them to long uor the endless immensity of themeeac\n",
            "g roumwant to build a ship, don't arum up people to  ther to collect wor' and won't assign them tosks and work, but father toach them to long uor the endless immensity of the eoac\n",
            "g roumwant to build a ship, don't arum up people tog ther to collect wor' and won't assign them tosks and work, but father toach them to long uor the endless immensity of themeeac\n",
            "g dau want to build a ship, don't arum up people tog ther to collect wor' and won't assign them tosks and work, but father toach the  to long uor the endless immensity of therseac\n",
            "g dau want to build a ship, don't arum up people tog ther to collect wor' and won't assign them tosks and work, but fathem toach them to long uor themendless immensity of themeaac\n",
            "g dau want to tuild a ship, don't arum up people to  ther to collect wor' and won't assign the  tosks and work, but rather toach the  to bong uor thersndless i mensity of the seac\n",
            "g rou want to build a ship, don't arum up people tog ther to collect woo' and won't assign them tosks and work, but tather toach them to long uor the endless immensity of themsaac\n",
            "p fou want to build a ship, don't arum up people tog ther to collect uoo' and uon't assign them tosks and work, but tather toach the  to long uor the endless immensity of themeesc\n",
            "g p u want to build a ship, don't arum up people tog ther to bollect wor' and won't assign them tosks and work, but tather toach the  to long uor the endless immensity of the eeat\n",
            "g p g want to build a shipl don't arum up people thg ther to bollect wor' and won't assign the  tosks and work, but rather toach the  to long for the sndless immensity af the seat\n",
            "g p u want to build a ship, don't arum fp people tog ther to collect wor' and won't assign them tosks and work, but rather toach them to long for the endless immensity of themseac\n",
            "g pou want to cuild a ship, don't arum fp people tog ther to collect woo' and don't assign them tosks and dork, but rather toach them to long for themendless immensity of themseac\n",
            "p p u want to build a ship, don't arum fp people together te collect woo' and won't assign them tosks and work, but rather teach them to long for the endless immensity of themseac\n",
            "  pou want to build a ship, don't arum fp people together te bollect wor' and won't assign them tesks and work, but rather teach them te long for the endless immensity of themseac\n",
            "  pou want to build a ship, don't arum up people together te bollect word and won't assign them tosks and work, but rather teach them to long for the endless immensity of themeeac\n",
            "  rou want to build a ship, don't arum up people together to bollect wood and don't assign them tosks and work, but rather toach them to long uor the endless immensity of themseac\n",
            "g rou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and wook, but rather toach them to long uor themendless immensity of themseac\n",
            "g dou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and wook, but rather toach the  to long uor the endless immensity of the seac\n",
            "p dou want to build a ship, don't arum fp people together to collect wood and won't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
            "p dou want to build a ship, don't arum fp people together to collect word and won't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p dou want to build a ship, don't arum fp people together to collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "g rou want to build a ship, don't arum fp people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of themseat\n",
            "p rou want to build a ship, don't arum up people together to collect wood and don't assign the  tosks and work, but rather teach the  to long for the endless immensity of the seat\n",
            "g dou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach the  to long for the endless immensity of themseat\n",
            "g pou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of themseat\n",
            "g pou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of themseac\n",
            "g pou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of themseac\n",
            "  dou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "  rou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "  rou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "  rou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "  rou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "  pou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "  pou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "  pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seat\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p pou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p pou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p pou want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of themsoac\n",
            "p you want to build a ship, don't arum up people together te collect wood and don't assign the  tosks and work, but rather teach the  to long for the endless immensity of the eeat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKkwWQebenhQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}