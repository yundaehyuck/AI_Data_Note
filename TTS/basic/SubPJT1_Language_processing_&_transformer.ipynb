{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiWulKAdmAHX"
      },
      "source": [
        "# Language processing & Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "음성 AI를 위한 자연어 처리와 Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. 텍스트 전처리 과정 이해\n",
        "    - tokenizing\n",
        "    - cleaning\n",
        "2. Multi-head attention 및 self-attention 구현.\n",
        "3. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 install & import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wySpJEZWWU13",
        "outputId": "d17305b5-1e37-4fb8-c305-90984d99f780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDtMioSQQ1bB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import torch\n",
        "import math\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ug0S4e2RxNm",
        "outputId": "ddcfa537-179d-4bec-ba49-f0aaf4cbe40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6d540e8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH0VdC4uJJVG"
      },
      "source": [
        "## Req. 1-1 텍스트 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "주어진 문장 5개를 cleaning, tokenizing 한 뒤 정수 인코딩 하시오.  \n",
        "\n",
        "원하는 다른 tokenizer를 사용해도 좋습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "CnEIg_m6WU14"
      },
      "outputs": [],
      "source": [
        "sentences = [[\"안녕하세요 음성 AI 실!@습에 오신 것을 환영#$^&@$&$합니다.\"], [\"이네들은 7895435너무나 멀리 있습니다.\"], \n",
        "[\"계절이 지나가는 하늘에는가을로 가&^%@!$!^득 차 있습니다.\"], [\"아직 나의 청!@$!%춘이 다하지!@% 않은 까닭입니다.\"], [\"가슴 속에 하!@$나 둘 새겨지는 별을\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정규표현식을 사용하여 숫자, 특수문자 제거"
      ],
      "metadata": {
        "id": "0gN3Ne8XaBn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_texts = []\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "punctuation = ['.']\n",
        "\n",
        "for sentence in sentences:\n",
        "    \n",
        "    s = sentence[0]\n",
        "\n",
        "    #문장에서 특수문자나 숫자를 매칭함\n",
        "    preprocessed_text = re.sub('[#$^&@!%0-9]','',s)\n",
        "\n",
        "    #okt 토크나이저로 문장 토큰화, 구두점 제거\n",
        "    tokenize_words = [word for word in okt.morphs(preprocessed_text) if word not in punctuation]\n",
        "\n",
        "    preprocessed_texts.append(tokenize_words)"
      ],
      "metadata": {
        "id": "K6NyxBchaAQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sfg-83vyJ4L",
        "outputId": "4a9a69f8-4ce0-408e-b14e-e78fdb433a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['안녕하세요', '음성', 'AI', '실습', '에', '오신', '것', '을', '환영', '합니다'],\n",
              " ['이', '네', '들', '은', '너무나', '멀리', '있습니다'],\n",
              " ['계절', '이', '지나가는', '하늘', '에는', '가을로', '가득', '차', '있습니다'],\n",
              " ['아직', '나', '의', '청춘', '이', '다', '하지', '않은', '까닭', '입니다'],\n",
              " ['가슴', '속', '에', '하나', '둘', '새겨지는', '별', '을']]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(preprocessed_texts)"
      ],
      "metadata": {
        "id": "OfINhBWlxLvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2UAx3dQxbth",
        "outputId": "562b59ce-8fd6-4b1f-a222-2d0ed689253b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'이': 1,\n",
              " '에': 2,\n",
              " '을': 3,\n",
              " '있습니다': 4,\n",
              " '안녕하세요': 5,\n",
              " '음성': 6,\n",
              " 'ai': 7,\n",
              " '실습': 8,\n",
              " '오신': 9,\n",
              " '것': 10,\n",
              " '환영': 11,\n",
              " '합니다': 12,\n",
              " '네': 13,\n",
              " '들': 14,\n",
              " '은': 15,\n",
              " '너무나': 16,\n",
              " '멀리': 17,\n",
              " '계절': 18,\n",
              " '지나가는': 19,\n",
              " '하늘': 20,\n",
              " '에는': 21,\n",
              " '가을로': 22,\n",
              " '가득': 23,\n",
              " '차': 24,\n",
              " '아직': 25,\n",
              " '나': 26,\n",
              " '의': 27,\n",
              " '청춘': 28,\n",
              " '다': 29,\n",
              " '하지': 30,\n",
              " '않은': 31,\n",
              " '까닭': 32,\n",
              " '입니다': 33,\n",
              " '가슴': 34,\n",
              " '속': 35,\n",
              " '하나': 36,\n",
              " '둘': 37,\n",
              " '새겨지는': 38,\n",
              " '별': 39}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_sentences = tokenizer.texts_to_sequences(preprocessed_texts)"
      ],
      "metadata": {
        "id": "xYpFpf0xz868"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QqUT8jO2pR6",
        "outputId": "b46aab10-8cc0-49b8-baaa-2a082068fd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 6, 7, 8, 2, 9, 10, 3, 11, 12],\n",
              " [1, 13, 14, 15, 16, 17, 4],\n",
              " [18, 1, 19, 20, 21, 22, 23, 24, 4],\n",
              " [25, 26, 27, 28, 1, 29, 30, 31, 32, 33],\n",
              " [34, 35, 2, 36, 37, 38, 39, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQj3ibkcWU15"
      },
      "source": [
        "결과는 다음과 같이 나와야 합니다.  \n",
        "\n",
        "\n",
        "[[5, 6, 7, 2, 8, 9, 3, 10, 11],  \n",
        " [1, 12, 13, 14, 15, 16, 4],  \n",
        " [17, 1, 18, 19, 20, 21, 22, 23, 4],  \n",
        " [24, 25, 26, 27, 1, 28, 29, 30, 31, 32],  \n",
        " [33, 34, 2, 35, 36, 37, 38, 3]]  \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIyYvrZXWU15"
      },
      "source": [
        "## Req. 1-2 Multi-head self-attention 구조 익히기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgwQtMcRWU15"
      },
      "source": [
        "위에서 전처리한 데이터를 가져와 아래 과정을 실행하면서 시퀀스 입력이 multi-head self attention으로 어떻게 모델링 되는지 파악하시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9ULZIqTenSc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "pad_id = 0\n",
        "vocab_size = 40\n",
        "\n",
        "data = encoding_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hx3mcivgMyH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3e8FiNvgX60",
        "outputId": "a6bb83fe-b87c-4929-86d6-48d680f5d74e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 56375.05it/s]\n"
          ]
        }
      ],
      "source": [
        "data, max_len = padding(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwPSIWYugaN0",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc9a078-29a4-4695-c68b-3c495513533d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 6, 7, 8, 2, 9, 10, 3, 11, 12],\n",
              " [1, 13, 14, 15, 16, 17, 4, 0, 0, 0],\n",
              " [18, 1, 19, 20, 21, 22, 23, 24, 4, 0],\n",
              " [25, 26, 27, 28, 1, 29, 30, 31, 32, 33],\n",
              " [34, 35, 2, 36, 37, 38, 39, 3, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Ngp2nWimS8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw58V_yydMTV",
        "outputId": "4918ea15-4d43-4128-e964-e252cb385d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6d540e8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJMi2Xsni5uq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tLCUQwojcUb",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2a576d-268d-428f-b629-94fa03d005d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.6057e+00,  5.1626e-01,  8.7614e-01,  ..., -1.4336e+00,\n",
            "          -3.4603e-01, -9.0669e-01],\n",
            "         [ 1.9242e+00,  1.2260e-01, -8.2242e-01,  ..., -3.4325e-01,\n",
            "          -1.5216e+00,  8.2277e-02],\n",
            "         [-9.7788e-01, -9.2131e-02, -4.7637e-01,  ...,  1.6527e+00,\n",
            "          -1.3805e+00, -1.5641e+00],\n",
            "         ...,\n",
            "         [ 2.2973e-01, -9.2456e-01, -5.6650e-01,  ...,  2.5386e+00,\n",
            "           1.0674e+00, -7.1239e-01],\n",
            "         [-1.7765e+00,  8.1844e-01,  9.6409e-01,  ..., -2.5248e-01,\n",
            "          -1.4807e-03, -9.0503e-01],\n",
            "         [-6.4328e-01, -7.1078e-01, -1.5838e-01,  ..., -6.2596e-01,\n",
            "           1.1199e+00, -1.0839e+00]],\n",
            "\n",
            "        [[ 1.2055e+00, -1.3480e+00, -9.8994e-02,  ..., -9.5853e-01,\n",
            "          -2.6443e+00,  2.4948e-01],\n",
            "         [ 5.3607e-02,  4.4246e-01,  8.0602e-01,  ..., -9.7368e-01,\n",
            "           2.3132e-02, -3.7473e-01],\n",
            "         [ 1.2890e+00,  8.4212e-01,  5.1969e-01,  ...,  1.5287e-01,\n",
            "           1.2314e+00, -9.0307e-01],\n",
            "         ...,\n",
            "         [ 1.8423e+00,  5.1889e-01, -1.7119e+00,  ...,  1.4346e+00,\n",
            "          -1.1415e+00, -8.3975e-02],\n",
            "         [ 1.8423e+00,  5.1889e-01, -1.7119e+00,  ...,  1.4346e+00,\n",
            "          -1.1415e+00, -8.3975e-02],\n",
            "         [ 1.8423e+00,  5.1889e-01, -1.7119e+00,  ...,  1.4346e+00,\n",
            "          -1.1415e+00, -8.3975e-02]],\n",
            "\n",
            "        [[ 8.7205e-01,  1.3252e+00, -5.9828e-01,  ...,  6.8929e-02,\n",
            "           7.3974e-01, -9.0905e-01],\n",
            "         [ 1.2055e+00, -1.3480e+00, -9.8994e-02,  ..., -9.5853e-01,\n",
            "          -2.6443e+00,  2.4948e-01],\n",
            "         [ 2.7739e-01,  2.6229e-01, -2.9604e-01,  ...,  1.9258e-01,\n",
            "           1.0183e+00,  4.2014e-01],\n",
            "         ...,\n",
            "         [-6.3828e-01, -1.5599e+00, -1.1012e-01,  ..., -1.0678e+00,\n",
            "           1.8419e+00, -4.2380e-01],\n",
            "         [ 8.6587e-01,  1.3186e+00, -5.2400e-01,  ...,  3.9089e-01,\n",
            "          -6.2442e-02, -2.6471e-01],\n",
            "         [ 1.8423e+00,  5.1889e-01, -1.7119e+00,  ...,  1.4346e+00,\n",
            "          -1.1415e+00, -8.3975e-02]],\n",
            "\n",
            "        [[ 8.8970e-01,  6.8769e-01, -4.3491e-01,  ...,  1.3865e+00,\n",
            "           2.1220e+00,  1.6175e+00],\n",
            "         [-1.2858e+00, -1.6481e+00,  3.5800e-01,  ...,  7.2144e-01,\n",
            "          -1.1933e+00, -2.1480e-01],\n",
            "         [ 1.0788e+00, -1.6056e+00, -5.4637e-02,  ...,  1.8420e-01,\n",
            "           8.8131e-02, -2.6583e+00],\n",
            "         ...,\n",
            "         [ 7.7438e-02, -1.1419e+00,  2.1635e+00,  ...,  5.4050e-01,\n",
            "          -8.2140e-01, -1.0530e+00],\n",
            "         [ 4.8655e-02, -2.9400e-01,  9.2160e-01,  ..., -1.9966e+00,\n",
            "           4.9997e-01,  1.2725e-01],\n",
            "         [-2.8605e-01,  6.5462e-01, -8.1278e-01,  ...,  5.2648e-01,\n",
            "           4.4832e-01, -6.7463e-01]],\n",
            "\n",
            "        [[-7.0592e-01, -1.1781e+00,  8.1103e-01,  ...,  3.8833e-01,\n",
            "           1.3528e+00,  1.1365e+00],\n",
            "         [-2.5587e-01, -7.0224e-01, -8.9652e-01,  ...,  2.3895e-01,\n",
            "           2.6874e-01, -1.9755e-01],\n",
            "         [ 3.7389e-01, -2.2328e-01, -1.1291e+00,  ...,  5.4005e-01,\n",
            "           6.6392e-02,  3.4402e-01],\n",
            "         ...,\n",
            "         [ 2.2973e-01, -9.2456e-01, -5.6650e-01,  ...,  2.5386e+00,\n",
            "           1.0674e+00, -7.1239e-01],\n",
            "         [ 1.8423e+00,  5.1889e-01, -1.7119e+00,  ...,  1.4346e+00,\n",
            "          -1.1415e+00, -8.3975e-02],\n",
            "         [ 1.8423e+00,  5.1889e-01, -1.7119e+00,  ...,  1.4346e+00,\n",
            "          -1.1415e+00, -8.3975e-02]]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([5, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DWKDqgCgfMk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcLuhda7m-Lm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGWyFs7SdpF-",
        "outputId": "de3424d2-3b0d-422e-b7a5-486cef9e3bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6d540e8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-vSL7PwnV6k",
        "outputId": "790ad346-cdb5-4aa4-97da-3bd7109f1e50",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 10, 512])\n",
            "torch.Size([5, 10, 512])\n",
            "torch.Size([5, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tiOKAv9nEli",
        "outputId": "6163e834-2eee-4a0c-e545-ba573aa56185",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 10, 8, 64])\n",
            "torch.Size([5, 10, 8, 64])\n",
            "torch.Size([5, 10, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNb2isfn5Cx",
        "outputId": "d68f5720-0d7c-4a9e-8512-99dba559f922",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 8, 10, 64])\n",
            "torch.Size([5, 8, 10, 64])\n",
            "torch.Size([5, 8, 10, 64])\n"
          ]
        }
      ],
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8l2-NRcdsXr",
        "outputId": "f954abb0-d131-4b09-85c4-92ef2d908697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6d540e8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4b256b-db47-48bb-85ef-ed9ae0a4e10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0550, 0.0868, 0.0645,  ..., 0.0749, 0.1359, 0.1409],\n",
            "          [0.0809, 0.0409, 0.0938,  ..., 0.1158, 0.0655, 0.1383],\n",
            "          [0.1304, 0.0951, 0.1111,  ..., 0.0791, 0.1291, 0.1318],\n",
            "          ...,\n",
            "          [0.0800, 0.0730, 0.1012,  ..., 0.1482, 0.0904, 0.0911],\n",
            "          [0.0676, 0.0691, 0.0932,  ..., 0.1479, 0.1008, 0.1347],\n",
            "          [0.0861, 0.0869, 0.1291,  ..., 0.0701, 0.1701, 0.1060]],\n",
            "\n",
            "         [[0.0919, 0.1111, 0.1576,  ..., 0.0720, 0.1390, 0.0932],\n",
            "          [0.1772, 0.0910, 0.1112,  ..., 0.1000, 0.0853, 0.1614],\n",
            "          [0.1266, 0.0877, 0.0814,  ..., 0.1188, 0.0829, 0.0969],\n",
            "          ...,\n",
            "          [0.1040, 0.0949, 0.1150,  ..., 0.1032, 0.1228, 0.1003],\n",
            "          [0.1372, 0.0652, 0.0749,  ..., 0.1390, 0.1136, 0.0868],\n",
            "          [0.1504, 0.0721, 0.0714,  ..., 0.0510, 0.0655, 0.0594]],\n",
            "\n",
            "         [[0.1033, 0.1156, 0.0730,  ..., 0.1512, 0.1451, 0.0434],\n",
            "          [0.1258, 0.1393, 0.0647,  ..., 0.0929, 0.0809, 0.0704],\n",
            "          [0.1134, 0.1157, 0.0913,  ..., 0.1168, 0.1275, 0.0684],\n",
            "          ...,\n",
            "          [0.0601, 0.1538, 0.1139,  ..., 0.1089, 0.1044, 0.0405],\n",
            "          [0.1541, 0.1515, 0.1047,  ..., 0.0703, 0.1282, 0.0786],\n",
            "          [0.0697, 0.0862, 0.0598,  ..., 0.0884, 0.0922, 0.1027]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0773, 0.0895, 0.0704,  ..., 0.0982, 0.0940, 0.1838],\n",
            "          [0.1073, 0.1227, 0.1198,  ..., 0.1044, 0.1001, 0.0991],\n",
            "          [0.1492, 0.0897, 0.0885,  ..., 0.0821, 0.1414, 0.0718],\n",
            "          ...,\n",
            "          [0.0670, 0.0770, 0.0892,  ..., 0.1175, 0.0616, 0.1781],\n",
            "          [0.1052, 0.1151, 0.1042,  ..., 0.0706, 0.1129, 0.1090],\n",
            "          [0.0962, 0.1274, 0.1080,  ..., 0.1283, 0.1105, 0.0784]],\n",
            "\n",
            "         [[0.0799, 0.0570, 0.0720,  ..., 0.0924, 0.1066, 0.1603],\n",
            "          [0.0835, 0.0759, 0.0870,  ..., 0.0819, 0.0473, 0.1500],\n",
            "          [0.1073, 0.0662, 0.0668,  ..., 0.0940, 0.0636, 0.1010],\n",
            "          ...,\n",
            "          [0.0778, 0.0638, 0.0631,  ..., 0.2021, 0.0566, 0.0577],\n",
            "          [0.1590, 0.0906, 0.1189,  ..., 0.0615, 0.0995, 0.1648],\n",
            "          [0.1389, 0.0670, 0.0802,  ..., 0.1220, 0.0657, 0.0681]],\n",
            "\n",
            "         [[0.0778, 0.1423, 0.0533,  ..., 0.1221, 0.0682, 0.0859],\n",
            "          [0.1060, 0.1188, 0.0819,  ..., 0.0962, 0.1584, 0.0512],\n",
            "          [0.1003, 0.0920, 0.0610,  ..., 0.1158, 0.0825, 0.1175],\n",
            "          ...,\n",
            "          [0.0641, 0.0689, 0.2021,  ..., 0.1303, 0.1133, 0.0797],\n",
            "          [0.1041, 0.1052, 0.1348,  ..., 0.0882, 0.0686, 0.0890],\n",
            "          [0.1138, 0.1248, 0.0606,  ..., 0.0786, 0.0971, 0.1329]]],\n",
            "\n",
            "\n",
            "        [[[0.1335, 0.0971, 0.0880,  ..., 0.0856, 0.0856, 0.0856],\n",
            "          [0.0956, 0.1164, 0.1066,  ..., 0.0981, 0.0981, 0.0981],\n",
            "          [0.0587, 0.1110, 0.1410,  ..., 0.1211, 0.1211, 0.1211],\n",
            "          ...,\n",
            "          [0.0672, 0.0973, 0.1358,  ..., 0.1162, 0.1162, 0.1162],\n",
            "          [0.0672, 0.0973, 0.1358,  ..., 0.1162, 0.1162, 0.1162],\n",
            "          [0.0672, 0.0973, 0.1358,  ..., 0.1162, 0.1162, 0.1162]],\n",
            "\n",
            "         [[0.1735, 0.0787, 0.1060,  ..., 0.0747, 0.0747, 0.0747],\n",
            "          [0.0984, 0.0792, 0.1242,  ..., 0.1218, 0.1218, 0.1218],\n",
            "          [0.1390, 0.1556, 0.1120,  ..., 0.0733, 0.0733, 0.0733],\n",
            "          ...,\n",
            "          [0.1366, 0.1036, 0.0928,  ..., 0.0905, 0.0905, 0.0905],\n",
            "          [0.1366, 0.1036, 0.0928,  ..., 0.0905, 0.0905, 0.0905],\n",
            "          [0.1366, 0.1036, 0.0928,  ..., 0.0905, 0.0905, 0.0905]],\n",
            "\n",
            "         [[0.1329, 0.0639, 0.0967,  ..., 0.0833, 0.0833, 0.0833],\n",
            "          [0.0571, 0.0937, 0.1698,  ..., 0.1150, 0.1150, 0.1150],\n",
            "          [0.0560, 0.1093, 0.1033,  ..., 0.0907, 0.0907, 0.0907],\n",
            "          ...,\n",
            "          [0.1068, 0.0877, 0.1165,  ..., 0.0957, 0.0957, 0.0957],\n",
            "          [0.1068, 0.0877, 0.1165,  ..., 0.0957, 0.0957, 0.0957],\n",
            "          [0.1068, 0.0877, 0.1165,  ..., 0.0957, 0.0957, 0.0957]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1774, 0.0911, 0.0864,  ..., 0.1208, 0.1208, 0.1208],\n",
            "          [0.1195, 0.0790, 0.0964,  ..., 0.1079, 0.1079, 0.1079],\n",
            "          [0.0586, 0.0891, 0.1097,  ..., 0.0859, 0.0859, 0.0859],\n",
            "          ...,\n",
            "          [0.0814, 0.1302, 0.0736,  ..., 0.0953, 0.0953, 0.0953],\n",
            "          [0.0814, 0.1302, 0.0736,  ..., 0.0953, 0.0953, 0.0953],\n",
            "          [0.0814, 0.1302, 0.0736,  ..., 0.0953, 0.0953, 0.0953]],\n",
            "\n",
            "         [[0.0531, 0.0677, 0.0607,  ..., 0.1467, 0.1467, 0.1467],\n",
            "          [0.0960, 0.1178, 0.1184,  ..., 0.1002, 0.1002, 0.1002],\n",
            "          [0.1511, 0.1002, 0.0581,  ..., 0.0794, 0.0794, 0.0794],\n",
            "          ...,\n",
            "          [0.1119, 0.1122, 0.1825,  ..., 0.0657, 0.0657, 0.0657],\n",
            "          [0.1119, 0.1122, 0.1825,  ..., 0.0657, 0.0657, 0.0657],\n",
            "          [0.1119, 0.1122, 0.1825,  ..., 0.0657, 0.0657, 0.0657]],\n",
            "\n",
            "         [[0.0953, 0.0869, 0.0847,  ..., 0.0908, 0.0908, 0.0908],\n",
            "          [0.1172, 0.0752, 0.0943,  ..., 0.1214, 0.1214, 0.1214],\n",
            "          [0.0748, 0.0643, 0.0912,  ..., 0.0991, 0.0991, 0.0991],\n",
            "          ...,\n",
            "          [0.1124, 0.0849, 0.0897,  ..., 0.0708, 0.0708, 0.0708],\n",
            "          [0.1124, 0.0849, 0.0897,  ..., 0.0708, 0.0708, 0.0708],\n",
            "          [0.1124, 0.0849, 0.0897,  ..., 0.0708, 0.0708, 0.0708]]],\n",
            "\n",
            "\n",
            "        [[[0.1285, 0.1026, 0.0890,  ..., 0.0819, 0.0890, 0.1030],\n",
            "          [0.1032, 0.1267, 0.0905,  ..., 0.1043, 0.0863, 0.0812],\n",
            "          [0.0731, 0.0703, 0.0795,  ..., 0.0708, 0.1153, 0.0686],\n",
            "          ...,\n",
            "          [0.0886, 0.1175, 0.0951,  ..., 0.0858, 0.0749, 0.1208],\n",
            "          [0.0526, 0.1147, 0.0588,  ..., 0.0667, 0.0592, 0.1290],\n",
            "          [0.1060, 0.0727, 0.1529,  ..., 0.0726, 0.1117, 0.1256]],\n",
            "\n",
            "         [[0.0884, 0.0768, 0.1271,  ..., 0.0947, 0.1865, 0.0658],\n",
            "          [0.1861, 0.1394, 0.0512,  ..., 0.0771, 0.1374, 0.0600],\n",
            "          [0.0859, 0.1112, 0.0927,  ..., 0.0830, 0.0566, 0.0487],\n",
            "          ...,\n",
            "          [0.0989, 0.1172, 0.0513,  ..., 0.0616, 0.0956, 0.0769],\n",
            "          [0.2241, 0.0643, 0.1210,  ..., 0.0904, 0.1340, 0.0840],\n",
            "          [0.1101, 0.1219, 0.0652,  ..., 0.0932, 0.0939, 0.0807]],\n",
            "\n",
            "         [[0.1540, 0.0813, 0.0698,  ..., 0.1212, 0.0984, 0.0617],\n",
            "          [0.1244, 0.1226, 0.0584,  ..., 0.0903, 0.1313, 0.0768],\n",
            "          [0.0711, 0.0900, 0.0820,  ..., 0.1298, 0.0771, 0.0729],\n",
            "          ...,\n",
            "          [0.0761, 0.0986, 0.0651,  ..., 0.0793, 0.0667, 0.1097],\n",
            "          [0.1158, 0.1469, 0.1031,  ..., 0.0607, 0.0594, 0.0743],\n",
            "          [0.1700, 0.1241, 0.0851,  ..., 0.0836, 0.0860, 0.1112]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1015, 0.0772, 0.1230,  ..., 0.0775, 0.1330, 0.1911],\n",
            "          [0.0556, 0.1965, 0.0972,  ..., 0.0621, 0.0816, 0.1338],\n",
            "          [0.0596, 0.0461, 0.1198,  ..., 0.1337, 0.1149, 0.0639],\n",
            "          ...,\n",
            "          [0.0906, 0.0927, 0.0731,  ..., 0.1220, 0.1024, 0.0551],\n",
            "          [0.0942, 0.1519, 0.1163,  ..., 0.0964, 0.0438, 0.0717],\n",
            "          [0.1016, 0.0830, 0.0910,  ..., 0.0763, 0.0771, 0.0971]],\n",
            "\n",
            "         [[0.0735, 0.0689, 0.1186,  ..., 0.1154, 0.1152, 0.1449],\n",
            "          [0.0751, 0.0546, 0.1093,  ..., 0.1194, 0.0895, 0.1509],\n",
            "          [0.0773, 0.0875, 0.0922,  ..., 0.1034, 0.1692, 0.0959],\n",
            "          ...,\n",
            "          [0.0836, 0.1168, 0.1282,  ..., 0.0854, 0.1071, 0.1140],\n",
            "          [0.1073, 0.1167, 0.0978,  ..., 0.1100, 0.1396, 0.0801],\n",
            "          [0.0871, 0.1055, 0.1171,  ..., 0.1577, 0.0564, 0.0619]],\n",
            "\n",
            "         [[0.0629, 0.1426, 0.1111,  ..., 0.0832, 0.0710, 0.0814],\n",
            "          [0.1055, 0.1061, 0.0693,  ..., 0.1106, 0.1093, 0.1011],\n",
            "          [0.1074, 0.0867, 0.0857,  ..., 0.0835, 0.0981, 0.1467],\n",
            "          ...,\n",
            "          [0.0725, 0.1032, 0.1305,  ..., 0.0939, 0.1035, 0.1000],\n",
            "          [0.1493, 0.1043, 0.0967,  ..., 0.0783, 0.0928, 0.0989],\n",
            "          [0.0799, 0.1170, 0.1260,  ..., 0.0890, 0.2034, 0.0737]]],\n",
            "\n",
            "\n",
            "        [[[0.1091, 0.0829, 0.0937,  ..., 0.0785, 0.1256, 0.0796],\n",
            "          [0.1504, 0.0973, 0.0685,  ..., 0.0640, 0.0447, 0.1233],\n",
            "          [0.0838, 0.1107, 0.0761,  ..., 0.1247, 0.0814, 0.0986],\n",
            "          ...,\n",
            "          [0.0949, 0.0706, 0.0709,  ..., 0.1262, 0.0669, 0.0805],\n",
            "          [0.0988, 0.0720, 0.2169,  ..., 0.1282, 0.1029, 0.0910],\n",
            "          [0.0948, 0.1030, 0.1052,  ..., 0.0826, 0.0796, 0.1795]],\n",
            "\n",
            "         [[0.1033, 0.0703, 0.0539,  ..., 0.0659, 0.1061, 0.1170],\n",
            "          [0.0917, 0.1235, 0.1015,  ..., 0.1426, 0.0798, 0.1421],\n",
            "          [0.0639, 0.1064, 0.0827,  ..., 0.1076, 0.0762, 0.1907],\n",
            "          ...,\n",
            "          [0.0633, 0.1124, 0.1905,  ..., 0.0858, 0.1263, 0.0924],\n",
            "          [0.1387, 0.0930, 0.1334,  ..., 0.0921, 0.0681, 0.0610],\n",
            "          [0.0740, 0.1086, 0.0814,  ..., 0.1226, 0.1291, 0.0818]],\n",
            "\n",
            "         [[0.1061, 0.0778, 0.0729,  ..., 0.0568, 0.1513, 0.1121],\n",
            "          [0.0614, 0.0969, 0.0866,  ..., 0.1507, 0.1072, 0.1360],\n",
            "          [0.1073, 0.1064, 0.0911,  ..., 0.0808, 0.1092, 0.0675],\n",
            "          ...,\n",
            "          [0.0850, 0.0868, 0.0930,  ..., 0.1049, 0.0975, 0.1126],\n",
            "          [0.1551, 0.0787, 0.1273,  ..., 0.0879, 0.0799, 0.0552],\n",
            "          [0.1182, 0.0731, 0.1143,  ..., 0.0720, 0.1011, 0.1300]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1100, 0.0922, 0.0640,  ..., 0.0827, 0.1146, 0.0893],\n",
            "          [0.0951, 0.1471, 0.1289,  ..., 0.0775, 0.0954, 0.0618],\n",
            "          [0.0959, 0.0593, 0.0734,  ..., 0.0996, 0.1499, 0.0805],\n",
            "          ...,\n",
            "          [0.1506, 0.0781, 0.0869,  ..., 0.1167, 0.0897, 0.1122],\n",
            "          [0.1140, 0.1096, 0.0681,  ..., 0.1004, 0.0941, 0.1363],\n",
            "          [0.1623, 0.0918, 0.0639,  ..., 0.0761, 0.0824, 0.1813]],\n",
            "\n",
            "         [[0.1103, 0.0836, 0.0837,  ..., 0.1875, 0.0907, 0.0728],\n",
            "          [0.0781, 0.1102, 0.0640,  ..., 0.0969, 0.0985, 0.1169],\n",
            "          [0.0834, 0.1409, 0.0727,  ..., 0.0746, 0.1026, 0.0790],\n",
            "          ...,\n",
            "          [0.1086, 0.0586, 0.1735,  ..., 0.0722, 0.0675, 0.0904],\n",
            "          [0.0608, 0.1363, 0.0696,  ..., 0.1324, 0.0561, 0.1313],\n",
            "          [0.1602, 0.0789, 0.0518,  ..., 0.1402, 0.0528, 0.1120]],\n",
            "\n",
            "         [[0.0974, 0.0607, 0.1004,  ..., 0.1181, 0.1713, 0.0728],\n",
            "          [0.0433, 0.1117, 0.0609,  ..., 0.0999, 0.0701, 0.0830],\n",
            "          [0.0580, 0.1162, 0.0579,  ..., 0.0495, 0.1734, 0.1099],\n",
            "          ...,\n",
            "          [0.0595, 0.0815, 0.1414,  ..., 0.0464, 0.1121, 0.0844],\n",
            "          [0.1276, 0.0911, 0.1225,  ..., 0.0853, 0.0698, 0.0679],\n",
            "          [0.0530, 0.0647, 0.1223,  ..., 0.0699, 0.1618, 0.1253]]],\n",
            "\n",
            "\n",
            "        [[[0.0562, 0.1518, 0.0808,  ..., 0.1816, 0.0841, 0.0841],\n",
            "          [0.0811, 0.1267, 0.1344,  ..., 0.1508, 0.0521, 0.0521],\n",
            "          [0.0862, 0.1006, 0.0926,  ..., 0.0843, 0.0616, 0.0616],\n",
            "          ...,\n",
            "          [0.0971, 0.1476, 0.0847,  ..., 0.1511, 0.0664, 0.0664],\n",
            "          [0.1074, 0.0605, 0.0812,  ..., 0.1287, 0.1258, 0.1258],\n",
            "          [0.1074, 0.0605, 0.0812,  ..., 0.1287, 0.1258, 0.1258]],\n",
            "\n",
            "         [[0.0915, 0.0931, 0.1018,  ..., 0.1227, 0.0704, 0.0704],\n",
            "          [0.1561, 0.0795, 0.0639,  ..., 0.0777, 0.1317, 0.1317],\n",
            "          [0.1327, 0.1356, 0.0640,  ..., 0.1286, 0.0570, 0.0570],\n",
            "          ...,\n",
            "          [0.1049, 0.1069, 0.0808,  ..., 0.1066, 0.0923, 0.0923],\n",
            "          [0.0742, 0.1014, 0.0633,  ..., 0.1118, 0.1002, 0.1002],\n",
            "          [0.0742, 0.1014, 0.0633,  ..., 0.1118, 0.1002, 0.1002]],\n",
            "\n",
            "         [[0.1281, 0.0859, 0.1106,  ..., 0.0966, 0.1105, 0.1105],\n",
            "          [0.1402, 0.0755, 0.0884,  ..., 0.1122, 0.1207, 0.1207],\n",
            "          [0.1022, 0.1004, 0.1408,  ..., 0.1488, 0.0700, 0.0700],\n",
            "          ...,\n",
            "          [0.0825, 0.1104, 0.0920,  ..., 0.1081, 0.0845, 0.0845],\n",
            "          [0.1185, 0.1069, 0.1200,  ..., 0.0919, 0.1137, 0.1137],\n",
            "          [0.1185, 0.1069, 0.1200,  ..., 0.0919, 0.1137, 0.1137]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0908, 0.1083, 0.0910,  ..., 0.0722, 0.0507, 0.0507],\n",
            "          [0.1158, 0.1028, 0.1336,  ..., 0.0699, 0.0655, 0.0655],\n",
            "          [0.1142, 0.0842, 0.0993,  ..., 0.1733, 0.0753, 0.0753],\n",
            "          ...,\n",
            "          [0.1363, 0.1236, 0.1063,  ..., 0.1344, 0.0778, 0.0778],\n",
            "          [0.1129, 0.0936, 0.0674,  ..., 0.0538, 0.0993, 0.0993],\n",
            "          [0.1129, 0.0936, 0.0674,  ..., 0.0538, 0.0993, 0.0993]],\n",
            "\n",
            "         [[0.1147, 0.1474, 0.0690,  ..., 0.0998, 0.1018, 0.1018],\n",
            "          [0.1178, 0.1156, 0.0481,  ..., 0.1219, 0.0640, 0.0640],\n",
            "          [0.1027, 0.0997, 0.1118,  ..., 0.1392, 0.0739, 0.0739],\n",
            "          ...,\n",
            "          [0.0868, 0.1191, 0.1562,  ..., 0.1789, 0.0650, 0.0650],\n",
            "          [0.1075, 0.0761, 0.1489,  ..., 0.0921, 0.0708, 0.0708],\n",
            "          [0.1075, 0.0761, 0.1489,  ..., 0.0921, 0.0708, 0.0708]],\n",
            "\n",
            "         [[0.0864, 0.0665, 0.0694,  ..., 0.0943, 0.1226, 0.1226],\n",
            "          [0.0561, 0.0722, 0.1124,  ..., 0.0544, 0.1798, 0.1798],\n",
            "          [0.0461, 0.1190, 0.1145,  ..., 0.0974, 0.1576, 0.1576],\n",
            "          ...,\n",
            "          [0.0938, 0.1047, 0.0759,  ..., 0.1108, 0.1541, 0.1541],\n",
            "          [0.1085, 0.0596, 0.1096,  ..., 0.1360, 0.0924, 0.0924],\n",
            "          [0.1085, 0.0596, 0.1096,  ..., 0.1360, 0.0924, 0.0924]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([5, 8, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "id": "TS22PMVcduhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7megouWpgCck",
        "outputId": "ec1a3f38-b7db-4087-eb8d-04bd1c21a78e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 8, 10, 64])\n"
          ]
        }
      ],
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaK0bpMGhQZ2",
        "outputId": "c4efd8b6-b854-4d51-c5ba-98289c3299d6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1DRnlVUdd3N",
        "outputId": "c091a0de-2456-477c-9989-0c53919b4beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6d540e8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTng_2SXhdH1",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de9e67c-cee5-469e-925d-ddf3d2b8dde8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.4145e-02,  9.0591e-02, -3.0949e-02,  ...,  1.9440e-01,\n",
            "          -1.5152e-02, -1.9763e-01],\n",
            "         [ 2.7790e-02,  1.3647e-01, -8.7589e-02,  ...,  1.5087e-01,\n",
            "          -5.9431e-02, -2.0513e-01],\n",
            "         [ 1.5193e-03,  5.1733e-02, -3.4423e-02,  ...,  1.6774e-01,\n",
            "          -8.2296e-02, -2.1104e-01],\n",
            "         ...,\n",
            "         [-6.9484e-05,  7.8142e-02, -4.7687e-02,  ...,  1.6356e-01,\n",
            "          -5.4219e-02, -1.8753e-01],\n",
            "         [-9.4693e-03,  7.9399e-02, -2.6237e-02,  ...,  1.3948e-01,\n",
            "           1.9279e-02, -1.9136e-01],\n",
            "         [ 2.6785e-02,  8.8562e-02, -2.0943e-02,  ...,  1.5453e-01,\n",
            "          -8.5891e-02, -1.8358e-01]],\n",
            "\n",
            "        [[ 1.1532e-01, -1.3643e-01, -1.5151e-01,  ...,  9.1063e-02,\n",
            "           1.0467e-01,  5.7146e-03],\n",
            "         [ 1.4090e-01, -1.0668e-01, -1.5049e-01,  ...,  1.2853e-01,\n",
            "           1.2191e-01, -2.5358e-02],\n",
            "         [ 1.3648e-01, -1.0209e-01, -7.6098e-02,  ...,  1.4596e-01,\n",
            "           1.0709e-01, -5.4055e-02],\n",
            "         ...,\n",
            "         [ 1.7824e-01, -1.1623e-01, -1.0544e-01,  ...,  1.7254e-01,\n",
            "           8.9939e-02, -3.1829e-02],\n",
            "         [ 1.7824e-01, -1.1623e-01, -1.0544e-01,  ...,  1.7254e-01,\n",
            "           8.9939e-02, -3.1829e-02],\n",
            "         [ 1.7824e-01, -1.1623e-01, -1.0544e-01,  ...,  1.7254e-01,\n",
            "           8.9939e-02, -3.1829e-02]],\n",
            "\n",
            "        [[ 2.2704e-02, -2.3585e-02,  9.8027e-02,  ...,  9.4162e-02,\n",
            "          -9.2977e-02, -6.4967e-02],\n",
            "         [ 1.4589e-02, -5.4421e-02,  8.0661e-02,  ...,  9.5594e-02,\n",
            "          -8.3012e-02, -1.1249e-01],\n",
            "         [ 2.4589e-02, -4.7193e-02,  1.5945e-01,  ...,  5.8175e-02,\n",
            "          -9.0574e-02, -7.7665e-02],\n",
            "         ...,\n",
            "         [ 2.9219e-02, -6.4263e-02,  1.7871e-01,  ...,  6.1235e-02,\n",
            "          -8.9987e-02, -2.4725e-02],\n",
            "         [ 2.9906e-02, -4.8688e-02,  1.8310e-01,  ...,  1.3380e-01,\n",
            "          -8.1150e-02, -8.5168e-02],\n",
            "         [ 2.6467e-02,  2.9672e-02,  1.1748e-01,  ...,  9.4146e-02,\n",
            "          -1.1137e-01, -1.5043e-01]],\n",
            "\n",
            "        [[-5.3385e-02,  3.3618e-02,  1.0500e-01,  ...,  1.0713e-01,\n",
            "          -1.4621e-01, -2.5270e-03],\n",
            "         [ 1.4758e-02,  7.8321e-02,  4.8046e-02,  ...,  7.7190e-02,\n",
            "          -8.7574e-02, -1.9435e-02],\n",
            "         [-6.7567e-02,  6.3996e-02,  8.3984e-02,  ...,  1.0576e-01,\n",
            "          -7.4006e-02, -1.1976e-03],\n",
            "         ...,\n",
            "         [-6.8670e-02,  2.6516e-02,  1.1802e-01,  ...,  5.9968e-02,\n",
            "          -1.0070e-01,  2.3672e-02],\n",
            "         [-6.2083e-03, -3.8906e-02,  9.3702e-02,  ...,  1.0666e-01,\n",
            "          -6.9081e-02, -7.3236e-03],\n",
            "         [ 1.9936e-02,  1.3548e-02,  5.8231e-02,  ...,  1.4633e-01,\n",
            "          -1.0540e-01, -7.2883e-03]],\n",
            "\n",
            "        [[-2.8832e-02,  5.2164e-02, -1.4727e-02,  ...,  7.7913e-02,\n",
            "           3.1945e-02, -1.3983e-01],\n",
            "         [-8.0300e-02,  1.0684e-01,  4.0451e-02,  ...,  8.3559e-02,\n",
            "           1.6365e-03, -1.0131e-01],\n",
            "         [-4.3448e-02,  2.5811e-02,  4.0339e-02,  ...,  5.7302e-02,\n",
            "           6.6311e-02, -1.3293e-01],\n",
            "         ...,\n",
            "         [-4.0645e-02,  5.9016e-02,  4.7167e-02,  ...,  4.4435e-02,\n",
            "           5.3252e-02, -8.1184e-02],\n",
            "         [-1.1656e-02,  1.9366e-02, -3.9333e-03,  ...,  1.2978e-01,\n",
            "           6.9616e-02, -1.5522e-01],\n",
            "         [-1.1656e-02,  1.9366e-02, -3.9333e-03,  ...,  1.2978e-01,\n",
            "           6.9616e-02, -1.5522e-01]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([5, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 1-3 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_kNhOTrkBHm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "    d_k = d_model // num_heads\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    q = w_q(q)  # (B, L, d_model)\n",
        "    k = w_k(k)  # (B, L, d_model)\n",
        "    v = w_v(v)  # (B, L, d_model)\n",
        "\n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # num_heads * d_k로 쪼갠다\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un6zIRi2dZca",
        "outputId": "d8c3930f-01ea-45b6-b5c9-fb941f97eb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6d540e8f10>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYLuu_9alQxT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cd7036-a57e-41ec-e15b-ee8e027357e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.5881e-02,  4.7264e-02,  7.3918e-02,  ..., -1.4889e-01,\n",
            "          -7.8450e-02, -8.2003e-02],\n",
            "         [ 7.7830e-02,  5.0064e-02,  1.5045e-01,  ..., -6.5740e-02,\n",
            "          -1.0189e-01, -7.6289e-03],\n",
            "         [ 6.7338e-02,  9.0291e-02,  1.0462e-01,  ..., -1.1704e-01,\n",
            "          -5.6486e-02, -2.3669e-02],\n",
            "         ...,\n",
            "         [ 4.9953e-02,  1.2863e-02,  9.6843e-02,  ..., -1.0637e-01,\n",
            "          -1.0702e-01, -7.6089e-02],\n",
            "         [ 4.7319e-02,  5.2183e-02,  8.2475e-02,  ..., -7.8222e-02,\n",
            "          -1.1873e-01, -3.5306e-02],\n",
            "         [ 5.6760e-02,  3.6103e-02,  1.0054e-01,  ..., -7.7613e-02,\n",
            "          -5.8231e-02, -6.0732e-02]],\n",
            "\n",
            "        [[ 2.3406e-01, -9.6137e-02, -6.8675e-02,  ..., -7.3635e-02,\n",
            "          -8.1034e-02,  5.7708e-02],\n",
            "         [ 1.5537e-01, -3.6611e-02, -6.2355e-02,  ..., -8.5579e-03,\n",
            "          -4.5835e-02,  2.2010e-02],\n",
            "         [ 6.6599e-02, -6.3743e-02,  1.8539e-02,  ..., -3.4993e-02,\n",
            "          -1.1480e-01, -1.7960e-02],\n",
            "         ...,\n",
            "         [ 9.2126e-02, -5.5362e-02, -2.5193e-02,  ...,  3.6412e-03,\n",
            "          -9.1324e-02,  6.5189e-02],\n",
            "         [ 9.2126e-02, -5.5362e-02, -2.5193e-02,  ...,  3.6412e-03,\n",
            "          -9.1324e-02,  6.5189e-02],\n",
            "         [ 9.2126e-02, -5.5362e-02, -2.5193e-02,  ...,  3.6412e-03,\n",
            "          -9.1324e-02,  6.5189e-02]],\n",
            "\n",
            "        [[ 1.5859e-01, -1.7022e-01,  7.2868e-03,  ...,  1.6530e-01,\n",
            "           4.2361e-02,  1.1476e-01],\n",
            "         [ 9.7952e-02, -2.0152e-01,  5.8493e-02,  ...,  1.4715e-01,\n",
            "           7.3116e-02,  1.0151e-01],\n",
            "         [ 1.0513e-01, -1.3526e-01, -5.0145e-02,  ...,  2.4414e-01,\n",
            "          -2.6621e-02,  6.9936e-02],\n",
            "         ...,\n",
            "         [ 7.1735e-02, -1.3981e-01,  1.2107e-02,  ...,  1.6511e-01,\n",
            "           4.1561e-02,  1.0752e-01],\n",
            "         [ 7.6005e-02, -2.2445e-01,  1.3787e-04,  ...,  1.8450e-01,\n",
            "           8.9300e-02,  1.3747e-01],\n",
            "         [ 5.6282e-02, -2.0169e-01,  1.0117e-01,  ...,  2.7164e-01,\n",
            "           5.3314e-02,  8.0174e-02]],\n",
            "\n",
            "        [[ 4.5322e-02, -1.0781e-01,  1.7287e-01,  ...,  1.2945e-01,\n",
            "           1.5984e-01,  5.8736e-02],\n",
            "         [ 1.3599e-01,  1.2375e-02,  2.7249e-01,  ...,  1.6272e-01,\n",
            "           1.1269e-01,  6.6994e-02],\n",
            "         [ 1.1330e-01, -7.5233e-02,  1.9565e-01,  ...,  1.5558e-01,\n",
            "           1.5530e-01,  7.6192e-02],\n",
            "         ...,\n",
            "         [ 1.1468e-01, -5.1589e-02,  2.1832e-01,  ...,  1.6780e-01,\n",
            "           1.6354e-01,  1.6926e-01],\n",
            "         [ 1.2270e-01, -4.6576e-02,  1.9465e-01,  ...,  1.3561e-01,\n",
            "           1.4951e-01,  4.1330e-02],\n",
            "         [ 7.8626e-02,  2.3950e-02,  2.1628e-01,  ...,  7.0627e-02,\n",
            "           1.0728e-01,  8.3351e-02]],\n",
            "\n",
            "        [[-1.1387e-02, -1.2359e-01, -1.0498e-01,  ...,  7.1910e-02,\n",
            "           7.0009e-02, -6.0600e-02],\n",
            "         [-7.0823e-02, -1.5822e-01, -8.0430e-02,  ...,  2.0898e-02,\n",
            "           7.6630e-02, -5.9342e-02],\n",
            "         [ 1.6359e-02, -1.3202e-01, -7.7177e-02,  ...,  9.2399e-03,\n",
            "           4.8041e-02, -5.9742e-02],\n",
            "         ...,\n",
            "         [ 8.0949e-03, -1.1556e-01, -9.8852e-02,  ...,  1.1825e-02,\n",
            "           5.1050e-02, -5.7684e-02],\n",
            "         [-1.0269e-02, -1.7423e-01, -1.0911e-01,  ...,  3.0705e-02,\n",
            "           7.1069e-02, -6.9632e-02],\n",
            "         [-1.0269e-02, -1.7423e-01, -1.0911e-01,  ...,  3.0705e-02,\n",
            "           7.1069e-02, -6.9632e-02]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([5, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTku1fySVR3L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}